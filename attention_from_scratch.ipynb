{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Mechanism from Scratch\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/adiel2012/deep-learning-abc/blob/main/attention_from_scratch.ipynb)\n",
    "\n",
    "This notebook implements the attention mechanism using **only low-level tensor operations** \u2014 no `nn.Module`, no `nn.Linear`, no high-level wrappers.\n",
    "\n",
    "We build everything from raw matrix multiplications and element-wise operations so you can see exactly what happens at each step.\n",
    "\n",
    "> **Companion notebook:** [attention_with_pytorch.ipynb](https://colab.research.google.com/github/adiel2012/deep-learning-abc/blob/main/attention_with_pytorch.ipynb) \u2014 same content using `nn.Module`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (Colab already has torch, but this ensures compatibility)\n",
    "!pip install torch matplotlib -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Use GPU if available (Colab: Runtime > Change runtime type > GPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_heatmap(data, tokens, title):\n",
    "    \"\"\"Helper to plot attention weights/scores with values.\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(5, 4))\n",
    "    im = ax.imshow(data, cmap='Blues')\n",
    "    \n",
    "    # Add colorbar\n",
    "    plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "    \n",
    "    # Ticks and labels\n",
    "    ax.set_xticks(range(len(tokens)))\n",
    "    ax.set_yticks(range(len(tokens)))\n",
    "    ax.set_xticklabels(tokens)\n",
    "    ax.set_yticklabels(tokens)\n",
    "    ax.set_xlabel(\"Key (attending to)\")\n",
    "    ax.set_ylabel(\"Query (token)\")\n",
    "    ax.set_title(title)\n",
    "    \n",
    "    # Text annotations\n",
    "    for i in range(len(tokens)):\n",
    "        for j in range(len(tokens)):\n",
    "            val = data[i, j]\n",
    "            color = 'white' if val > data.max()/2 else 'black'\n",
    "            ax.text(j, i, f\"{val:.2f}\", ha='center', va='center', color=color)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Input Pipeline: Text to Tensors\n",
    "\n",
    "Before we jump into attention, let's see how raw text becomes the matrix `X` we use as input. This process involves three main steps:\n",
    "1. **Tokenization**: Converting words to discrete IDs.\n",
    "2. **Embedding**: Mapping IDs to continuous vectors.\n",
    "3. **Positional Encoding**: Adding information about word order.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"The cat sat down\"\n",
    "tokens = sentence.split()\n",
    "vocab = {word: i for i, word in enumerate(tokens)}\n",
    "token_ids = torch.tensor([vocab[w] for w in tokens])\n",
    "\n",
    "print(f\"Sentence: {sentence}\")\n",
    "print(f\"Token IDs: {token_ids}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.1: Learned Word Embeddings\n",
    "\n",
    "Each token ID points to a row in an **Embedding Matrix**. This matrix is learned during training, allowing the model to represent word meanings as vectors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 8  # embedding dimension\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# In a real model, this is an nn.Embedding layer (learned parameters)\n",
    "embedding_matrix = torch.randn(vocab_size, d_model)\n",
    "\n",
    "# Lookup embeddings for our IDs\n",
    "word_embeddings = embedding_matrix[token_ids]  # Shape: (seq_len, d_model)\n",
    "\n",
    "print(f\"Word Embeddings shape: {word_embeddings.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.2: Positional Encoding (The 'Where')\n",
    "\n",
    "Attention is permutation-invariant \u2014 it treats the sentence like a 'bag of words'. To give the model a sense of order, we add **Positional Encodings** ($PE$).\n",
    "\n",
    "We use the standard Sine/Cosine formula from the original Transformer paper:\n",
    "$$PE(pos, 2i) = \\sin\\left(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\right)$$\n",
    "$$PE(pos, 2i+1) = \\cos\\left(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\right)$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_positional_encoding(seq_len, d_model):\n",
    "    pe = torch.zeros(seq_len, d_model)\n",
    "    position = torch.arange(0, seq_len).unsqueeze(1)\n",
    "    div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
    "    \n",
    "    pe[:, 0::2] = torch.sin(position * div_term)\n",
    "    pe[:, 1::2] = torch.cos(position * div_term)\n",
    "    return pe\n",
    "\n",
    "pos_encoding = get_positional_encoding(seq_len, d_model)\n",
    "print(f\"Positional Encoding shape: {pos_encoding.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.3: Final Input Matrix X\n",
    "\n",
    "We simply add the two matrices. Now, each vector in `X` contains both **what** the word is and **where** it is.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = word_embeddings + pos_encoding\n",
    "print(f\"Final Input X shape: {X.shape} (seq_len, d_model)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Projecting to Query, Key, Value\n",
    "\n",
    "Attention doesn't operate on raw embeddings directly. We first project `X` into three different spaces using weight matrices:\n",
    "\n",
    "$$Q = X \\cdot W_Q$$\n",
    "$$K = X \\cdot W_K$$\n",
    "$$V = X \\cdot W_V$$\n",
    "\n",
    "Each weight matrix has shape `(d_model, d_k)`. We use raw `torch.randn` to create them \u2014 no `nn.Linear`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Mathematical Foundations\n",
    "\n",
    "Before writing any code, let's build up the math that attention relies on.\n",
    "\n",
    "---\n",
    "\n",
    "### 0.1 Vectors and Dot Products\n",
    "\n",
    "A **vector** $\\mathbf{a} \\in \\mathbb{R}^d$ is an ordered list of $d$ real numbers. The **dot product** of two vectors measures their alignment:\n",
    "\n",
    "$$\\mathbf{a} \\cdot \\mathbf{b} = \\sum_{i=1}^{d} a_i \\, b_i = \\|\\mathbf{a}\\| \\, \\|\\mathbf{b}\\| \\, \\cos\\theta$$\n",
    "\n",
    "where $\\theta$ is the angle between them. Key intuitions:\n",
    "- **Positive & large**: vectors point in a similar direction (high similarity)\n",
    "- **Near zero**: vectors are roughly orthogonal (unrelated)\n",
    "- **Negative**: vectors point in opposite directions\n",
    "\n",
    "In attention, the dot product $q_i \\cdot k_j$ measures **how relevant** token $j$ is to token $i$.\n",
    "\n",
    "---\n",
    "\n",
    "### 0.2 Matrix Multiplication\n",
    "\n",
    "If $X \\in \\mathbb{R}^{n \\times d}$ and $W \\in \\mathbb{R}^{d \\times m}$, then:\n",
    "\n",
    "$$(XW)_{ij} = \\sum_{k=1}^{d} X_{ik} \\, W_{kj}$$\n",
    "\n",
    "Each row of the result is a **linear combination** of the columns of $W$, weighted by the corresponding row of $X$. This is how we project embeddings into Q, K, V spaces \u2014 it's a learned linear transformation.\n",
    "\n",
    "When we compute $Q K^T$, we're doing all pairwise dot products at once:\n",
    "\n",
    "$$(Q K^T)_{ij} = \\mathbf{q}_i \\cdot \\mathbf{k}_j$$\n",
    "\n",
    "So row $i$ of the resulting matrix contains the similarity of query $i$ with every key.\n",
    "\n",
    "---\n",
    "\n",
    "### 0.3 The Softmax Function\n",
    "\n",
    "Softmax converts a vector of arbitrary real numbers into a **probability distribution** (non-negative, sums to 1):\n",
    "\n",
    "$$\\text{softmax}(\\mathbf{z})_i = \\frac{e^{z_i}}{\\sum_{j=1}^{n} e^{z_j}}$$\n",
    "\n",
    "Properties:\n",
    "- **Output range**: each value is in $(0, 1)$, and they sum to exactly $1$\n",
    "- **Monotonic**: larger inputs get larger probabilities\n",
    "- **Sharpness**: as differences between inputs grow, softmax approaches a one-hot vector (winner-take-all)\n",
    "\n",
    "**Numerical stability trick**: since $e^{z_i}$ can overflow for large $z_i$, we use the identity:\n",
    "\n",
    "$$\\text{softmax}(\\mathbf{z})_i = \\text{softmax}(\\mathbf{z} - c)_i \\quad \\text{for any constant } c$$\n",
    "\n",
    "Setting $c = \\max_j z_j$ keeps the exponents in a safe range.\n",
    "\n",
    "---\n",
    "\n",
    "### 0.4 Visualizing `d_k` and Matrix Multiplication\n",
    "\n",
    "**What is `d_k`?**\n",
    "It is the **inner dimension** of the query/key vectors. When we compute the attention scores ($Q \\cdot K^T$), we perform a matrix multiplication where `d_k` is the dimension we sum over.\n",
    "\n",
    "**Visual Representation:**\n",
    "\n",
    "Let's visualize the operation `Score = Q @ K.T`.\n",
    "\n",
    "```text\n",
    "       Q (Query Matrix)               K.T (Transposed Key Matrix)\n",
    "    [ seq_len x d_k ]                    [ d_k x seq_len ]\n",
    "\n",
    "      +-----------+d_k+                +-------------------+\n",
    "      | . . . . . | ^                  | . . . . | . . . . |\n",
    "      | - Row i - | |                  | . . . . | Col j . |\n",
    "      | . . . . . | | seq_len      d_k | . . . . | . . . . |\n",
    "      +-----------+ v                  | . . . . | . . . . |\n",
    "                                       +-------------------+\n",
    "                                                 ^\n",
    "                                                 | seq_len\n",
    "```\n",
    "\n",
    "When calculating the score for a single pair (Row $i$ of $Q$, Column $j$ of $K^T$), we compute the **dot product**:\n",
    "\n",
    "$$ \\text{Score}_{i,j} = \\sum_{z=1}^{d_k} Q_{i,z} \\cdot K^T_{z,j} $$\n",
    "\n",
    "**Why does `d_k` matter?**\n",
    "Notice the sum symbol $\\sum_{z=1}^{d_k}$.\n",
    "- We are adding up **`d_k` distinct terms**.\n",
    "- If `d_k` is small (e.g., 64), we sum 64 terms.\n",
    "- If `d_k` is huge (e.g., 1024), we sum 1024 terms.\n",
    "\n",
    "**The variance problem:**\n",
    "If the elements of $Q$ and $K$ are random variables with variance 1, their dot product has a variance equal to $d_k$.\n",
    "- **Larger `d_k`** $\\rightarrow$ **Larger variance** $\\rightarrow$ **Larger values** (e.g., +30, -30).\n",
    "\n",
    "Large values push the **Softmax** function into regions with extremely small gradients (vanishing gradients), causing the model to stop learning.\n",
    "\n",
    "**The Fix - Scaling:**\n",
    "We divide by $\\sqrt{d_k}$ to scale the variance back to 1, keeping the gradients healthy.\n",
    "\n",
    "$$\\text{Var}\\!\\left(\\frac{\\mathbf{q} \\cdot \\mathbf{k}}{\\sqrt{d_k}}\\right) = \\frac{d_k}{d_k} = 1$$\n",
    "\n",
    "---\n",
    "\n",
    "### 0.5 Attention as a Weighted Average\n",
    "\n",
    "The final attention output for token $i$ is:\n",
    "\n",
    "$$\\text{output}_i = \\sum_{j=1}^{n} \\alpha_{ij} \\, \\mathbf{v}_j$$\n",
    "\n",
    "where $\\alpha_{ij} = \\text{softmax}\\!\\left(\\frac{\\mathbf{q}_i \\cdot \\mathbf{k}_j}{\\sqrt{d_k}}\\right)_j$ are the attention weights.\n",
    "\n",
    "This is a **convex combination** of the value vectors \u2014 the output lives in the convex hull of the $\\mathbf{v}_j$'s. Tokens with higher similarity scores contribute more to the output.\n",
    "\n",
    "---\n",
    "\n",
    "### 0.6 Linear Projections: Why Q, K, V?\n",
    "\n",
    "Raw embeddings aren't optimized for measuring relevance. The learnable matrices $W_Q, W_K, W_V$ project each token into three roles:\n",
    "\n",
    "| Matrix | Role | Analogy |\n",
    "|--------|------|--------|\n",
    "| $W_Q$ | **Query** \u2014 \"what am I looking for?\" | A database query |\n",
    "| $W_K$ | **Key** \u2014 \"what do I contain?\" | A database index |\n",
    "| $W_V$ | **Value** \u2014 \"what information do I carry?\" | The actual data |\n",
    "\n",
    "The dot product $\\mathbf{q}_i \\cdot \\mathbf{k}_j$ computes **relevance** (query matches key), and the weighted sum over $\\mathbf{v}_j$ retrieves **content** from the relevant tokens.\n",
    "\n",
    "---\n",
    "\n",
    "Now let's implement all of this step by step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_k = 6  # dimension of queries and keys\n",
    "d_v = 6  # dimension of values\n",
    "\n",
    "# Weight matrices \u2014 raw parameters, no nn.Module\n",
    "W_Q = torch.randn(d_model, d_k, device=device) * 0.1 # Shape: (d_model, d_k)\n",
    "W_K = torch.randn(d_model, d_k, device=device) * 0.1 # Shape: (d_model, d_k)\n",
    "W_V = torch.randn(d_model, d_v, device=device) * 0.1 # Shape: (d_model, d_v)\n",
    "\n",
    "# Project: simple matrix multiplication\n",
    "Q = X @ W_Q   # (seq_len, d_k) # Shape: (seq_len, d_k)\n",
    "K = X @ W_K   # (seq_len, d_k) # Shape: (seq_len, d_k)\n",
    "V = X @ W_V   # (seq_len, d_v) # Shape: (seq_len, d_v)\n",
    "\n",
    "print(f\"Q shape: {Q.shape}      (seq_len, d_k)\")\n",
    "print(f\"K shape: {K.shape}      (seq_len, d_k)\")\n",
    "print(f\"V shape: {V.shape}      (seq_len, d_v)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Scaled Dot-Product Attention (Step by Step)\n",
    "\n",
    "The core formula:\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\!\\left(\\frac{Q K^T}{\\sqrt{d_k}}\\right) V$$\n",
    "\n",
    "Let's break it into individual steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3a: Raw Attention Scores\n",
    "\n",
    "$$ \\text{scores} = Q \\cdot K^T $$\n",
    "\n",
    "Each element `scores[i][j]` measures how much token `i`'s query aligns with token `j`'s key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dot product between every pair of (query, key)\n",
    "scores = Q @ K.T   # (seq_len, seq_len)\n",
    "\n",
    "print(f\"Raw attention scores shape: {scores.shape} (seq_len, seq_len)\")\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [\"The\", \"cat\", \"sat\", \"down\"]\n",
    "plot_heatmap(scores.detach().cpu().numpy(), tokens, \"Raw Attention Scores (Q @ K.T)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3b: Scale\n",
    "\n",
    "$$ \\text{scaled\\_scores} = \\frac{\\text{scores}}{\\sqrt{d_k}} $$\n",
    "\n",
    "Without scaling, large `d_k` pushes dot products to extreme values, making softmax output nearly one-hot (vanishing gradients)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = math.sqrt(d_k)\n",
    "scaled_scores = scores / scale\n",
    "\n",
    "print(f\"Scale factor: sqrt({d_k}) = {scale:.4f}\")\n",
    "print(\"Scaled scores:\")\n",
    "print(scaled_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3c: Softmax (implemented manually)\n",
    "\n",
    "$$\\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_j e^{x_j}}$$\n",
    "\n",
    "We implement it by hand with the numerical stability trick: subtract the max before exponentiating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Row-wise softmax, implemented from scratch.\"\"\"\n",
    "    # Subtract max for numerical stability (prevents overflow in exp)\n",
    "    x_max = x.max(dim=-1, keepdim=True).values\n",
    "    exp_x = torch.exp(x - x_max)\n",
    "    return exp_x / exp_x.sum(dim=-1, keepdim=True)\n",
    "\n",
    "attn_weights = softmax(scaled_scores)  # (seq_len, seq_len)\n",
    "\n",
    "\n",
    "plot_heatmap(attn_weights.detach().cpu().numpy(), tokens, \"Attention Weights (Softmax)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading the weight matrix:** Row `i` tells you how much each token contributes to the output of token `i`.\n",
    "\n",
    "For example, if `attn_weights[1] = [0.1, 0.5, 0.3, 0.1]`, then the output for \"cat\" (token 1) is 50% influenced by itself, 30% by \"sat\", and 10% each by \"The\" and \"down\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3d: Weighted Sum of Values\n",
    "\n",
    "$$\\text{output} = \\text{attn\\_weights} \\cdot V$$\n",
    "\n",
    "Each output row is a weighted combination of all value vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualizing the Weighted Sum:**\n\n",
    "\n\n",
    "How do the attention weights and the Value matrix ($V$) combine to produce the final output?\n\n",
    "\n\n",
    "```text\n\n",
    "      Weights (Softmax)                V (Value Matrix)                Output Matrix\n\n",
    "    [ seq_len x seq_len ]             [ seq_len x d_v ]              [ seq_len x d_v ]\n\n",
    "\n\n",
    "      +---------------+                +-----------+d_v+              +---------------+\n\n",
    "      | . . . . . . . |                | . . . . . | ^                | . . . . . . . |\n\n",
    "      | --- Row i --- |       @        | - Row 0 - | |       =        | --- Row i --- |\n\n",
    "      | . . . . . . . |                | - Row 1 - | | seq_len        | . . . . . . . |\n\n",
    "      +---------------+                | . . . . . | |                +---------------+\n\n",
    "                                       +-----------+ v\n\n",
    "```\n\n",
    "\n\n",
    "**Row $i$ Calculation:**\n\n",
    "\n\n",
    "Each element $j$ in **Row $i$ of the Weights matrix** tells us the importance of **Row $j$ of the Value matrix ($V$)** for the current token $i$.\n\n",
    "\n\n",
    "$$\\text{Output}_i = \\sum_{j=1}^{\\text{seq\\_len}} \\text{Weight}_{i,j} \\cdot \\mathbf{v}_j$$\n\n",
    "\n\n",
    "Where $\\mathbf{v}_j$ is the $j$-th row of $V$. This shows that the output is a **weighted average** of all the value vectors. If a weight is near 1, the output will be very similar to that specific value vector; if weights are distributed, the output is a blend.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_output = attn_weights @ V  # (seq_len, d_v)\n",
    "\n",
    "print(f\"Attention output shape: {attn_output.shape} (seq_len, d_v)\")\n",
    "print(attn_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
    "    \"\"\"\n",
    "    Computes scaled dot-product attention using only basic ops.\n",
    "    \n",
    "    Q: (seq_len, d_k)\n",
    "    K: (seq_len, d_k)\n",
    "    V: (seq_len, d_v)\n",
    "    mask: optional (seq_len, seq_len) \u2014 True where we want to block attention\n",
    "    \n",
    "    Returns: (seq_len, d_v) attention output, (seq_len, seq_len) weights\n",
    "    \"\"\"\n",
    "    d_k = Q.shape[-1]\n",
    "    \n",
    "    # 1. Compute scores\n",
    "    scores = Q @ K.T / math.sqrt(d_k)\n",
    "    \n",
    "    # 2. Apply mask (for causal / decoder attention)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask, float('-inf'))\n",
    "    \n",
    "    # 3. Softmax\n",
    "    weights = softmax(scores)\n",
    "    \n",
    "    # 4. Weighted sum\n",
    "    output = weights @ V\n",
    "    \n",
    "    return output, weights\n",
    "\n",
    "# Verify it matches our step-by-step result\n",
    "out, w = scaled_dot_product_attention(Q, K, V)\n",
    "print(\"Matches step-by-step?\", torch.allclose(out, attn_output, atol=1e-6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Causal (Decoder) Mask\n",
    "\n",
    "In autoregressive models (like GPT), token `i` should only attend to tokens `0..i`, not future tokens. We achieve this with a **causal mask** that sets future positions to `-inf` before softmax.\n",
    "\n",
    "```\n",
    "         The  cat  sat  down\n",
    "The    [  ok  -inf -inf -inf ]\n",
    "cat    [  ok   ok  -inf -inf ]\n",
    "sat    [  ok   ok   ok  -inf ]\n",
    "down   [  ok   ok   ok   ok  ]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create Mask\n",
    "causal_mask = torch.triu(torch.ones(seq_len, seq_len, dtype=torch.bool, device=device), diagonal=1)\n",
    "\n",
    "# 2. Apply Mask to raw scaled scores (re-calculating for demo)\n",
    "scores_raw = Q @ K.T / math.sqrt(d_k)\n",
    "scores_masked = scores_raw.masked_fill(causal_mask, float('-inf'))\n",
    "\n",
    "# 3. Softmax\n",
    "weights_masked = softmax(scores_masked)\n",
    "\n",
    "# Visual Comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "# Pre-mask (just raw weights for comparison sake, though strictly we mask before softmax)\n",
    "plot_heatmap(softmax(scores_raw).detach().cpu().numpy(), tokens, \"Pre-Mask Weights\")\n",
    "# Actually, helper creates its own fig. Let's just call helper twice for simplicity in this notebook context.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Causal Mask (Upper Triangle blocked):\")\n",
    "plot_heatmap(causal_mask.float().cpu().numpy(), tokens, \"Causal Mask\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Weights AFTER masking (Futures set to 0.00):\")\n",
    "plot_heatmap(weights_masked.detach().cpu().numpy(), tokens, \"Masked Attention Weights\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Multi-Head Attention (from raw ops)\n",
    "\n",
    "Instead of one attention function, we run `n_heads` in parallel, each with its own Q/K/V projections, then concatenate and project the result.\n",
    "\n",
    "$$\\text{MultiHead}(X) = \\text{Concat}(\\text{head}_1, \\dots, \\text{head}_h) \\cdot W_O$$\n",
    "\n",
    "where each head:\n",
    "$$\\text{head}_i = \\text{Attention}(X W_Q^i,\\; X W_K^i,\\; X W_V^i)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_heads = 2\n",
    "d_k_per_head = d_model // n_heads  # 4\n",
    "d_v_per_head = d_model // n_heads  # 4\n",
    "\n",
    "print(f\"n_heads={n_heads}, d_k_per_head={d_k_per_head}, d_v_per_head={d_v_per_head}\")\n",
    "\n",
    "# Create separate weight matrices for each head \u2014 raw tensors\n",
    "W_Qs = [torch.randn(d_model, d_k_per_head, device=device) * 0.1 for _ in range(n_heads)] # List of (d_model, d_k_per_head)\n",
    "W_Ks = [torch.randn(d_model, d_k_per_head, device=device) * 0.1 for _ in range(n_heads)] # List of (d_model, d_k_per_head)\n",
    "W_Vs = [torch.randn(d_model, d_v_per_head, device=device) * 0.1 for _ in range(n_heads)] # List of (d_model, d_v_per_head)\n",
    "\n",
    "# Output projection: maps concatenated heads back to d_model\n",
    "W_O = torch.randn(n_heads * d_v_per_head, d_model, device=device) * 0.1 # Shape: (n_heads * d_v_per_head, d_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run each head independently\n",
    "head_outputs = []\n",
    "\n",
    "for i in range(n_heads):\n",
    "    Q_i = X @ W_Qs[i]  # (seq_len, d_k_per_head) # Shape: (seq_len, d_k_per_head)\n",
    "    K_i = X @ W_Ks[i] # Shape: (seq_len, d_k_per_head)\n",
    "    V_i = X @ W_Vs[i] # Shape: (seq_len, d_v_per_head)\n",
    "    \n",
    "    head_out, head_weights = scaled_dot_product_attention(Q_i, K_i, V_i)\n",
    "    head_outputs.append(head_out)\n",
    "    \n",
    "    print(f\"Head {i} attention weights:\")\n",
    "    print(head_weights)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all heads along the last dimension\n",
    "concat = torch.cat(head_outputs, dim=-1)  # (seq_len, n_heads * d_v_per_head) # Shape: (seq_len, n_heads * d_v_per_head)\n",
    "print(f\"Concatenated shape: {concat.shape} (seq_len, d_model)\")\n",
    "\n",
    "# Final linear projection (raw matmul, no nn.Linear)\n",
    "multi_head_output = concat @ W_O  # (seq_len, d_model)\n",
    "print(f\"Multi-head output shape: {multi_head_output.shape} (seq_len, d_model)\")\n",
    "print(multi_head_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why multiple heads?** Each head can learn to attend to different types of relationships. For instance, one head might focus on adjacent tokens (local syntax), while another head attends to the subject of the sentence (long-range dependency)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Understanding the Reshape Logic\n",
    "\n",
    "In efficient Multi-Head Attention, we don't loop.\n",
    "Instead, we use `view` and `transpose` to shuffle the data.\n",
    "\n",
    "**The Goal:** Transform `(seq_len, d_model)` $\\rightarrow$ `(n_heads, seq_len, d_k)`.\n",
    "\n",
    "**Step 1: View (Split heads)**\n",
    "`x.view(seq_len, n_heads, d_k)` separates the `d_model` dimension into heads.\n",
    "Logically, this groups features belonging to the same head together, but they are still nested inside each token.\n",
    "\n",
    "**Step 2: Transpose (Group by head)**\n",
    "`x.transpose(0, 1)` swaps the first two dimensions.\n",
    "Now, all tokens for `head_0` are contiguous, allowing us to do matrix multiplication for `head_0` in one go.\n",
    "\n",
    "```text\n",
    "Input:  [Token 1] [Token 2] ...\n",
    "           |         |\n",
    "        [h1,h2]   [h1,h2]\n",
    "\n",
    "          || view\n",
    "          VV\n",
    "        [[h1],    [[h1],\n",
    "         [h2]]     [h2]]\n",
    "\n",
    "          || transpose\n",
    "          VV\n",
    "Head 1: [Token 1_h1, Token 2_h1, ...]\n",
    "Head 2: [Token 1_h2, Token 2_h2, ...]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Multi-Head Attention (Batched / Efficient Version)\n",
    "\n",
    "The loop above is clear but slow. In practice, we pack all heads into a single large projection, then reshape. Still no `nn.Module` \u2014 just reshape tricks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_head_attention(X, W_Q, W_K, W_V, W_O, n_heads, mask=None):\n",
    "    \"\"\"\n",
    "    Efficient multi-head attention using reshape instead of loops.\n",
    "    \n",
    "    X:    (seq_len, d_model)\n",
    "    W_Q:  (d_model, d_model)  \u2014 all heads packed into one matrix\n",
    "    W_K:  (d_model, d_model)\n",
    "    W_V:  (d_model, d_model)\n",
    "    W_O:  (d_model, d_model)\n",
    "    \"\"\"\n",
    "    seq_len, d_model = X.shape\n",
    "    d_k = d_model // n_heads\n",
    "    \n",
    "    # 1. Project all heads at once\n",
    "    Q = X @ W_Q  # Shape: (seq_len, d_model)\n",
    "    K = X @ W_K\n",
    "    V = X @ W_V\n",
    "    \n",
    "    # 2. Reshape to (n_heads, seq_len, d_k) \u2014 split d_model into heads\n",
    "    Q = Q.view(seq_len, n_heads, d_k).transpose(0, 1)  # Shape: (n_heads, seq_len, d_k)\n",
    "    K = K.view(seq_len, n_heads, d_k).transpose(0, 1)\n",
    "    V = V.view(seq_len, n_heads, d_k).transpose(0, 1)\n",
    "    \n",
    "    # 3. Scaled dot-product attention (batched over heads)\n",
    "    scores = Q @ K.transpose(-2, -1) / math.sqrt(d_k)  # Shape: (n_heads, seq_len, seq_len)\n",
    "    \n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask.unsqueeze(0), float('-inf'))\n",
    "    \n",
    "    weights = softmax(scores)  # softmax works because we wrote it for dim=-1\n",
    "    attn_out = weights @ V     # Shape: (n_heads, seq_len, d_k)\n",
    "    \n",
    "    # 4. Concatenate heads: transpose back and reshape\n",
    "    attn_out = attn_out.transpose(0, 1).contiguous().view(seq_len, d_model)\n",
    "    \n",
    "    # 5. Output projection\n",
    "    output = attn_out @ W_O  # (seq_len, d_model)\n",
    "    \n",
    "    return output, weights\n",
    "\n",
    "\n",
    "# Create packed weight matrices (all heads in one tensor)\n",
    "W_Q_packed = torch.randn(d_model, d_model, device=device) * 0.1\n",
    "W_K_packed = torch.randn(d_model, d_model, device=device) * 0.1\n",
    "W_V_packed = torch.randn(d_model, d_model, device=device) * 0.1\n",
    "W_O_packed = torch.randn(d_model, d_model, device=device) * 0.1\n",
    "\n",
    "mha_output, mha_weights = multi_head_attention(\n",
    "    X, W_Q_packed, W_K_packed, W_V_packed, W_O_packed, n_heads\n",
    ")\n",
    "\n",
    "print(\"MHA output shape:\", mha_output.shape)\n",
    "print(\"Attention weights shape:\", mha_weights.shape, \" \u2014 (n_heads, seq_len, seq_len)\")\n",
    "print(\"\\nHead 0 weights:\")\n",
    "print(mha_weights[0])\n",
    "print(\"\\nHead 1 weights:\")\n",
    "print(mha_weights[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualizing Attention Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing heads using our helper\n",
    "for i in range(n_heads):\n",
    "    w = mha_weights[i].detach().cpu().numpy()\n",
    "    plot_heatmap(w, tokens, f\"Multi-Head Attention - Head {i}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary\n",
    "\n",
    "Everything we built uses only these primitive operations:\n",
    "\n",
    "| Operation | PyTorch op | Purpose |\n",
    "|-----------|-----------|----------|\n",
    "| Matrix multiply | `@` / `torch.matmul` | Project Q, K, V; compute scores; weighted sum |\n",
    "| Division | `/` | Scale scores by \u221ad_k |\n",
    "| Exp | `torch.exp` | Part of softmax |\n",
    "| Sum | `.sum()` | Part of softmax |\n",
    "| Max | `.max()` | Numerical stability in softmax |\n",
    "| Masked fill | `.masked_fill()` | Causal masking |\n",
    "| Reshape | `.view()`, `.transpose()` | Splitting/merging heads |\n",
    "\n",
    "No `nn.Module`, no `nn.Linear`, no `nn.MultiheadAttention` \u2014 just tensors and math."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}