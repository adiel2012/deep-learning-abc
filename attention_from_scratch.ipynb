{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Mechanism from Scratch\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/adiel2012/deep-learning-abc/blob/main/attention_from_scratch.ipynb)\n",
    "\n",
    "This notebook implements the attention mechanism using **only low-level tensor operations** — no `nn.Module`, no `nn.Linear`, no high-level wrappers.\n",
    "\n",
    "We build everything from raw matrix multiplications and element-wise operations so you can see exactly what happens at each step.\n",
    "\n",
    "> **Companion notebook:** [attention_with_pytorch.ipynb](https://colab.research.google.com/github/adiel2012/deep-learning-abc/blob/main/attention_with_pytorch.ipynb) — same content using `nn.Module`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (Colab already has torch, but this ensures compatibility)\n",
    "!pip install torch matplotlib -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Mathematical Foundations\n",
    "\n",
    "Before writing any code, let's build up the math that attention relies on.\n",
    "\n",
    "---\n",
    "\n",
    "### 0.1 Vectors and Dot Products\n",
    "\n",
    "A **vector** $\\mathbf{a} \\in \\mathbb{R}^d$ is an ordered list of $d$ real numbers. The **dot product** of two vectors measures their alignment:\n",
    "\n",
    "$$\\mathbf{a} \\cdot \\mathbf{b} = \\sum_{i=1}^{d} a_i \\, b_i = \\|\\mathbf{a}\\| \\, \\|\\mathbf{b}\\| \\, \\cos\\theta$$\n",
    "\n",
    "where $\\theta$ is the angle between them. Key intuitions:\n",
    "- **Positive & large**: vectors point in a similar direction (high similarity)\n",
    "- **Near zero**: vectors are roughly orthogonal (unrelated)\n",
    "- **Negative**: vectors point in opposite directions\n",
    "\n",
    "In attention, the dot product $q_i \\cdot k_j$ measures **how relevant** token $j$ is to token $i$.\n",
    "\n",
    "---\n",
    "\n",
    "### 0.2 Matrix Multiplication\n",
    "\n",
    "If $X \\in \\mathbb{R}^{n \\times d}$ and $W \\in \\mathbb{R}^{d \\times m}$, then:\n",
    "\n",
    "$$(XW)_{ij} = \\sum_{k=1}^{d} X_{ik} \\, W_{kj}$$\n",
    "\n",
    "Each row of the result is a **linear combination** of the columns of $W$, weighted by the corresponding row of $X$. This is how we project embeddings into Q, K, V spaces — it's a learned linear transformation.\n",
    "\n",
    "When we compute $Q K^T$, we're doing all pairwise dot products at once:\n",
    "\n",
    "$$(Q K^T)_{ij} = \\mathbf{q}_i \\cdot \\mathbf{k}_j$$\n",
    "\n",
    "So row $i$ of the resulting matrix contains the similarity of query $i$ with every key.\n",
    "\n",
    "---\n",
    "\n",
    "### 0.3 The Softmax Function\n",
    "\n",
    "Softmax converts a vector of arbitrary real numbers into a **probability distribution** (non-negative, sums to 1):\n",
    "\n",
    "$$\\text{softmax}(\\mathbf{z})_i = \\frac{e^{z_i}}{\\sum_{j=1}^{n} e^{z_j}}$$\n",
    "\n",
    "Properties:\n",
    "- **Output range**: each value is in $(0, 1)$, and they sum to exactly $1$\n",
    "- **Monotonic**: larger inputs get larger probabilities\n",
    "- **Sharpness**: as differences between inputs grow, softmax approaches a one-hot vector (winner-take-all)\n",
    "\n",
    "**Numerical stability trick**: since $e^{z_i}$ can overflow for large $z_i$, we use the identity:\n",
    "\n",
    "$$\\text{softmax}(\\mathbf{z})_i = \\text{softmax}(\\mathbf{z} - c)_i \\quad \\text{for any constant } c$$\n",
    "\n",
    "Setting $c = \\max_j z_j$ keeps the exponents in a safe range.\n",
    "\n",
    "---\n",
    "\n",
    "### 0.4 Why Scale by $\\sqrt{d_k}$?\n",
    "\n",
    "If the entries of $\\mathbf{q}$ and $\\mathbf{k}$ are i.i.d. with mean 0 and variance 1, then:\n",
    "\n",
    "$$\\text{Var}(\\mathbf{q} \\cdot \\mathbf{k}) = \\text{Var}\\!\\left(\\sum_{i=1}^{d_k} q_i k_i\\right) = d_k$$\n",
    "\n",
    "So the dot product's variance **grows linearly** with $d_k$. For large $d_k$, the scores become large in magnitude, pushing softmax into saturation (near-zero gradients).\n",
    "\n",
    "Dividing by $\\sqrt{d_k}$ normalizes the variance back to 1:\n",
    "\n",
    "$$\\text{Var}\\!\\left(\\frac{\\mathbf{q} \\cdot \\mathbf{k}}{\\sqrt{d_k}}\\right) = \\frac{d_k}{d_k} = 1$$\n",
    "\n",
    "---\n",
    "\n",
    "### 0.5 Attention as a Weighted Average\n",
    "\n",
    "The final attention output for token $i$ is:\n",
    "\n",
    "$$\\text{output}_i = \\sum_{j=1}^{n} \\alpha_{ij} \\, \\mathbf{v}_j$$\n",
    "\n",
    "where $\\alpha_{ij} = \\text{softmax}\\!\\left(\\frac{\\mathbf{q}_i \\cdot \\mathbf{k}_j}{\\sqrt{d_k}}\\right)_j$ are the attention weights.\n",
    "\n",
    "This is a **convex combination** of the value vectors — the output lives in the convex hull of the $\\mathbf{v}_j$'s. Tokens with higher similarity scores contribute more to the output.\n",
    "\n",
    "---\n",
    "\n",
    "### 0.6 Linear Projections: Why Q, K, V?\n",
    "\n",
    "Raw embeddings aren't optimized for measuring relevance. The learnable matrices $W_Q, W_K, W_V$ project each token into three roles:\n",
    "\n",
    "| Matrix | Role | Analogy |\n",
    "|--------|------|--------|\n",
    "| $W_Q$ | **Query** — \"what am I looking for?\" | A database query |\n",
    "| $W_K$ | **Key** — \"what do I contain?\" | A database index |\n",
    "| $W_V$ | **Value** — \"what information do I carry?\" | The actual data |\n",
    "\n",
    "The dot product $\\mathbf{q}_i \\cdot \\mathbf{k}_j$ computes **relevance** (query matches key), and the weighted sum over $\\mathbf{v}_j$ retrieves **content** from the relevant tokens.\n",
    "\n",
    "---\n",
    "\n",
    "Now let's implement all of this step by step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Use GPU if available (Colab: Runtime > Change runtime type > GPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup: Fake Token Embeddings\n",
    "\n",
    "Imagine a sentence with 4 tokens, each represented by an 8-dimensional embedding vector.\n",
    "\n",
    "| Index | Token   |\n",
    "|-------|---------|\n",
    "| 0     | \"The\"   |\n",
    "| 1     | \"cat\"   |\n",
    "| 2     | \"sat\"   |\n",
    "| 3     | \"down\"  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 4    # number of tokens\n",
    "d_model = 8    # embedding dimension\n",
    "\n",
    "# Random embeddings simulating the output of an embedding layer\n",
    "X = torch.randn(seq_len, d_model, device=device)\n",
    "print(\"Input X shape:\", X.shape)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Projecting to Query, Key, Value\n",
    "\n",
    "Attention doesn't operate on raw embeddings directly. We first project `X` into three different spaces using weight matrices:\n",
    "\n",
    "$$Q = X \\cdot W_Q$$\n",
    "$$K = X \\cdot W_K$$\n",
    "$$V = X \\cdot W_V$$\n",
    "\n",
    "Each weight matrix has shape `(d_model, d_k)`. We use raw `torch.randn` to create them — no `nn.Linear`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_k = 6  # dimension of queries and keys\n",
    "d_v = 6  # dimension of values\n",
    "\n",
    "# Weight matrices — raw parameters, no nn.Module\n",
    "W_Q = torch.randn(d_model, d_k, device=device) * 0.1\n",
    "W_K = torch.randn(d_model, d_k, device=device) * 0.1\n",
    "W_V = torch.randn(d_model, d_v, device=device) * 0.1\n",
    "\n",
    "# Project: simple matrix multiplication\n",
    "Q = X @ W_Q   # (seq_len, d_k)\n",
    "K = X @ W_K   # (seq_len, d_k)\n",
    "V = X @ W_V   # (seq_len, d_v)\n",
    "\n",
    "print(\"Q shape:\", Q.shape)\n",
    "print(\"K shape:\", K.shape)\n",
    "print(\"V shape:\", V.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Scaled Dot-Product Attention (Step by Step)\n",
    "\n",
    "The core formula:\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\!\\left(\\frac{Q K^T}{\\sqrt{d_k}}\\right) V$$\n",
    "\n",
    "Let's break it into individual steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3a: Raw Attention Scores\n",
    "\n",
    "$$ \\text{scores} = Q \\cdot K^T $$\n",
    "\n",
    "Each element `scores[i][j]` measures how much token `i`'s query aligns with token `j`'s key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dot product between every pair of (query, key)\n",
    "scores = Q @ K.T   # (seq_len, seq_len)\n",
    "\n",
    "print(\"Raw attention scores shape:\", scores.shape)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3b: Scale\n",
    "\n",
    "$$ \\text{scaled\\_scores} = \\frac{\\text{scores}}{\\sqrt{d_k}} $$\n",
    "\n",
    "Without scaling, large `d_k` pushes dot products to extreme values, making softmax output nearly one-hot (vanishing gradients)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = math.sqrt(d_k)\n",
    "scaled_scores = scores / scale\n",
    "\n",
    "print(f\"Scale factor: sqrt({d_k}) = {scale:.4f}\")\n",
    "print(\"Scaled scores:\")\n",
    "print(scaled_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3c: Softmax (implemented manually)\n",
    "\n",
    "$$\\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_j e^{x_j}}$$\n",
    "\n",
    "We implement it by hand with the numerical stability trick: subtract the max before exponentiating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Row-wise softmax, implemented from scratch.\"\"\"\n",
    "    # Subtract max for numerical stability (prevents overflow in exp)\n",
    "    x_max = x.max(dim=-1, keepdim=True).values\n",
    "    exp_x = torch.exp(x - x_max)\n",
    "    return exp_x / exp_x.sum(dim=-1, keepdim=True)\n",
    "\n",
    "attn_weights = softmax(scaled_scores)  # (seq_len, seq_len)\n",
    "\n",
    "print(\"Attention weights (each row sums to 1):\")\n",
    "print(attn_weights)\n",
    "print(\"\\nRow sums:\", attn_weights.sum(dim=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading the weight matrix:** Row `i` tells you how much each token contributes to the output of token `i`.\n",
    "\n",
    "For example, if `attn_weights[1] = [0.1, 0.5, 0.3, 0.1]`, then the output for \"cat\" (token 1) is 50% influenced by itself, 30% by \"sat\", and 10% each by \"The\" and \"down\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3d: Weighted Sum of Values\n",
    "\n",
    "$$\\text{output} = \\text{attn\\_weights} \\cdot V$$\n",
    "\n",
    "Each output row is a weighted combination of all value vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_output = attn_weights @ V  # (seq_len, d_v)\n",
    "\n",
    "print(\"Attention output shape:\", attn_output.shape)\n",
    "print(attn_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
    "    \"\"\"\n",
    "    Computes scaled dot-product attention using only basic ops.\n",
    "    \n",
    "    Q: (seq_len, d_k)\n",
    "    K: (seq_len, d_k)\n",
    "    V: (seq_len, d_v)\n",
    "    mask: optional (seq_len, seq_len) — True where we want to block attention\n",
    "    \n",
    "    Returns: (seq_len, d_v) attention output, (seq_len, seq_len) weights\n",
    "    \"\"\"\n",
    "    d_k = Q.shape[-1]\n",
    "    \n",
    "    # 1. Compute scores\n",
    "    scores = Q @ K.T / math.sqrt(d_k)\n",
    "    \n",
    "    # 2. Apply mask (for causal / decoder attention)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask, float('-inf'))\n",
    "    \n",
    "    # 3. Softmax\n",
    "    weights = softmax(scores)\n",
    "    \n",
    "    # 4. Weighted sum\n",
    "    output = weights @ V\n",
    "    \n",
    "    return output, weights\n",
    "\n",
    "# Verify it matches our step-by-step result\n",
    "out, w = scaled_dot_product_attention(Q, K, V)\n",
    "print(\"Matches step-by-step?\", torch.allclose(out, attn_output, atol=1e-6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Causal (Decoder) Mask\n",
    "\n",
    "In autoregressive models (like GPT), token `i` should only attend to tokens `0..i`, not future tokens. We achieve this with a **causal mask** that sets future positions to `-inf` before softmax.\n",
    "\n",
    "```\n",
    "         The  cat  sat  down\n",
    "The    [  ok  -inf -inf -inf ]\n",
    "cat    [  ok   ok  -inf -inf ]\n",
    "sat    [  ok   ok   ok  -inf ]\n",
    "down   [  ok   ok   ok   ok  ]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upper triangular = True where we want to BLOCK attention\n",
    "causal_mask = torch.triu(torch.ones(seq_len, seq_len, dtype=torch.bool, device=device), diagonal=1)\n",
    "print(\"Causal mask (True = blocked):\")\n",
    "print(causal_mask.int())\n",
    "\n",
    "causal_out, causal_weights = scaled_dot_product_attention(Q, K, V, mask=causal_mask)\n",
    "print(\"\\nCausal attention weights:\")\n",
    "print(causal_weights)\n",
    "print(\"\\nNotice: upper triangle is 0 — no attending to future tokens.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Multi-Head Attention (from raw ops)\n",
    "\n",
    "Instead of one attention function, we run `n_heads` in parallel, each with its own Q/K/V projections, then concatenate and project the result.\n",
    "\n",
    "$$\\text{MultiHead}(X) = \\text{Concat}(\\text{head}_1, \\dots, \\text{head}_h) \\cdot W_O$$\n",
    "\n",
    "where each head:\n",
    "$$\\text{head}_i = \\text{Attention}(X W_Q^i,\\; X W_K^i,\\; X W_V^i)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_heads = 2\n",
    "d_k_per_head = d_model // n_heads  # 4\n",
    "d_v_per_head = d_model // n_heads  # 4\n",
    "\n",
    "print(f\"n_heads={n_heads}, d_k_per_head={d_k_per_head}, d_v_per_head={d_v_per_head}\")\n",
    "\n",
    "# Create separate weight matrices for each head — raw tensors\n",
    "W_Qs = [torch.randn(d_model, d_k_per_head, device=device) * 0.1 for _ in range(n_heads)]\n",
    "W_Ks = [torch.randn(d_model, d_k_per_head, device=device) * 0.1 for _ in range(n_heads)]\n",
    "W_Vs = [torch.randn(d_model, d_v_per_head, device=device) * 0.1 for _ in range(n_heads)]\n",
    "\n",
    "# Output projection: maps concatenated heads back to d_model\n",
    "W_O = torch.randn(n_heads * d_v_per_head, d_model, device=device) * 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run each head independently\n",
    "head_outputs = []\n",
    "\n",
    "for i in range(n_heads):\n",
    "    Q_i = X @ W_Qs[i]  # (seq_len, d_k_per_head)\n",
    "    K_i = X @ W_Ks[i]\n",
    "    V_i = X @ W_Vs[i]\n",
    "    \n",
    "    head_out, head_weights = scaled_dot_product_attention(Q_i, K_i, V_i)\n",
    "    head_outputs.append(head_out)\n",
    "    \n",
    "    print(f\"Head {i} attention weights:\")\n",
    "    print(head_weights)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all heads along the last dimension\n",
    "concat = torch.cat(head_outputs, dim=-1)  # (seq_len, n_heads * d_v_per_head)\n",
    "print(\"Concatenated shape:\", concat.shape)\n",
    "\n",
    "# Final linear projection (raw matmul, no nn.Linear)\n",
    "multi_head_output = concat @ W_O  # (seq_len, d_model)\n",
    "print(\"Multi-head output shape:\", multi_head_output.shape)\n",
    "print(multi_head_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why multiple heads?** Each head can learn to attend to different types of relationships. For instance, one head might focus on adjacent tokens (local syntax), while another head attends to the subject of the sentence (long-range dependency)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Multi-Head Attention (Batched / Efficient Version)\n",
    "\n",
    "The loop above is clear but slow. In practice, we pack all heads into a single large projection, then reshape. Still no `nn.Module` — just reshape tricks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_head_attention(X, W_Q, W_K, W_V, W_O, n_heads, mask=None):\n",
    "    \"\"\"\n",
    "    Efficient multi-head attention using reshape instead of loops.\n",
    "    \n",
    "    X:    (seq_len, d_model)\n",
    "    W_Q:  (d_model, d_model)  — all heads packed into one matrix\n",
    "    W_K:  (d_model, d_model)\n",
    "    W_V:  (d_model, d_model)\n",
    "    W_O:  (d_model, d_model)\n",
    "    \"\"\"\n",
    "    seq_len, d_model = X.shape\n",
    "    d_k = d_model // n_heads\n",
    "    \n",
    "    # 1. Project all heads at once\n",
    "    Q = X @ W_Q  # (seq_len, d_model)\n",
    "    K = X @ W_K\n",
    "    V = X @ W_V\n",
    "    \n",
    "    # 2. Reshape to (n_heads, seq_len, d_k) — split d_model into heads\n",
    "    Q = Q.view(seq_len, n_heads, d_k).transpose(0, 1)  # (n_heads, seq_len, d_k)\n",
    "    K = K.view(seq_len, n_heads, d_k).transpose(0, 1)\n",
    "    V = V.view(seq_len, n_heads, d_k).transpose(0, 1)\n",
    "    \n",
    "    # 3. Scaled dot-product attention (batched over heads)\n",
    "    scores = Q @ K.transpose(-2, -1) / math.sqrt(d_k)  # (n_heads, seq_len, seq_len)\n",
    "    \n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask.unsqueeze(0), float('-inf'))\n",
    "    \n",
    "    weights = softmax(scores)  # softmax works because we wrote it for dim=-1\n",
    "    attn_out = weights @ V     # (n_heads, seq_len, d_k)\n",
    "    \n",
    "    # 4. Concatenate heads: transpose back and reshape\n",
    "    attn_out = attn_out.transpose(0, 1).contiguous().view(seq_len, d_model)\n",
    "    \n",
    "    # 5. Output projection\n",
    "    output = attn_out @ W_O  # (seq_len, d_model)\n",
    "    \n",
    "    return output, weights\n",
    "\n",
    "\n",
    "# Create packed weight matrices (all heads in one tensor)\n",
    "W_Q_packed = torch.randn(d_model, d_model, device=device) * 0.1\n",
    "W_K_packed = torch.randn(d_model, d_model, device=device) * 0.1\n",
    "W_V_packed = torch.randn(d_model, d_model, device=device) * 0.1\n",
    "W_O_packed = torch.randn(d_model, d_model, device=device) * 0.1\n",
    "\n",
    "mha_output, mha_weights = multi_head_attention(\n",
    "    X, W_Q_packed, W_K_packed, W_V_packed, W_O_packed, n_heads\n",
    ")\n",
    "\n",
    "print(\"MHA output shape:\", mha_output.shape)\n",
    "print(\"Attention weights shape:\", mha_weights.shape, \" — (n_heads, seq_len, seq_len)\")\n",
    "print(\"\\nHead 0 weights:\")\n",
    "print(mha_weights[0])\n",
    "print(\"\\nHead 1 weights:\")\n",
    "print(mha_weights[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualizing Attention Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [\"The\", \"cat\", \"sat\", \"down\"]\n",
    "\n",
    "fig, axes = plt.subplots(1, n_heads, figsize=(5 * n_heads, 4))\n",
    "if n_heads == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    w = mha_weights[i].detach().cpu().numpy()\n",
    "    im = ax.imshow(w, cmap='Blues', vmin=0, vmax=1)\n",
    "    ax.set_xticks(range(len(tokens)))\n",
    "    ax.set_yticks(range(len(tokens)))\n",
    "    ax.set_xticklabels(tokens)\n",
    "    ax.set_yticklabels(tokens)\n",
    "    ax.set_xlabel(\"Key (attending to)\")\n",
    "    ax.set_ylabel(\"Query (token)\")\n",
    "    ax.set_title(f\"Head {i}\")\n",
    "    \n",
    "    # Annotate cells with values\n",
    "    for row in range(len(tokens)):\n",
    "        for col in range(len(tokens)):\n",
    "            ax.text(col, row, f\"{w[row, col]:.2f}\",\n",
    "                    ha='center', va='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle(\"Multi-Head Attention Weights\", y=1.02, fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary\n",
    "\n",
    "Everything we built uses only these primitive operations:\n",
    "\n",
    "| Operation | PyTorch op | Purpose |\n",
    "|-----------|-----------|----------|\n",
    "| Matrix multiply | `@` / `torch.matmul` | Project Q, K, V; compute scores; weighted sum |\n",
    "| Division | `/` | Scale scores by √d_k |\n",
    "| Exp | `torch.exp` | Part of softmax |\n",
    "| Sum | `.sum()` | Part of softmax |\n",
    "| Max | `.max()` | Numerical stability in softmax |\n",
    "| Masked fill | `.masked_fill()` | Causal masking |\n",
    "| Reshape | `.view()`, `.transpose()` | Splitting/merging heads |\n",
    "\n",
    "No `nn.Module`, no `nn.Linear`, no `nn.MultiheadAttention` — just tensors and math."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
