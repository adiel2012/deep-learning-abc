{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Variants: MQA, GQA & Sliding Window\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/adiel2012/deep-learning-abc/blob/main/attention_variants.ipynb)\n",
    "\n",
    "This notebook implements from scratch the key attention mechanism variants that improve upon standard Multi-Head Attention (MHA):\n",
    "\n",
    "1. **Multi-Head Attention (MHA)** — the original (baseline)\n",
    "2. **Multi-Query Attention (MQA)** — all heads share one K,V projection\n",
    "3. **Grouped-Query Attention (GQA)** — groups of heads share K,V projections\n",
    "4. **Sliding Window Attention** — each token attends only to a local window\n",
    "\n",
    "We compare memory usage, KV cache sizes, and attention patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Mathematical Foundations\n",
    "\n",
    "### The KV Cache Problem\n",
    "\n",
    "During autoregressive generation (generating one token at a time), the model must store the Key and Value projections for all previous tokens — this is the **KV cache**.\n",
    "\n",
    "For standard Multi-Head Attention:\n",
    "- KV cache per layer = $2 \\times n_{\\text{heads}} \\times \\text{seq\\_len} \\times d_k$\n",
    "- For LLaMA-70B: $2 \\times 64 \\times 8192 \\times 128 = 128$ MB per layer (in FP16)\n",
    "- With 80 layers: **10.2 GB** just for the KV cache!\n",
    "\n",
    "### Multi-Query Attention (MQA)\n",
    "\n",
    "All heads share **one** set of K,V projections, but each head still has its own Q projection:\n",
    "\n",
    "$$Q_h = XW_h^Q, \\quad K = XW^K, \\quad V = XW^V$$\n",
    "\n",
    "KV cache reduced by factor of $n_{\\text{heads}}$.\n",
    "\n",
    "### Grouped-Query Attention (GQA)\n",
    "\n",
    "Compromise: divide heads into $G$ groups. Each group shares one K,V:\n",
    "\n",
    "$$Q_h = XW_h^Q, \\quad K_g = XW_g^K, \\quad V_g = XW_g^V \\quad (\\text{where } g = \\lfloor h \\cdot G / n_{\\text{heads}} \\rfloor)$$\n",
    "\n",
    "KV cache reduced by factor of $n_{\\text{heads}} / G$.\n",
    "\n",
    "| Method | KV heads | KV cache size | Quality |\n",
    "|--------|----------|---------------|--------|\n",
    "| MHA | $n_{\\text{heads}}$ | $1\\times$ | Best |\n",
    "| GQA | $G$ (e.g., 8) | $G/n_{\\text{heads}}$ | Near-MHA |\n",
    "| MQA | 1 | $1/n_{\\text{heads}}$ | Slightly worse |\n",
    "\n",
    "### Sliding Window Attention\n",
    "\n",
    "Instead of attending to all positions, each token attends only to the nearest $w$ tokens:\n",
    "\n",
    "$$\\text{mask}_{ij} = \\begin{cases} 0 & \\text{if } |i - j| \\leq w/2 \\\\ -\\infty & \\text{otherwise} \\end{cases}$$\n",
    "\n",
    "Complexity drops from $O(n^2)$ to $O(n \\cdot w)$. With stacked layers, the effective receptive field grows: layer $L$ can \"see\" $L \\times w$ tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Multi-Head Attention (MHA) — Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_head_attention(X, W_Q, W_K, W_V, W_O, n_heads):\n",
    "    \"\"\"Standard Multi-Head Attention.\n",
    "    \n",
    "    Each head has its own Q, K, V projections.\n",
    "    \n",
    "    Args:\n",
    "        X: (batch, seq_len, d_model)\n",
    "        W_Q, W_K, W_V: (d_model, d_model) — projections for all heads concatenated\n",
    "        W_O: (d_model, d_model) — output projection\n",
    "        n_heads: number of attention heads\n",
    "    \"\"\"\n",
    "    batch, seq_len, d_model = X.shape\n",
    "    d_k = d_model // n_heads\n",
    "    \n",
    "    # Project Q, K, V\n",
    "    Q = X @ W_Q  # (batch, seq_len, d_model)\n",
    "    K = X @ W_K\n",
    "    V = X @ W_V\n",
    "    \n",
    "    # Reshape to (batch, n_heads, seq_len, d_k)\n",
    "    Q = Q.view(batch, seq_len, n_heads, d_k).transpose(1, 2)\n",
    "    K = K.view(batch, seq_len, n_heads, d_k).transpose(1, 2)\n",
    "    V = V.view(batch, seq_len, n_heads, d_k).transpose(1, 2)\n",
    "    \n",
    "    # Scaled dot-product attention\n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    weights = torch.softmax(scores, dim=-1)\n",
    "    attn_out = torch.matmul(weights, V)  # (batch, n_heads, seq_len, d_k)\n",
    "    \n",
    "    # Concatenate heads and project\n",
    "    attn_out = attn_out.transpose(1, 2).contiguous().view(batch, seq_len, d_model)\n",
    "    output = attn_out @ W_O\n",
    "    \n",
    "    return output, weights\n",
    "\n",
    "# Test\n",
    "torch.manual_seed(42)\n",
    "batch, seq_len, d_model, n_heads = 2, 8, 32, 4\n",
    "d_k = d_model // n_heads\n",
    "\n",
    "X = torch.randn(batch, seq_len, d_model, device=device)\n",
    "W_Q = torch.randn(d_model, d_model, device=device) * 0.1\n",
    "W_K = torch.randn(d_model, d_model, device=device) * 0.1\n",
    "W_V = torch.randn(d_model, d_model, device=device) * 0.1\n",
    "W_O = torch.randn(d_model, d_model, device=device) * 0.1\n",
    "\n",
    "out_mha, w_mha = multi_head_attention(X, W_Q, W_K, W_V, W_O, n_heads)\n",
    "print('MHA output shape:', out_mha.shape)\n",
    "print('MHA weights shape:', w_mha.shape)\n",
    "\n",
    "# KV cache size\n",
    "kv_cache_mha = 2 * n_heads * seq_len * d_k\n",
    "print(f'\\nKV cache entries per layer: {kv_cache_mha}')\n",
    "print(f'  = 2 (K+V) x {n_heads} heads x {seq_len} tokens x {d_k} dims')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Multi-Query Attention (MQA)\n",
    "\n",
    "All heads share **one** K and V projection. Each head still has its own Q."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_query_attention(X, W_Q, W_K, W_V, W_O, n_heads):\n",
    "    \"\"\"Multi-Query Attention: all heads share one K,V.\n",
    "    \n",
    "    Args:\n",
    "        X: (batch, seq_len, d_model)\n",
    "        W_Q: (d_model, d_model) — per-head Q projections concatenated\n",
    "        W_K: (d_model, d_k) — SINGLE shared K projection\n",
    "        W_V: (d_model, d_k) — SINGLE shared V projection\n",
    "        W_O: (d_model, d_model) — output projection\n",
    "    \"\"\"\n",
    "    batch, seq_len, d_model = X.shape\n",
    "    d_k = d_model // n_heads\n",
    "    \n",
    "    # Q: each head has its own projection\n",
    "    Q = X @ W_Q  # (batch, seq_len, d_model)\n",
    "    Q = Q.view(batch, seq_len, n_heads, d_k).transpose(1, 2)  # (batch, n_heads, seq_len, d_k)\n",
    "    \n",
    "    # K, V: SHARED across all heads\n",
    "    K = X @ W_K  # (batch, seq_len, d_k)\n",
    "    V = X @ W_V  # (batch, seq_len, d_k)\n",
    "    \n",
    "    # Expand K,V to match Q's head dimension for broadcasting\n",
    "    K = K.unsqueeze(1)  # (batch, 1, seq_len, d_k) — broadcasts over n_heads\n",
    "    V = V.unsqueeze(1)  # (batch, 1, seq_len, d_k)\n",
    "    \n",
    "    # Attention\n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    weights = torch.softmax(scores, dim=-1)\n",
    "    attn_out = torch.matmul(weights, V)  # (batch, n_heads, seq_len, d_k)\n",
    "    \n",
    "    # Concatenate and project\n",
    "    attn_out = attn_out.transpose(1, 2).contiguous().view(batch, seq_len, d_model)\n",
    "    output = attn_out @ W_O\n",
    "    \n",
    "    return output, weights\n",
    "\n",
    "# Test MQA\n",
    "torch.manual_seed(42)\n",
    "W_Q_mqa = torch.randn(d_model, d_model, device=device) * 0.1\n",
    "W_K_mqa = torch.randn(d_model, d_k, device=device) * 0.1  # Only d_k output dims!\n",
    "W_V_mqa = torch.randn(d_model, d_k, device=device) * 0.1\n",
    "W_O_mqa = torch.randn(d_model, d_model, device=device) * 0.1\n",
    "\n",
    "out_mqa, w_mqa = multi_query_attention(X, W_Q_mqa, W_K_mqa, W_V_mqa, W_O_mqa, n_heads)\n",
    "print('MQA output shape:', out_mqa.shape)\n",
    "print('MQA weights shape:', w_mqa.shape)\n",
    "\n",
    "# KV cache size\n",
    "kv_cache_mqa = 2 * 1 * seq_len * d_k  # Only 1 KV head\n",
    "print(f'\\nKV cache entries per layer: {kv_cache_mqa}')\n",
    "print(f'  = 2 (K+V) x 1 KV head x {seq_len} tokens x {d_k} dims')\n",
    "print(f'  → {n_heads}x smaller than MHA!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Grouped-Query Attention (GQA)\n",
    "\n",
    "GQA is the sweet spot: groups of Q heads share K,V projections. With $G$ groups and $H$ heads, each group has $H/G$ query heads sharing one K,V pair.\n",
    "\n",
    "- $G = H$ → Standard MHA\n",
    "- $G = 1$ → MQA\n",
    "- $1 < G < H$ → GQA (the useful range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grouped_query_attention(X, W_Q, W_K, W_V, W_O, n_heads, n_kv_groups):\n",
    "    \"\"\"Grouped-Query Attention.\n",
    "    \n",
    "    Args:\n",
    "        X: (batch, seq_len, d_model)\n",
    "        W_Q: (d_model, d_model) — all Q heads\n",
    "        W_K: (d_model, n_kv_groups * d_k) — KV for each group\n",
    "        W_V: (d_model, n_kv_groups * d_k)\n",
    "        W_O: (d_model, d_model)\n",
    "        n_heads: number of Q heads\n",
    "        n_kv_groups: number of KV groups\n",
    "    \"\"\"\n",
    "    batch, seq_len, d_model = X.shape\n",
    "    d_k = d_model // n_heads\n",
    "    heads_per_group = n_heads // n_kv_groups\n",
    "    \n",
    "    # Q: per-head projections\n",
    "    Q = X @ W_Q  # (batch, seq_len, d_model)\n",
    "    Q = Q.view(batch, seq_len, n_heads, d_k).transpose(1, 2)  # (batch, n_heads, seq_len, d_k)\n",
    "    \n",
    "    # K, V: per-group projections\n",
    "    K = X @ W_K  # (batch, seq_len, n_kv_groups * d_k)\n",
    "    V = X @ W_V\n",
    "    K = K.view(batch, seq_len, n_kv_groups, d_k).transpose(1, 2)  # (batch, n_kv_groups, seq_len, d_k)\n",
    "    V = V.view(batch, seq_len, n_kv_groups, d_k).transpose(1, 2)\n",
    "    \n",
    "    # Repeat K,V for each head in the group\n",
    "    # (batch, n_kv_groups, seq_len, d_k) → (batch, n_heads, seq_len, d_k)\n",
    "    K = K.repeat_interleave(heads_per_group, dim=1)\n",
    "    V = V.repeat_interleave(heads_per_group, dim=1)\n",
    "    \n",
    "    # Standard attention\n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    weights = torch.softmax(scores, dim=-1)\n",
    "    attn_out = torch.matmul(weights, V)\n",
    "    \n",
    "    # Concatenate and project\n",
    "    attn_out = attn_out.transpose(1, 2).contiguous().view(batch, seq_len, d_model)\n",
    "    output = attn_out @ W_O\n",
    "    \n",
    "    return output, weights\n",
    "\n",
    "# Test GQA with 8 heads and 2 KV groups\n",
    "torch.manual_seed(42)\n",
    "n_heads_gqa = 8\n",
    "n_kv_groups = 2\n",
    "d_model_gqa = 64\n",
    "d_k_gqa = d_model_gqa // n_heads_gqa  # 8\n",
    "\n",
    "X_gqa = torch.randn(2, 8, d_model_gqa, device=device)\n",
    "W_Q_gqa = torch.randn(d_model_gqa, d_model_gqa, device=device) * 0.1\n",
    "W_K_gqa = torch.randn(d_model_gqa, n_kv_groups * d_k_gqa, device=device) * 0.1\n",
    "W_V_gqa = torch.randn(d_model_gqa, n_kv_groups * d_k_gqa, device=device) * 0.1\n",
    "W_O_gqa = torch.randn(d_model_gqa, d_model_gqa, device=device) * 0.1\n",
    "\n",
    "out_gqa, w_gqa = grouped_query_attention(X_gqa, W_Q_gqa, W_K_gqa, W_V_gqa, W_O_gqa, n_heads_gqa, n_kv_groups)\n",
    "print('GQA output shape:', out_gqa.shape)\n",
    "print('GQA weights shape:', w_gqa.shape)\n",
    "print(f'\\n{n_heads_gqa} Q heads, {n_kv_groups} KV groups')\n",
    "print(f'  → Heads 0-3 share KV group 0')\n",
    "print(f'  → Heads 4-7 share KV group 1')\n",
    "\n",
    "# KV cache\n",
    "kv_cache_gqa = 2 * n_kv_groups * seq_len * d_k_gqa\n",
    "kv_cache_mha_full = 2 * n_heads_gqa * seq_len * d_k_gqa\n",
    "print(f'\\nKV cache: {kv_cache_gqa} entries ({n_heads_gqa // n_kv_groups}x smaller than MHA)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify: heads in same group produce identical K,V but different Q\n",
    "# This means their attention patterns differ even though they read the same values\n",
    "print('Attention weights comparison within GQA groups:')\n",
    "print(f'  Heads 0 and 1 (same group): weight diff = {(w_gqa[0,0] - w_gqa[0,1]).abs().mean():.6f}')\n",
    "print(f'  Heads 0 and 4 (diff groups): weight diff = {(w_gqa[0,0] - w_gqa[0,4]).abs().mean():.6f}')\n",
    "print('\\n→ Same group = same K,V but different Q → different attention patterns')\n",
    "print('→ Different groups = different K,V AND different Q → more diverse patterns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Sliding Window Attention\n",
    "\n",
    "Instead of attending to all tokens, each token only attends to a fixed window of nearby tokens. This reduces complexity from $O(n^2)$ to $O(n \\cdot w)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window_mask(seq_len, window_size, device=None):\n",
    "    \"\"\"Create a sliding window attention mask.\n",
    "    \n",
    "    Tokens can only attend to positions within window_size distance.\n",
    "    Combined with causal masking (can't attend to future).\n",
    "    \n",
    "    Returns:\n",
    "        mask: (seq_len, seq_len) — True where attention is BLOCKED\n",
    "    \"\"\"\n",
    "    # Distance matrix\n",
    "    positions = torch.arange(seq_len, device=device)\n",
    "    dist = (positions.unsqueeze(0) - positions.unsqueeze(1))  # (seq_len, seq_len)\n",
    "    \n",
    "    # Block: future tokens (causal) OR too-distant past tokens (window)\n",
    "    causal_mask = dist < 0  # future positions\n",
    "    window_mask = dist > window_size  # too far in the past\n",
    "    mask = causal_mask | window_mask\n",
    "    \n",
    "    return mask\n",
    "\n",
    "def sliding_window_attention(X, W_Q, W_K, W_V, W_O, n_heads, window_size):\n",
    "    \"\"\"Multi-head attention with sliding window mask.\"\"\"\n",
    "    batch, seq_len, d_model = X.shape\n",
    "    d_k = d_model // n_heads\n",
    "    \n",
    "    Q = (X @ W_Q).view(batch, seq_len, n_heads, d_k).transpose(1, 2)\n",
    "    K = (X @ W_K).view(batch, seq_len, n_heads, d_k).transpose(1, 2)\n",
    "    V = (X @ W_V).view(batch, seq_len, n_heads, d_k).transpose(1, 2)\n",
    "    \n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    \n",
    "    # Apply sliding window + causal mask\n",
    "    mask = sliding_window_mask(seq_len, window_size, device=X.device)\n",
    "    scores = scores.masked_fill(mask.unsqueeze(0).unsqueeze(0), float('-inf'))\n",
    "    \n",
    "    weights = torch.softmax(scores, dim=-1)\n",
    "    attn_out = torch.matmul(weights, V)\n",
    "    \n",
    "    attn_out = attn_out.transpose(1, 2).contiguous().view(batch, seq_len, d_model)\n",
    "    output = attn_out @ W_O\n",
    "    \n",
    "    return output, weights\n",
    "\n",
    "# Test\n",
    "torch.manual_seed(42)\n",
    "seq_len_sw = 16\n",
    "window_size = 4\n",
    "X_sw = torch.randn(1, seq_len_sw, d_model, device=device)\n",
    "\n",
    "out_sw, w_sw = sliding_window_attention(X_sw, W_Q, W_K, W_V, W_O, n_heads, window_size)\n",
    "print(f'Sliding Window Attention (window={window_size}):')\n",
    "print(f'  Output shape: {out_sw.shape}')\n",
    "print(f'  Attention weights shape: {w_sw.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize masks and attention patterns\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Full causal mask\n",
    "causal = torch.triu(torch.ones(seq_len_sw, seq_len_sw), diagonal=1).bool()\n",
    "axes[0].imshow(~causal.cpu().numpy(), cmap='Blues')\n",
    "axes[0].set_title(f'Full Causal Mask\\n(all past positions visible)')\n",
    "axes[0].set_xlabel('Key position')\n",
    "axes[0].set_ylabel('Query position')\n",
    "\n",
    "# Sliding window mask\n",
    "sw_mask = sliding_window_mask(seq_len_sw, window_size, device=device)\n",
    "axes[1].imshow(~sw_mask.cpu().numpy(), cmap='Blues')\n",
    "axes[1].set_title(f'Sliding Window Mask (w={window_size})\\n(only nearby past visible)')\n",
    "axes[1].set_xlabel('Key position')\n",
    "axes[1].set_ylabel('Query position')\n",
    "\n",
    "# Actual attention weights\n",
    "axes[2].imshow(w_sw[0, 0].detach().cpu().numpy(), cmap='Blues')\n",
    "axes[2].set_title(f'Attention Weights (head 0)\\n(sliding window, w={window_size})')\n",
    "axes[2].set_xlabel('Key position')\n",
    "axes[2].set_ylabel('Query position')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effective Receptive Field\n",
    "\n",
    "With sliding window attention, stacking $L$ layers gives an effective receptive field of $L \\times w$ tokens. Information propagates through the network layer by layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize how receptive field grows with layers\n",
    "def compute_receptive_field(n_layers, window_size, seq_len):\n",
    "    \"\"\"Simulate information flow through stacked sliding window layers.\"\"\"\n",
    "    # Start: token at position (seq_len-1) can only see itself\n",
    "    visible = torch.zeros(n_layers + 1, seq_len)\n",
    "    visible[0, seq_len - 1] = 1.0  # token we're tracking\n",
    "    \n",
    "    for layer in range(n_layers):\n",
    "        prev = visible[layer]\n",
    "        new = prev.clone()\n",
    "        for pos in range(seq_len):\n",
    "            if prev[pos] > 0:\n",
    "                # This position can see window_size tokens back\n",
    "                start = max(0, pos - window_size)\n",
    "                new[start:pos + 1] = 1.0\n",
    "        visible[layer + 1] = new\n",
    "    \n",
    "    return visible\n",
    "\n",
    "n_layers = 4\n",
    "window_size = 4\n",
    "vis_seq_len = 32\n",
    "receptive = compute_receptive_field(n_layers, window_size, vis_seq_len)\n",
    "\n",
    "fig, axes = plt.subplots(1, n_layers + 1, figsize=(20, 3))\n",
    "for layer in range(n_layers + 1):\n",
    "    axes[layer].imshow(receptive[layer].unsqueeze(0).numpy(), cmap='Blues', aspect='auto')\n",
    "    visible_count = int(receptive[layer].sum().item())\n",
    "    if layer == 0:\n",
    "        axes[layer].set_title(f'Input\\n({visible_count} pos visible)')\n",
    "    else:\n",
    "        axes[layer].set_title(f'After Layer {layer}\\n({visible_count} pos visible)')\n",
    "    axes[layer].set_yticks([])\n",
    "    axes[layer].set_xlabel('Position')\n",
    "\n",
    "plt.suptitle(f'Receptive Field Growth: window={window_size}, effective = layers × window', fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f'Window size: {window_size}')\n",
    "for layer in range(1, n_layers + 1):\n",
    "    print(f'After {layer} layer(s): can see {int(receptive[layer].sum().item())} of {vis_seq_len} positions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comprehensive Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter count and KV cache comparison\n",
    "d_model = 64\n",
    "n_heads = 8\n",
    "d_k = d_model // n_heads\n",
    "\n",
    "print('=' * 75)\n",
    "print('PARAMETER & MEMORY COMPARISON')\n",
    "print(f'd_model={d_model}, n_heads={n_heads}, d_k={d_k}')\n",
    "print('=' * 75)\n",
    "\n",
    "configs = [\n",
    "    ('MHA', n_heads, n_heads),\n",
    "    ('GQA (4 groups)', n_heads, 4),\n",
    "    ('GQA (2 groups)', n_heads, 2),\n",
    "    ('MQA', n_heads, 1),\n",
    "]\n",
    "\n",
    "print(f'{\"Method\":<20} {\"Q params\":<12} {\"KV params\":<12} {\"Total\":<12} {\"KV cache/token\":<15}')\n",
    "print('-' * 75)\n",
    "\n",
    "for name, n_q, n_kv in configs:\n",
    "    q_params = d_model * (n_q * d_k)  # Q projection\n",
    "    kv_params = d_model * (n_kv * d_k) * 2  # K and V projections\n",
    "    total = q_params + kv_params + d_model * d_model  # + output projection\n",
    "    kv_cache = 2 * n_kv * d_k  # per token\n",
    "    print(f'{name:<20} {q_params:<12} {kv_params:<12} {total:<12} {kv_cache:<15}')\n",
    "\n",
    "print('-' * 75)\n",
    "print(f'\\nMQA saves {n_heads}x KV cache vs MHA')\n",
    "print(f'GQA(2) saves {n_heads//2}x KV cache vs MHA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize KV cache scaling with sequence length\n",
    "seq_lengths = [512, 1024, 2048, 4096, 8192, 16384, 32768]\n",
    "d_model_real = 4096  # realistic LLM size\n",
    "n_heads_real = 32\n",
    "d_k_real = d_model_real // n_heads_real\n",
    "n_layers = 32\n",
    "bytes_per_param = 2  # FP16\n",
    "\n",
    "def kv_cache_gb(seq_len, n_kv_heads):\n",
    "    return 2 * n_kv_heads * seq_len * d_k_real * n_layers * bytes_per_param / (1024**3)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "methods_real = [\n",
    "    ('MHA (32 KV heads)', 32, 'C0'),\n",
    "    ('GQA (8 KV groups)', 8, 'C1'),\n",
    "    ('GQA (4 KV groups)', 4, 'C2'),\n",
    "    ('MQA (1 KV head)', 1, 'C3'),\n",
    "]\n",
    "\n",
    "for name, n_kv, color in methods_real:\n",
    "    cache_sizes = [kv_cache_gb(sl, n_kv) for sl in seq_lengths]\n",
    "    ax.plot(seq_lengths, cache_sizes, 'o-', linewidth=2, markersize=6, label=name, color=color)\n",
    "\n",
    "ax.set_xlabel('Sequence Length')\n",
    "ax.set_ylabel('KV Cache Size (GB)')\n",
    "ax.set_title(f'KV Cache Memory vs Sequence Length\\n(d_model={d_model_real}, {n_layers} layers, FP16)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xscale('log', base=2)\n",
    "ax.set_yscale('log', base=2)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complexity comparison for sliding window\n",
    "seq_lengths_complexity = list(range(64, 2049, 64))\n",
    "window_size = 256\n",
    "\n",
    "full_attn_ops = [s * s for s in seq_lengths_complexity]  # O(n^2)\n",
    "window_attn_ops = [s * min(s, window_size) for s in seq_lengths_complexity]  # O(n*w)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.plot(seq_lengths_complexity, full_attn_ops, linewidth=2, label='Full Attention O(n²)')\n",
    "ax.plot(seq_lengths_complexity, window_attn_ops, linewidth=2, label=f'Sliding Window O(n·w), w={window_size}')\n",
    "ax.set_xlabel('Sequence Length (n)')\n",
    "ax.set_ylabel('Operations (proportional)')\n",
    "ax.set_title('Attention Complexity: Full vs Sliding Window')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f'At seq_len=2048 with window={window_size}:')\n",
    "print(f'  Full attention: {2048*2048:,} ops')\n",
    "print(f'  Sliding window: {2048*window_size:,} ops')\n",
    "print(f'  → {2048*2048 / (2048*window_size):.1f}x reduction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary table\n",
    "print('=' * 90)\n",
    "print('COMPARISON: Attention Variants')\n",
    "print('=' * 90)\n",
    "print(f'{\"Property\":<25} {\"MHA\":<16} {\"MQA\":<16} {\"GQA\":<16} {\"Sliding Window\":<16}')\n",
    "print('-' * 90)\n",
    "print(f'{\"KV heads\":<25} {\"H\":<16} {\"1\":<16} {\"G (1<G<H)\":<16} {\"H\":<16}')\n",
    "print(f'{\"KV cache\":<25} {\"2·H·n·d_k\":<16} {\"2·n·d_k\":<16} {\"2·G·n·d_k\":<16} {\"2·H·w·d_k\":<16}')\n",
    "print(f'{\"Complexity\":<25} {\"O(n²)\":<16} {\"O(n²)\":<16} {\"O(n²)\":<16} {\"O(n·w)\":<16}')\n",
    "print(f'{\"Quality\":<25} {\"Best\":<16} {\"Slightly less\":<16} {\"Near-MHA\":<16} {\"Good (local)\":<16}')\n",
    "print(f'{\"Speed (inference)\":<25} {\"1x\":<16} {\"4-8x\":<16} {\"~2-4x\":<16} {\"n/w x\":<16}')\n",
    "print(f'{\"Used in\":<25} {\"Original TF\":<16} {\"PaLM,Falcon\":<16} {\"LLaMA2,Mistral\":<16} {\"Mistral\":<16}')\n",
    "print('=' * 90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook we implemented from scratch:\n",
    "\n",
    "1. **MHA (Multi-Head Attention)** — each head has its own Q, K, V projections. Full quality, full KV cache cost.\n",
    "\n",
    "2. **MQA (Multi-Query Attention)** — all heads share one K, V. Reduces KV cache by $H\\times$, slightly lower quality.\n",
    "\n",
    "3. **GQA (Grouped-Query Attention)** — groups of heads share K, V. The Goldilocks solution: 90-95% of MHA quality with large KV cache savings. **Current best practice** (LLaMA 2/3, Mistral, Gemma).\n",
    "\n",
    "4. **Sliding Window Attention** — each token attends only to $w$ nearby tokens. $O(n \\cdot w)$ instead of $O(n^2)$. Stacking $L$ layers gives $L \\times w$ effective receptive field.\n",
    "\n",
    "**Key insight:** Modern production models often combine these — e.g., Mistral 7B uses both GQA and sliding window attention. The trend is toward maximizing quality per byte of KV cache memory."
   ]
  }
 ]
}