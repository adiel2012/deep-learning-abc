{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Mechanism with PyTorch Modules\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/adiel2012/deep-learning-abc/blob/main/attention_with_pytorch.ipynb)\n",
    "\n",
    "This notebook implements the same attention mechanism as `attention_from_scratch.ipynb`, but using **PyTorch's `nn.Module` and `nn.Linear`** instead of raw tensor ops.\n",
    "\n",
    "> **Companion notebook:** [attention_from_scratch.ipynb](https://colab.research.google.com/github/adiel2012/deep-learning-abc/blob/main/attention_from_scratch.ipynb) — same content using only raw tensor ops + math foundations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (Colab already has torch, but this ensures compatibility)\n",
    "!pip install torch matplotlib -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Use GPU if available (Colab: Runtime > Change runtime type > GPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup: Token Embeddings via `nn.Embedding`\n",
    "\n",
    "Instead of random tensors, we use PyTorch's embedding layer to map token indices to dense vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50\n",
    "seq_len = 4\n",
    "d_model = 8\n",
    "\n",
    "embedding = nn.Embedding(vocab_size, d_model).to(device)\n",
    "\n",
    "# Simulate token indices for [\"The\", \"cat\", \"sat\", \"down\"]\n",
    "token_ids = torch.tensor([5, 12, 31, 7], device=device)\n",
    "X = embedding(token_ids)  # (seq_len, d_model)\n",
    "\n",
    "print(\"Token IDs:\", token_ids)\n",
    "print(\"Embeddings shape:\", X.shape)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Scaled Dot-Product Attention\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\!\\left(\\frac{Q K^T}{\\sqrt{d_k}}\\right) V$$\n",
    "\n",
    "We use `nn.Linear` for the Q, K, V projections and `F.softmax` for the softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, d_model, d_k, d_v):\n",
    "        super().__init__()\n",
    "        self.W_Q = nn.Linear(d_model, d_k, bias=False)\n",
    "        self.W_K = nn.Linear(d_model, d_k, bias=False)\n",
    "        self.W_V = nn.Linear(d_model, d_v, bias=False)\n",
    "        self.scale = math.sqrt(d_k)\n",
    "\n",
    "    def forward(self, X, mask=None):\n",
    "        Q = self.W_Q(X)  # (seq_len, d_k)\n",
    "        K = self.W_K(X)  # (seq_len, d_k)\n",
    "        V = self.W_V(X)  # (seq_len, d_v)\n",
    "\n",
    "        scores = Q @ K.T / self.scale  # (seq_len, seq_len)\n",
    "\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask, float('-inf'))\n",
    "\n",
    "        weights = F.softmax(scores, dim=-1)\n",
    "        output = weights @ V  # (seq_len, d_v)\n",
    "\n",
    "        return output, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_k = 6\n",
    "d_v = 6\n",
    "\n",
    "attn = ScaledDotProductAttention(d_model, d_k, d_v).to(device)\n",
    "output, weights = attn(X)\n",
    "\n",
    "print(\"Output shape:\", output.shape)\n",
    "print(\"Attention weights (each row sums to 1):\")\n",
    "print(weights)\n",
    "print(\"Row sums:\", weights.sum(dim=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key difference from the from-scratch version\n",
    "\n",
    "| From scratch | With modules |\n",
    "|---|---|\n",
    "| `W_Q = torch.randn(d_model, d_k) * 0.1` | `self.W_Q = nn.Linear(d_model, d_k, bias=False)` |\n",
    "| `Q = X @ W_Q` | `Q = self.W_Q(X)` |\n",
    "| Custom `softmax()` function | `F.softmax(scores, dim=-1)` |\n",
    "\n",
    "`nn.Linear` manages the weight tensor internally, handles initialization, and registers it as a trainable parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Causal (Decoder) Mask\n",
    "\n",
    "Same concept — block future tokens so token $i$ only attends to positions $0 \\ldots i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "causal_mask = torch.triu(torch.ones(seq_len, seq_len, dtype=torch.bool, device=device), diagonal=1)\n",
    "print(\"Causal mask (True = blocked):\")\n",
    "print(causal_mask.int())\n",
    "\n",
    "causal_output, causal_weights = attn(X, mask=causal_mask)\n",
    "print(\"\\nCausal attention weights:\")\n",
    "print(causal_weights)\n",
    "print(\"\\nUpper triangle is 0 — no attending to future tokens.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Multi-Head Attention with `nn.Module`\n",
    "\n",
    "$$\\text{MultiHead}(X) = \\text{Concat}(\\text{head}_1, \\dots, \\text{head}_h) \\cdot W_O$$\n",
    "\n",
    "We pack all head projections into single `nn.Linear` layers and use reshaping to split/merge heads — same efficient approach as the from-scratch version, but with cleaner code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
    "\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads\n",
    "\n",
    "        # All heads packed into single projections\n",
    "        self.W_Q = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_K = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_V = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_O = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "    def forward(self, X, mask=None):\n",
    "        seq_len = X.shape[0]\n",
    "\n",
    "        # 1. Project\n",
    "        Q = self.W_Q(X)  # (seq_len, d_model)\n",
    "        K = self.W_K(X)\n",
    "        V = self.W_V(X)\n",
    "\n",
    "        # 2. Split into heads: (seq_len, d_model) -> (n_heads, seq_len, d_k)\n",
    "        Q = Q.view(seq_len, self.n_heads, self.d_k).transpose(0, 1)\n",
    "        K = K.view(seq_len, self.n_heads, self.d_k).transpose(0, 1)\n",
    "        V = V.view(seq_len, self.n_heads, self.d_k).transpose(0, 1)\n",
    "\n",
    "        # 3. Attention\n",
    "        scores = Q @ K.transpose(-2, -1) / math.sqrt(self.d_k)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask.unsqueeze(0), float('-inf'))\n",
    "        weights = F.softmax(scores, dim=-1)  # (n_heads, seq_len, seq_len)\n",
    "        attn_out = weights @ V               # (n_heads, seq_len, d_k)\n",
    "\n",
    "        # 4. Merge heads: (n_heads, seq_len, d_k) -> (seq_len, d_model)\n",
    "        attn_out = attn_out.transpose(0, 1).contiguous().view(seq_len, -1)\n",
    "\n",
    "        # 5. Output projection\n",
    "        output = self.W_O(attn_out)\n",
    "\n",
    "        return output, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_heads = 2\n",
    "mha = MultiHeadAttention(d_model, n_heads).to(device)\n",
    "\n",
    "mha_output, mha_weights = mha(X)\n",
    "\n",
    "print(\"MHA output shape:\", mha_output.shape)\n",
    "print(\"Weights shape:\", mha_weights.shape, \"— (n_heads, seq_len, seq_len)\")\n",
    "print(\"\\nHead 0 weights:\")\n",
    "print(mha_weights[0])\n",
    "print(\"\\nHead 1 weights:\")\n",
    "print(mha_weights[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Inspecting Parameters\n",
    "\n",
    "A major advantage of `nn.Module`: all learnable parameters are automatically tracked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Trainable parameters in MultiHeadAttention:\")\n",
    "total = 0\n",
    "for name, param in mha.named_parameters():\n",
    "    print(f\"  {name:12s}  shape={str(list(param.shape)):16s}  params={param.numel()}\")\n",
    "    total += param.numel()\n",
    "print(f\"  {'Total':12s}  {'':{16}}  params={total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Loop Example\n",
    "\n",
    "With `nn.Module`, we can plug attention into a gradient-based training loop. Here's a toy example that trains the attention layer to copy its input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toy task: train attention to reconstruct X from X\n",
    "# Detach X so the embedding graph doesn't interfere with the training loop\n",
    "X_target = X.detach()\n",
    "\n",
    "mha_train = MultiHeadAttention(d_model, n_heads).to(device)\n",
    "optimizer = torch.optim.Adam(mha_train.parameters(), lr=0.01)\n",
    "\n",
    "losses = []\n",
    "for step in range(200):\n",
    "    optimizer.zero_grad()\n",
    "    out, _ = mha_train(X_target)\n",
    "    loss = F.mse_loss(out, X_target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    losses.append(loss.item())\n",
    "\n",
    "print(f\"Initial loss: {losses[0]:.4f}\")\n",
    "print(f\"Final loss:   {losses[-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 3))\n",
    "plt.plot(losses)\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"MSE Loss\")\n",
    "plt.title(\"Training: Attention learns to copy input\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualizing Learned Attention Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [\"The\", \"cat\", \"sat\", \"down\"]\n",
    "\n",
    "with torch.no_grad():\n",
    "    _, learned_weights = mha_train(X_target)\n",
    "\n",
    "fig, axes = plt.subplots(1, n_heads, figsize=(5 * n_heads, 4))\n",
    "if n_heads == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    w = learned_weights[i].cpu().numpy()\n",
    "    im = ax.imshow(w, cmap='Blues', vmin=0, vmax=1)\n",
    "    ax.set_xticks(range(len(tokens)))\n",
    "    ax.set_yticks(range(len(tokens)))\n",
    "    ax.set_xticklabels(tokens)\n",
    "    ax.set_yticklabels(tokens)\n",
    "    ax.set_xlabel(\"Key (attending to)\")\n",
    "    ax.set_ylabel(\"Query (token)\")\n",
    "    ax.set_title(f\"Head {i} (trained)\")\n",
    "\n",
    "    for row in range(len(tokens)):\n",
    "        for col in range(len(tokens)):\n",
    "            ax.text(col, row, f\"{w[row, col]:.2f}\",\n",
    "                    ha='center', va='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle(\"Learned Attention Weights (after training)\", y=1.02, fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Using PyTorch's Built-in `nn.MultiheadAttention`\n",
    "\n",
    "PyTorch provides a ready-made implementation. Let's compare it to ours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch's built-in expects (seq_len, batch, d_model) for unbatched input\n",
    "builtin_mha = nn.MultiheadAttention(embed_dim=d_model, num_heads=n_heads,\n",
    "                                     bias=False, batch_first=False).to(device)\n",
    "\n",
    "# Add batch dimension: (seq_len, 1, d_model)\n",
    "X_batched = X.unsqueeze(1)\n",
    "\n",
    "with torch.no_grad():\n",
    "    builtin_out, builtin_weights = builtin_mha(X_batched, X_batched, X_batched)\n",
    "\n",
    "print(\"Built-in output shape:\", builtin_out.squeeze(1).shape)\n",
    "print(\"Built-in weights shape:\", builtin_weights.shape)\n",
    "print(\"\\nBuilt-in attention weights:\")\n",
    "print(builtin_weights.squeeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Comparison: Three Levels of Abstraction\n",
    "\n",
    "| Level | Notebook | What you manage | What PyTorch manages |\n",
    "|-------|----------|-----------------|----------------------|\n",
    "| **Low** | `attention_from_scratch.ipynb` | Weight tensors, matmul, softmax, masking, reshaping | Nothing |\n",
    "| **Mid** | This notebook (`MultiHeadAttention`) | Forward logic, reshaping | Weight init, parameter tracking, gradients |\n",
    "| **High** | `nn.MultiheadAttention` | Just call it | Everything |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
