{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT & Family Models from Scratch\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/adiel2012/deep-learning-abc/blob/main/bert_family.ipynb)\n",
    "\n",
    "This notebook implements BERT and its key variants from scratch using raw tensor operations:\n",
    "\n",
    "1. **BERT** — Bidirectional Encoder Representations from Transformers (Devlin et al., 2018)\n",
    "2. **RoBERTa** — Robustly Optimized BERT (Liu et al., 2019)\n",
    "3. **ALBERT** — A Lite BERT with parameter sharing (Lan et al., 2019)\n",
    "4. **DistilBERT** — Knowledge distillation (Sanh et al., 2019)\n",
    "5. **DeBERTa** — Disentangled attention (He et al., 2020)\n",
    "\n",
    "We build each model's distinctive components and compare their design choices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Mathematical Foundations\n",
    "\n",
    "### BERT: Bidirectional Pre-training\n",
    "\n",
    "Unlike GPT (left-to-right), BERT uses **bidirectional** attention — each token can attend to all other tokens in both directions. BERT is pre-trained with two objectives:\n",
    "\n",
    "1. **Masked Language Model (MLM):** Randomly mask 15% of tokens, predict them from context.\n",
    "2. **Next Sentence Prediction (NSP):** Given two sentences, predict if the second follows the first.\n",
    "\n",
    "### Key Architecture Differences\n",
    "\n",
    "| Model | Key Innovation | Parameters |\n",
    "|-------|---------------|------------|\n",
    "| BERT-base | Bidirectional encoder, MLM + NSP | 110M |\n",
    "| RoBERTa | Better training recipe, no NSP | 125M |\n",
    "| ALBERT | Cross-layer parameter sharing + factorized embeddings | 12M |\n",
    "| DistilBERT | Knowledge distillation (6 layers from 12) | 66M |\n",
    "| DeBERTa | Disentangled content + position attention | 140M |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Core Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_norm(x, gamma, beta, eps=1e-5):\n",
    "    mean = x.mean(dim=-1, keepdim=True)\n",
    "    var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "    return gamma * (x - mean) / torch.sqrt(var + eps) + beta\n",
    "\n",
    "def gelu(x):\n",
    "    return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * x ** 3)))\n",
    "\n",
    "def multi_head_attention(Q, K, V, W_Q, W_K, W_V, W_O, n_heads, mask=None):\n",
    "    \"\"\"Standard multi-head attention.\"\"\"\n",
    "    batch, seq_len, d_model = Q.shape\n",
    "    d_k = d_model // n_heads\n",
    "    \n",
    "    Q = (Q @ W_Q).view(batch, seq_len, n_heads, d_k).transpose(1, 2)\n",
    "    K = (K @ W_K).view(batch, -1, n_heads, d_k).transpose(1, 2)\n",
    "    V = (V @ W_V).view(batch, -1, n_heads, d_k).transpose(1, 2)\n",
    "    \n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask.unsqueeze(1) == 0, float('-inf'))\n",
    "    \n",
    "    weights = torch.softmax(scores, dim=-1)\n",
    "    out = torch.matmul(weights, V)\n",
    "    out = out.transpose(1, 2).contiguous().view(batch, seq_len, d_model)\n",
    "    return out @ W_O, weights\n",
    "\n",
    "def ffn(x, W1, b1, W2, b2):\n",
    "    return gelu(x @ W1 + b1) @ W2 + b2\n",
    "\n",
    "print('Core components ready: layer_norm, gelu, multi_head_attention, ffn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. BERT from Scratch\n",
    "\n",
    "BERT = Token Embeddings + Segment Embeddings + Position Embeddings → N Transformer Encoder Layers → Task-specific heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_bert(vocab_size, d_model, n_heads, d_ff, n_layers, max_len, device):\n",
    "    \"\"\"Initialize BERT parameters.\"\"\"\n",
    "    scale = 0.02\n",
    "    params = {\n",
    "        # Embeddings\n",
    "        'token_emb': torch.randn(vocab_size, d_model, device=device) * scale,\n",
    "        'segment_emb': torch.randn(2, d_model, device=device) * scale,  # 2 segments (A, B)\n",
    "        'position_emb': torch.randn(max_len, d_model, device=device) * scale,\n",
    "        'emb_ln_gamma': torch.ones(d_model, device=device),\n",
    "        'emb_ln_beta': torch.zeros(d_model, device=device),\n",
    "        'layers': [],\n",
    "        # MLM head\n",
    "        'mlm_dense_W': torch.randn(d_model, d_model, device=device) * scale,\n",
    "        'mlm_dense_b': torch.zeros(d_model, device=device),\n",
    "        'mlm_ln_gamma': torch.ones(d_model, device=device),\n",
    "        'mlm_ln_beta': torch.zeros(d_model, device=device),\n",
    "        # NSP head\n",
    "        'nsp_W': torch.randn(d_model, 2, device=device) * scale,\n",
    "        'nsp_b': torch.zeros(2, device=device),\n",
    "    }\n",
    "    \n",
    "    for _ in range(n_layers):\n",
    "        layer = {\n",
    "            'W_Q': torch.randn(d_model, d_model, device=device) * scale,\n",
    "            'W_K': torch.randn(d_model, d_model, device=device) * scale,\n",
    "            'W_V': torch.randn(d_model, d_model, device=device) * scale,\n",
    "            'W_O': torch.randn(d_model, d_model, device=device) * scale,\n",
    "            'ln1_gamma': torch.ones(d_model, device=device),\n",
    "            'ln1_beta': torch.zeros(d_model, device=device),\n",
    "            'W1': torch.randn(d_model, d_ff, device=device) * scale,\n",
    "            'b1': torch.zeros(d_ff, device=device),\n",
    "            'W2': torch.randn(d_ff, d_model, device=device) * scale,\n",
    "            'b2': torch.zeros(d_model, device=device),\n",
    "            'ln2_gamma': torch.ones(d_model, device=device),\n",
    "            'ln2_beta': torch.zeros(d_model, device=device),\n",
    "        }\n",
    "        params['layers'].append(layer)\n",
    "    \n",
    "    return params\n",
    "\n",
    "def bert_forward(token_ids, segment_ids, params, n_heads):\n",
    "    \"\"\"BERT forward pass with MLM and NSP outputs.\"\"\"\n",
    "    batch, seq_len = token_ids.shape\n",
    "    \n",
    "    # Embedding: token + segment + position\n",
    "    positions = torch.arange(seq_len, device=token_ids.device)\n",
    "    x = (params['token_emb'][token_ids] +\n",
    "         params['segment_emb'][segment_ids] +\n",
    "         params['position_emb'][positions])\n",
    "    x = layer_norm(x, params['emb_ln_gamma'], params['emb_ln_beta'])\n",
    "    \n",
    "    # Encoder layers (Post-LN, as in original BERT)\n",
    "    all_weights = []\n",
    "    for layer in params['layers']:\n",
    "        # Self-attention\n",
    "        attn_out, w = multi_head_attention(\n",
    "            x, x, x, layer['W_Q'], layer['W_K'], layer['W_V'], layer['W_O'], n_heads\n",
    "        )\n",
    "        x = layer_norm(x + attn_out, layer['ln1_gamma'], layer['ln1_beta'])\n",
    "        all_weights.append(w)\n",
    "        \n",
    "        # FFN\n",
    "        ffn_out = ffn(x, layer['W1'], layer['b1'], layer['W2'], layer['b2'])\n",
    "        x = layer_norm(x + ffn_out, layer['ln2_gamma'], layer['ln2_beta'])\n",
    "    \n",
    "    # MLM head: predict masked tokens\n",
    "    mlm_hidden = gelu(x @ params['mlm_dense_W'] + params['mlm_dense_b'])\n",
    "    mlm_hidden = layer_norm(mlm_hidden, params['mlm_ln_gamma'], params['mlm_ln_beta'])\n",
    "    mlm_logits = mlm_hidden @ params['token_emb'].T  # tied weights\n",
    "    \n",
    "    # NSP head: use [CLS] token (position 0)\n",
    "    cls_output = x[:, 0]  # (batch, d_model)\n",
    "    nsp_logits = cls_output @ params['nsp_W'] + params['nsp_b']\n",
    "    \n",
    "    return mlm_logits, nsp_logits, x, all_weights\n",
    "\n",
    "# Test BERT\n",
    "torch.manual_seed(42)\n",
    "vocab_size, d_model, n_heads, d_ff, n_layers, max_len = 1000, 64, 4, 256, 4, 128\n",
    "\n",
    "bert_params = init_bert(vocab_size, d_model, n_heads, d_ff, n_layers, max_len, device)\n",
    "\n",
    "# Simulated input: [CLS] tokens_A [SEP] tokens_B [SEP]\n",
    "token_ids = torch.randint(0, vocab_size, (2, 12), device=device)\n",
    "segment_ids = torch.tensor([[0,0,0,0,0,0,1,1,1,1,1,1],\n",
    "                             [0,0,0,0,0,0,0,1,1,1,1,1]], device=device)\n",
    "\n",
    "mlm_logits, nsp_logits, hidden, attn_weights = bert_forward(\n",
    "    token_ids, segment_ids, bert_params, n_heads\n",
    ")\n",
    "\n",
    "print('BERT outputs:')\n",
    "print(f'  MLM logits: {mlm_logits.shape} (batch, seq_len, vocab_size)')\n",
    "print(f'  NSP logits: {nsp_logits.shape} (batch, 2)')\n",
    "print(f'  Hidden states: {hidden.shape}')\n",
    "print(f'  Attention weights: {len(attn_weights)} layers x {attn_weights[0].shape}')\n",
    "\n",
    "# Count parameters\n",
    "total_params = (vocab_size * d_model + 2 * d_model + max_len * d_model +  # embeddings\n",
    "                n_layers * (4 * d_model * d_model + d_model * d_ff * 2 + d_ff + d_model + 4 * d_model) +  # layers\n",
    "                d_model * d_model + d_model + 2 * d_model + d_model * 2 + 2)  # heads\n",
    "print(f'\\nTotal parameters: ~{total_params:,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate Masked Language Modeling\n",
    "print('=== Masked Language Modeling Demo ===')\n",
    "print(f'Input tokens: {token_ids[0].cpu().tolist()}')\n",
    "\n",
    "# Mask positions 3 and 7\n",
    "mask_positions = [3, 7]\n",
    "print(f'Masked positions: {mask_positions}')\n",
    "\n",
    "# MLM predictions at masked positions\n",
    "for pos in mask_positions:\n",
    "    probs = torch.softmax(mlm_logits[0, pos], dim=-1)\n",
    "    top5 = probs.topk(5)\n",
    "    print(f'\\nPosition {pos} (true token={token_ids[0, pos].item()}):')\n",
    "    print(f'  Top 5 predictions: {top5.indices.cpu().tolist()}')\n",
    "    print(f'  Probabilities:     {[f\"{p:.4f}\" for p in top5.values.cpu().tolist()]}')\n",
    "\n",
    "# NSP prediction\n",
    "nsp_probs = torch.softmax(nsp_logits, dim=-1)\n",
    "print(f'\\nNSP predictions (0=not-next, 1=is-next):')\n",
    "for b in range(2):\n",
    "    print(f'  Sample {b}: {nsp_probs[b].cpu().tolist()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. RoBERTa: Better Training Recipe\n",
    "\n",
    "RoBERTa uses the **same architecture** as BERT but with improved training:\n",
    "- **No NSP objective** (dropped — found to be unnecessary)\n",
    "- **Dynamic masking** (different mask each epoch, not static)\n",
    "- **Larger batches**, **more data**, longer training\n",
    "- **Byte-level BPE** tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roberta_forward(token_ids, params, n_heads):\n",
    "    \"\"\"RoBERTa forward — same as BERT but NO segment embeddings, NO NSP.\"\"\"\n",
    "    batch, seq_len = token_ids.shape\n",
    "    positions = torch.arange(seq_len, device=token_ids.device)\n",
    "    \n",
    "    # No segment embeddings — all zeros segment\n",
    "    x = (params['token_emb'][token_ids] +\n",
    "         params['position_emb'][positions])\n",
    "    x = layer_norm(x, params['emb_ln_gamma'], params['emb_ln_beta'])\n",
    "    \n",
    "    for layer_p in params['layers']:\n",
    "        attn_out, _ = multi_head_attention(\n",
    "            x, x, x, layer_p['W_Q'], layer_p['W_K'], layer_p['W_V'], layer_p['W_O'], n_heads\n",
    "        )\n",
    "        x = layer_norm(x + attn_out, layer_p['ln1_gamma'], layer_p['ln1_beta'])\n",
    "        ffn_out = ffn(x, layer_p['W1'], layer_p['b1'], layer_p['W2'], layer_p['b2'])\n",
    "        x = layer_norm(x + ffn_out, layer_p['ln2_gamma'], layer_p['ln2_beta'])\n",
    "    \n",
    "    # MLM head only — no NSP\n",
    "    mlm_hidden = gelu(x @ params['mlm_dense_W'] + params['mlm_dense_b'])\n",
    "    mlm_hidden = layer_norm(mlm_hidden, params['mlm_ln_gamma'], params['mlm_ln_beta'])\n",
    "    mlm_logits = mlm_hidden @ params['token_emb'].T\n",
    "    \n",
    "    return mlm_logits, x\n",
    "\n",
    "# Dynamic masking — different mask each call\n",
    "def dynamic_masking(token_ids, mask_prob=0.15, mask_token_id=103):\n",
    "    \"\"\"RoBERTa-style dynamic masking: new random mask each time.\"\"\"\n",
    "    masked_ids = token_ids.clone()\n",
    "    mask = torch.rand_like(token_ids.float()) < mask_prob\n",
    "    # 80% → [MASK], 10% → random, 10% → keep\n",
    "    rand = torch.rand_like(token_ids.float())\n",
    "    mask_token = mask & (rand < 0.8)\n",
    "    random_token = mask & (rand >= 0.8) & (rand < 0.9)\n",
    "    \n",
    "    masked_ids[mask_token] = mask_token_id\n",
    "    masked_ids[random_token] = torch.randint_like(masked_ids[random_token], 0, 1000)\n",
    "    return masked_ids, mask\n",
    "\n",
    "# Test dynamic masking\n",
    "tokens = torch.randint(0, 100, (1, 20), device=device)\n",
    "print('Original:', tokens[0].cpu().tolist())\n",
    "for i in range(3):\n",
    "    masked, mask = dynamic_masking(tokens)\n",
    "    print(f'Mask {i+1}:  {masked[0].cpu().tolist()}')\n",
    "    print(f'  Masked positions: {mask[0].nonzero().squeeze(-1).cpu().tolist()}')\n",
    "\n",
    "print('\\n→ Different mask each time! (BERT used the same mask for all epochs)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ALBERT: Parameter Efficiency\n",
    "\n",
    "ALBERT introduces two key tricks to reduce parameters:\n",
    "\n",
    "1. **Factorized Embedding**: Instead of projecting directly from vocab → d_model, use vocab → d_emb → d_model (with d_emb << d_model)\n",
    "2. **Cross-Layer Parameter Sharing**: All encoder layers share the same weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_albert(vocab_size, d_emb, d_model, n_heads, d_ff, n_layers, max_len, device):\n",
    "    \"\"\"Initialize ALBERT — factorized embeddings + shared layers.\"\"\"\n",
    "    scale = 0.02\n",
    "    params = {\n",
    "        # Factorized embedding: vocab → d_emb → d_model\n",
    "        'token_emb': torch.randn(vocab_size, d_emb, device=device) * scale,  # Small!\n",
    "        'emb_proj': torch.randn(d_emb, d_model, device=device) * scale,  # Project up\n",
    "        'position_emb': torch.randn(max_len, d_emb, device=device) * scale,\n",
    "        'emb_ln_gamma': torch.ones(d_model, device=device),\n",
    "        'emb_ln_beta': torch.zeros(d_model, device=device),\n",
    "        'n_layers': n_layers,\n",
    "        # SHARED layer — only ONE set of weights for all layers!\n",
    "        'shared_layer': {\n",
    "            'W_Q': torch.randn(d_model, d_model, device=device) * scale,\n",
    "            'W_K': torch.randn(d_model, d_model, device=device) * scale,\n",
    "            'W_V': torch.randn(d_model, d_model, device=device) * scale,\n",
    "            'W_O': torch.randn(d_model, d_model, device=device) * scale,\n",
    "            'ln1_gamma': torch.ones(d_model, device=device),\n",
    "            'ln1_beta': torch.zeros(d_model, device=device),\n",
    "            'W1': torch.randn(d_model, d_ff, device=device) * scale,\n",
    "            'b1': torch.zeros(d_ff, device=device),\n",
    "            'W2': torch.randn(d_ff, d_model, device=device) * scale,\n",
    "            'b2': torch.zeros(d_model, device=device),\n",
    "            'ln2_gamma': torch.ones(d_model, device=device),\n",
    "            'ln2_beta': torch.zeros(d_model, device=device),\n",
    "        },\n",
    "    }\n",
    "    return params\n",
    "\n",
    "def albert_forward(token_ids, params, n_heads):\n",
    "    \"\"\"ALBERT forward: factorized embedding + shared layers.\"\"\"\n",
    "    batch, seq_len = token_ids.shape\n",
    "    positions = torch.arange(seq_len, device=token_ids.device)\n",
    "    \n",
    "    # Factorized embedding: token + position in small space, then project up\n",
    "    x = params['token_emb'][token_ids] + params['position_emb'][positions]\n",
    "    x = x @ params['emb_proj']  # d_emb → d_model\n",
    "    x = layer_norm(x, params['emb_ln_gamma'], params['emb_ln_beta'])\n",
    "    \n",
    "    # Reuse same layer N times\n",
    "    layer_p = params['shared_layer']\n",
    "    for _ in range(params['n_layers']):\n",
    "        attn_out, _ = multi_head_attention(\n",
    "            x, x, x, layer_p['W_Q'], layer_p['W_K'], layer_p['W_V'], layer_p['W_O'], n_heads\n",
    "        )\n",
    "        x = layer_norm(x + attn_out, layer_p['ln1_gamma'], layer_p['ln1_beta'])\n",
    "        ffn_out = ffn(x, layer_p['W1'], layer_p['b1'], layer_p['W2'], layer_p['b2'])\n",
    "        x = layer_norm(x + ffn_out, layer_p['ln2_gamma'], layer_p['ln2_beta'])\n",
    "    \n",
    "    return x\n",
    "\n",
    "# Compare parameter counts\n",
    "torch.manual_seed(42)\n",
    "d_emb = 16  # Small embedding dimension\n",
    "albert_params = init_albert(vocab_size, d_emb, d_model, n_heads, d_ff, n_layers, max_len, device)\n",
    "\n",
    "albert_out = albert_forward(token_ids, albert_params, n_heads)\n",
    "print(f'ALBERT output: {albert_out.shape}')\n",
    "\n",
    "# Parameter comparison\n",
    "bert_emb_params = vocab_size * d_model  # Direct: V × d_model\n",
    "albert_emb_params = vocab_size * d_emb + d_emb * d_model  # Factorized: V × d_emb + d_emb × d_model\n",
    "\n",
    "bert_layer_params = 4 * d_model * d_model + 2 * d_model * d_ff  # Per layer\n",
    "albert_layer_params = bert_layer_params  # Same size, but shared!\n",
    "\n",
    "print(f'\\nEmbedding parameter comparison:')\n",
    "print(f'  BERT:   V × d_model = {vocab_size} × {d_model} = {bert_emb_params:,}')\n",
    "print(f'  ALBERT: V × d_emb + d_emb × d_model = {vocab_size} × {d_emb} + {d_emb} × {d_model} = {albert_emb_params:,}')\n",
    "print(f'  Savings: {bert_emb_params / albert_emb_params:.1f}x fewer embedding params')\n",
    "\n",
    "print(f'\\nEncoder parameter comparison:')\n",
    "print(f'  BERT:   {n_layers} layers × {bert_layer_params} = {n_layers * bert_layer_params:,}')\n",
    "print(f'  ALBERT: 1 shared layer × {albert_layer_params} = {albert_layer_params:,}')\n",
    "print(f'  Savings: {n_layers}x fewer encoder params')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. DistilBERT: Knowledge Distillation\n",
    "\n",
    "DistilBERT trains a smaller student (6 layers) to mimic a larger teacher (12 layers) using:\n",
    "- **Distillation loss**: Match teacher's soft probability distribution\n",
    "- **MLM loss**: Standard masked language modeling\n",
    "- **Cosine embedding loss**: Align hidden representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distillation_loss(student_logits, teacher_logits, temperature=4.0):\n",
    "    \"\"\"Compute KL-divergence between teacher and student soft predictions.\n",
    "    \n",
    "    Higher temperature → softer distributions → more information transferred.\n",
    "    \"\"\"\n",
    "    # Soft targets from teacher\n",
    "    teacher_probs = torch.softmax(teacher_logits / temperature, dim=-1)\n",
    "    \n",
    "    # Soft predictions from student\n",
    "    student_log_probs = torch.log_softmax(student_logits / temperature, dim=-1)\n",
    "    \n",
    "    # KL divergence (scaled by T²)\n",
    "    kl = -(teacher_probs * student_log_probs).sum(dim=-1).mean()\n",
    "    return kl * (temperature ** 2)\n",
    "\n",
    "def cosine_embedding_loss(student_hidden, teacher_hidden):\n",
    "    \"\"\"Align student and teacher hidden representations.\"\"\"\n",
    "    cos_sim = torch.nn.functional.cosine_similarity(student_hidden, teacher_hidden, dim=-1)\n",
    "    return (1 - cos_sim).mean()\n",
    "\n",
    "# Demonstrate distillation\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Teacher: 12-layer BERT\n",
    "teacher_params = init_bert(vocab_size, d_model, n_heads, d_ff, 12, max_len, device)\n",
    "teacher_mlm, _, teacher_hidden, _ = bert_forward(token_ids, segment_ids, teacher_params, n_heads)\n",
    "\n",
    "# Student: 6-layer BERT (DistilBERT)\n",
    "student_params = init_bert(vocab_size, d_model, n_heads, d_ff, 6, max_len, device)\n",
    "student_mlm, _, student_hidden, _ = bert_forward(token_ids, segment_ids, student_params, n_heads)\n",
    "\n",
    "# Compute losses\n",
    "d_loss = distillation_loss(student_mlm, teacher_mlm.detach(), temperature=4.0)\n",
    "c_loss = cosine_embedding_loss(student_hidden, teacher_hidden.detach())\n",
    "\n",
    "print('DistilBERT training losses:')\n",
    "print(f'  Distillation loss (KL-div): {d_loss.item():.4f}')\n",
    "print(f'  Cosine embedding loss:      {c_loss.item():.4f}')\n",
    "print(f'\\nTeacher: 12 layers, Student: 6 layers')\n",
    "print(f'→ Student is ~2x faster with ~97% of teacher performance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize temperature effect on soft distributions\n",
    "torch.manual_seed(42)\n",
    "logits = torch.tensor([5.0, 2.0, 0.5, -1.0, -3.0], device=device)\n",
    "temperatures = [0.5, 1.0, 2.0, 4.0, 8.0]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "x_pos = range(len(logits))\n",
    "width = 0.15\n",
    "\n",
    "for i, T in enumerate(temperatures):\n",
    "    probs = torch.softmax(logits / T, dim=-1).cpu()\n",
    "    offset = (i - len(temperatures) / 2) * width\n",
    "    ax.bar([p + offset for p in x_pos], probs, width=width, label=f'T={T}')\n",
    "\n",
    "ax.set_xlabel('Class')\n",
    "ax.set_ylabel('Probability')\n",
    "ax.set_title('Temperature Effect on Softmax\\n(Higher T → softer distribution → more dark knowledge)')\n",
    "ax.legend()\n",
    "ax.set_xticks(x_pos)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. DeBERTa: Disentangled Attention\n",
    "\n",
    "DeBERTa's key innovation: separate **content** and **position** information in attention. Instead of adding position embeddings to token embeddings, DeBERTa computes attention from three separate terms:\n",
    "\n",
    "$$A_{ij} = \\underbrace{H_i H_j^T}_{\\text{content-to-content}} + \\underbrace{H_i P_{j|i}^T}_{\\text{content-to-position}} + \\underbrace{P_{i|j} H_j^T}_{\\text{position-to-content}}$$\n",
    "\n",
    "where $H$ = content embeddings, $P$ = relative position embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def disentangled_attention(x, rel_pos_emb, W_Qc, W_Kc, W_Qp, W_Kp, n_heads):\n",
    "    \"\"\"DeBERTa-style disentangled attention.\n",
    "    \n",
    "    Computes three attention components:\n",
    "    1. Content-to-content (standard)\n",
    "    2. Content-to-position (what position am I attending to?)\n",
    "    3. Position-to-content (what content is at this relative position?)\n",
    "    \"\"\"\n",
    "    batch, seq_len, d_model = x.shape\n",
    "    d_k = d_model // n_heads\n",
    "    \n",
    "    # Content projections\n",
    "    Q_c = (x @ W_Qc).view(batch, seq_len, n_heads, d_k).transpose(1, 2)\n",
    "    K_c = (x @ W_Kc).view(batch, seq_len, n_heads, d_k).transpose(1, 2)\n",
    "    \n",
    "    # Position projections (shared across batch)\n",
    "    Q_p = (rel_pos_emb @ W_Qp).view(1, -1, n_heads, d_k).transpose(1, 2)\n",
    "    K_p = (rel_pos_emb @ W_Kp).view(1, -1, n_heads, d_k).transpose(1, 2)\n",
    "    \n",
    "    # 1. Content-to-content: standard attention\n",
    "    c2c = torch.matmul(Q_c, K_c.transpose(-2, -1))\n",
    "    \n",
    "    # 2. Content-to-position: query content attends to key positions\n",
    "    # We use relative position indices\n",
    "    c2p = torch.matmul(Q_c, K_p[:, :, :seq_len, :].transpose(-2, -1))\n",
    "    \n",
    "    # 3. Position-to-content: query position attends to key content\n",
    "    p2c = torch.matmul(Q_p[:, :, :seq_len, :], K_c.transpose(-2, -1))\n",
    "    \n",
    "    # Combine all three\n",
    "    scores = (c2c + c2p + p2c) / math.sqrt(3 * d_k)\n",
    "    \n",
    "    return scores, (c2c, c2p, p2c)\n",
    "\n",
    "# Test\n",
    "torch.manual_seed(42)\n",
    "max_rel_pos = 20\n",
    "rel_pos_emb = torch.randn(max_rel_pos, d_model, device=device) * 0.02\n",
    "\n",
    "W_Qc = torch.randn(d_model, d_model, device=device) * 0.02\n",
    "W_Kc = torch.randn(d_model, d_model, device=device) * 0.02\n",
    "W_Qp = torch.randn(d_model, d_model, device=device) * 0.02\n",
    "W_Kp = torch.randn(d_model, d_model, device=device) * 0.02\n",
    "\n",
    "x_test = torch.randn(1, 8, d_model, device=device)\n",
    "scores, (c2c, c2p, p2c) = disentangled_attention(\n",
    "    x_test, rel_pos_emb, W_Qc, W_Kc, W_Qp, W_Kp, n_heads\n",
    ")\n",
    "\n",
    "print(f'Disentangled attention scores: {scores.shape}')\n",
    "print(f'\\nComponent contributions (mean absolute value):')\n",
    "print(f'  Content-to-content: {c2c.abs().mean().item():.4f}')\n",
    "print(f'  Content-to-position: {c2p.abs().mean().item():.4f}')\n",
    "print(f'  Position-to-content: {p2c.abs().mean().item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the three attention components\n",
    "fig, axes = plt.subplots(1, 4, figsize=(20, 4))\n",
    "\n",
    "components = [\n",
    "    ('Content→Content', c2c),\n",
    "    ('Content→Position', c2p),\n",
    "    ('Position→Content', p2c),\n",
    "    ('Combined', c2c + c2p + p2c),\n",
    "]\n",
    "\n",
    "for ax, (title, comp) in zip(axes, components):\n",
    "    im = ax.imshow(comp[0, 0].detach().cpu().numpy(), cmap='RdBu', aspect='auto')\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('Key pos')\n",
    "    ax.set_ylabel('Query pos')\n",
    "    plt.colorbar(im, ax=ax)\n",
    "\n",
    "plt.suptitle('DeBERTa: Disentangled Attention Components (Head 0)', fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Comparison Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize attention patterns from BERT\n",
    "fig, axes = plt.subplots(1, n_heads, figsize=(16, 4))\n",
    "for h in range(n_heads):\n",
    "    ax = axes[h]\n",
    "    im = ax.imshow(attn_weights[0][0, h].detach().cpu().numpy(), cmap='Blues')\n",
    "    ax.set_title(f'Head {h}')\n",
    "    ax.set_xlabel('Key')\n",
    "    ax.set_ylabel('Query')\n",
    "plt.suptitle('BERT Attention Weights (Layer 0) — Bidirectional!', fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('→ Notice: BERT attention is BIDIRECTIONAL — no causal mask!')\n",
    "print('  Every token can attend to every other token (past AND future).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=' * 85)\n",
    "print('COMPARISON: BERT Family Models')\n",
    "print('=' * 85)\n",
    "print(f'{\"Model\":<12} {\"Layers\":<8} {\"Key Innovation\":<35} {\"Params (base)\":<15} {\"Year\"}')\n",
    "print('-' * 85)\n",
    "print(f'{\"BERT\":<12} {\"12\":<8} {\"MLM + NSP, bidirectional\":<35} {\"110M\":<15} {\"2018\"}')\n",
    "print(f'{\"RoBERTa\":<12} {\"12\":<8} {\"No NSP, dynamic masking, more data\":<35} {\"125M\":<15} {\"2019\"}')\n",
    "print(f'{\"ALBERT\":<12} {\"12*\":<8} {\"Shared layers + factorized emb\":<35} {\"12M\":<15} {\"2019\"}')\n",
    "print(f'{\"DistilBERT\":<12} {\"6\":<8} {\"Knowledge distillation from BERT\":<35} {\"66M\":<15} {\"2019\"}')\n",
    "print(f'{\"DeBERTa\":<12} {\"12\":<8} {\"Disentangled content+position attn\":<35} {\"140M\":<15} {\"2020\"}')\n",
    "print('-' * 85)\n",
    "print('* ALBERT uses 12 virtual layers but only 1 set of shared parameters')\n",
    "print('=' * 85)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook we implemented from scratch:\n",
    "\n",
    "1. **BERT** — Bidirectional encoder with MLM + NSP pre-training, token/segment/position embeddings\n",
    "2. **RoBERTa** — Same architecture, better training: no NSP, dynamic masking, more data\n",
    "3. **ALBERT** — Factorized embeddings (V×d_emb + d_emb×d_model) + cross-layer parameter sharing\n",
    "4. **DistilBERT** — Knowledge distillation: smaller student trained to match teacher's soft distributions\n",
    "5. **DeBERTa** — Disentangled attention: separate content-to-content, content-to-position, and position-to-content scores\n",
    "\n",
    "**Key insight:** BERT established that bidirectional pre-training on masked language modeling produces powerful general-purpose representations. Each variant optimized a different axis: training recipe (RoBERTa), parameter efficiency (ALBERT), inference speed (DistilBERT), or attention quality (DeBERTa)."
   ]
  }
 ]
}