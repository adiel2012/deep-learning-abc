{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Fine-Tuning for Downstream Tasks\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/adiel2012/deep-learning-abc/blob/main/bert_fine_tuning.ipynb)\n",
    "\n",
    "In this notebook, we implement **BERT fine-tuning** from scratch for three common downstream tasks:\n",
    "\n",
    "1. **Sequence Classification** (e.g., Sentiment Analysis)\n",
    "2. **Token Classification** (e.g., Named Entity Recognition)\n",
    "3. **Question Answering** (e.g., SQuAD)\n",
    "\n",
    "We use a pre-trained `BertModel` (from Hugging Face for convenience, but the architecture matches our scratch implementation) and add custom heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch transformers datasets scikit-learn matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Load pre-trained BERT base (uncased)\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "bert = BertModel.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Sequence Classification (e.g., Sentiment Analysis)\n",
    "\n",
    "**Goal:** Classify an entire sentence (e.g., Positive vs. Negative).\n",
    "**Method:** Use the `[CLS]` token embedding (first token) as the sentence representation.\n",
    "\n",
    "Head: `Dropout` → `Linear(d_model, num_classes)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertForSequenceClassification(nn.Module):\n",
    "    def __init__(self, bert_model, num_classes):\n",
    "        super().__init__()\n",
    "        self.bert = bert_model\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.classifier = nn.Linear(768, num_classes)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # Pooler output is usually the [CLS] token processed by a linear layer + Tanh\n",
    "        # We'll use the raw [CLS] hidden state for transparency:\n",
    "        cls_token = outputs.last_hidden_state[:, 0, :]  # [Batch, 768]\n",
    "        \n",
    "        x = self.dropout(cls_token)\n",
    "        logits = self.classifier(x)\n",
    "        return logits\n",
    "\n",
    "# Demo\n",
    "model_seq = BertForSequenceClassification(bert, num_classes=2).to(device)\n",
    "inputs = tokenizer([\"I love this movie!\", \"This film was terrible.\"], \n",
    "                   return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model_seq(inputs.input_ids, inputs.attention_mask)\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "\n",
    "print(\"Sequence Classification (Sentiment):\")\n",
    "print(f\"  'I love this movie!'    -> {probs[0].cpu().numpy()}\")\n",
    "print(f\"  'This film was terrible.' -> {probs[1].cpu().numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Token Classification (e.g., NER)\n",
    "\n",
    "**Goal:** Classify each token in the sequence (e.g., Person, Org, Loc, O).\n",
    "**Method:** Apply a classifier to **every** token's embedding.\n",
    "\n",
    "Head: `Dropout` → `Linear(d_model, num_classes)` (applied per token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertForTokenClassification(nn.Module):\n",
    "    def __init__(self, bert_model, num_classes):\n",
    "        super().__init__()\n",
    "        self.bert = bert_model\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.classifier = nn.Linear(768, num_classes)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = outputs.last_hidden_state  # [Batch, SeqLen, 768]\n",
    "        \n",
    "        x = self.dropout(sequence_output)\n",
    "        logits = self.classifier(x)  # [Batch, SeqLen, NumClasses]\n",
    "        return logits\n",
    "\n",
    "# Demo: Named Entity Recognition (3 classes: O, B-PER, I-PER)\n",
    "model_token = BertForTokenClassification(bert, num_classes=3).to(device)\n",
    "text = \"Hugging Face is based in New York.\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model_token(inputs.input_ids, inputs.attention_mask)\n",
    "    preds = torch.argmax(logits, dim=-1)[0]\n",
    "\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs.input_ids[0])\n",
    "print(\"\\nToken Classification (NER):\")\n",
    "for token, pred in zip(tokens, preds.cpu().numpy()):\n",
    "    print(f\"  {token:<12} -> Class {pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Question Answering (e.g., SQuAD)\n",
    "\n",
    "**Goal:** Find the *answer span* (start and end indices) in the text.\n",
    "**Method:** Predict `start` and `end` scores for every token.\n",
    "\n",
    "Head: `Linear(d_model, 2)` → Splits into `start_logits` and `end_logits`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertForQuestionAnswering(nn.Module):\n",
    "    def __init__(self, bert_model):\n",
    "        super().__init__()\n",
    "        self.bert = bert_model\n",
    "        # Output 2 logits per token: start_score, end_score\n",
    "        self.qa_outputs = nn.Linear(768, 2)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = outputs.last_hidden_state\n",
    "        \n",
    "        logits = self.qa_outputs(sequence_output)  # [Batch, SeqLen, 2]\n",
    "        start_logits, end_logits = logits.split(1, dim=-1)\n",
    "        \n",
    "        return start_logits.squeeze(-1), end_logits.squeeze(-1)\n",
    "\n",
    "# Demo: Question Answering\n",
    "model_qa = BertForQuestionAnswering(bert).to(device)\n",
    "\n",
    "question = \"Where do I live?\"\n",
    "context = \"My name is Sarah and I live in London.\"\n",
    "inputs = tokenizer(question, context, return_tensors=\"pt\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    start_logits, end_logits = model_qa(inputs.input_ids, inputs.attention_mask)\n",
    "    start_idx = torch.argmax(start_logits)\n",
    "    end_idx = torch.argmax(end_logits)\n",
    "\n",
    "all_tokens = tokenizer.convert_ids_to_tokens(inputs.input_ids[0])\n",
    "answer = tokenizer.decode(inputs.input_ids[0][start_idx : end_idx + 1])\n",
    "\n",
    "print(\"\\nQuestion Answering:\")\n",
    "print(f\"  Question: {question}\")\n",
    "print(f\"  Context:  {context}\")\n",
    "print(f\"  Answer Span: tokens[{start_idx}:{end_idx+1}]\")\n",
    "print(f\"  Predicted Answer: '{answer}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Fine-Tuning Architectures\n",
    "\n",
    "Here is a comparison of how the same pre-trained BERT body is adapted for different tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Architecture Summary Table\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Task':<25} {'Input':<20} {'Output Head'}\")\n",
    "print(\"-\"*60)\n",
    "print(f\"{'Sequence Classification':<25} {'[CLS] token':<20} {'Linear(768, K)'}\")\n",
    "print(f\"{'Token Classification':<25} {'All tokens':<20} {'Linear(768, K) per token'}\")\n",
    "print(f\"{'Question Answering':<25} {'All tokens':<20} {'Linear(768, 2) per token'}\")\n",
    "print(f\"{'Multiple Choice':<25} {'[CLS] per choice':<20} {'Linear(768, 1)'}\")\n",
    "print(\"=\"*60)"
   ]
  }
 ]
}