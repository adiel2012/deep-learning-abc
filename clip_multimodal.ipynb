{
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "colab": {
            "provenance": [],
            "gpuType": "T4"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        },
        "language_info": {
            "name": "python"
        },
        "accelerator": "GPU"
    },
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# CLIP (Contrastive Language-Image Pre-training) from Scratch\n",
                "\n",
                "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/adiel2012/deep-learning-abc/blob/main/clip_multimodal.ipynb)\n",
                "\n",
                "CLIP aligns text and images in a shared embedding space using contrastive learning.\n",
                "\n",
                "Key Concepts:\n",
                "1. **Image Encoder:** Converts images to vector $I_f$.\n",
                "2. **Text Encoder:** Converts text to vector $T_f$.\n",
                "3. **Contrastive Loss:** Maximizes dot product for correct (image, text) pairs and minimizes it for incorrect ones in a batch.\n",
                "\n",
                "$$ Loss = \\frac{1}{2} (CE(I \\cdot T^T, labels) + CE(T \\cdot I^T, labels)) $$"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install torch torchvision matplotlib transformers"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "from torchvision import transforms\n",
                "\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f'Using device: {device}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Encoders (Simplified)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class ImageEncoder(nn.Module):\n",
                "    def __init__(self, embed_dim):\n",
                "        super().__init__()\n",
                "        # Simple ResNet-like or Convolutional backbone\n",
                "        self.conv1 = nn.Conv2d(3, 32, 3, stride=2, padding=1)\n",
                "        self.conv2 = nn.Conv2d(32, 64, 3, stride=2, padding=1)\n",
                "        self.conv3 = nn.Conv2d(64, 128, 3, stride=2, padding=1)\n",
                "        self.global_pool = nn.AdaptiveAvgPool2d(1)\n",
                "        self.fc = nn.Linear(128, embed_dim)\n",
                "\n",
                "    def forward(self, x):\n",
                "        x = F.relu(self.conv1(x))\n",
                "        x = F.relu(self.conv2(x))\n",
                "        x = F.relu(self.conv3(x))\n",
                "        x = self.global_pool(x).flatten(1)\n",
                "        return self.fc(x)\n",
                "\n",
                "class TextEncoder(nn.Module):\n",
                "    def __init__(self, vocab_size, embed_dim, max_seq_len=32):\n",
                "        super().__init__()\n",
                "        self.embedding = nn.Embedding(vocab_size, 128)\n",
                "        self.transformer = nn.TransformerEncoder(\n",
                "            nn.TransformerEncoderLayer(d_model=128, nhead=4, batch_first=True),\n",
                "            num_layers=2\n",
                "        )\n",
                "        self.fc = nn.Linear(128, embed_dim)\n",
                "        \n",
                "    def forward(self, x):\n",
                "        x = self.embedding(x)\n",
                "        x = self.transformer(x)\n",
                "        # Avg pooling over sequence\n",
                "        x = x.mean(dim=1)\n",
                "        return self.fc(x)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. CLIP Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class CLIP(nn.Module):\n",
                "    def __init__(self, vocab_size, embed_dim=256):\n",
                "        super().__init__()\n",
                "        self.image_encoder = ImageEncoder(embed_dim)\n",
                "        self.text_encoder = TextEncoder(vocab_size, embed_dim)\n",
                "        self.temperature = nn.Parameter(torch.ones([]) * 0.07)\n",
                "\n",
                "    def forward(self, image, text):\n",
                "        I_e = self.image_encoder(image)\n",
                "        T_e = self.text_encoder(text)\n",
                "        \n",
                "        # Normalize embeddings\n",
                "        I_e = I_e / I_e.norm(dim=-1, keepdim=True)\n",
                "        T_e = T_e / T_e.norm(dim=-1, keepdim=True)\n",
                "        \n",
                "        # Scaled pairwise cosine similarities\n",
                "        logits = (I_e @ T_e.t()) * torch.exp(self.temperature)\n",
                "        return logits"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Dummy Training Example"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Mock Data\n",
                "vocab_size = 1000\n",
                "batch_size = 8\n",
                "embed_dim = 64\n",
                "\n",
                "model = CLIP(vocab_size, embed_dim).to(device)\n",
                "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
                "\n",
                "images = torch.randn(batch_size, 3, 224, 224).to(device)\n",
                "text = torch.randint(0, vocab_size, (batch_size, 20)).to(device)\n",
                "\n",
                "# Labels: The diagonal elements are the correct pairs (image[i] matches text[i])\n",
                "labels = torch.arange(batch_size).to(device)\n",
                "\n",
                "# Forward pass\n",
                "logits_per_image = model(images, text)\n",
                "logits_per_text = logits_per_image.t()\n",
                "\n",
                "# Contrastive Loss\n",
                "loss_i = F.cross_entropy(logits_per_image, labels)\n",
                "loss_t = F.cross_entropy(logits_per_text, labels)\n",
                "loss = (loss_i + loss_t) / 2\n",
                "\n",
                "print(f\"Initial Loss: {loss.item():.4f}\")\n",
                "\n",
                "optimizer.zero_grad()\n",
                "loss.backward()\n",
                "optimizer.step()\n",
                "\n",
                "print(\"Training step complete. In a real scenario, use COCO or Flickr30k datasets.\")"
            ]
        }
    ]
}