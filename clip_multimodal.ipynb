{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# CLIP (Contrastive Language-Image Pre-training) from Scratch\n",
                "\n",
                "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/adiel2012/deep-learning-abc/blob/main/clip_multimodal.ipynb)\n",
                "\n",
                "## 1. Mathematical Foundations: InfoNCE Loss\n",
                "\n",
                "CLIP aligns text and images by maximizing the cosine similarity of correct pairs in a batch.\n",
                "\n",
                "Given a batch of $N$ (image, text) pairs, let $I_f \\in \\mathbb{R}^{N \\times d}$ and $T_f \\in \\mathbb{R}^{N \\times d}$ be the normalized feature embeddings.\n",
                "\n",
                "The pairwise similarity matrix is $S = I_f T_f^T$. We scale this by a learnable temperature $\\tau = e^t$:\n",
                "\n",
                "$$ \\text{logits} = S \\cdot e^t $$\n",
                "\n",
                "We compute the symmetric Cross Entropy loss. For the $i$-th image, the target is the $i$-th text (diagonal).\n",
                "\n",
                "$$ \\mathcal{L}_I = -\\frac{1}{N} \\sum_{i=1}^N \\log \\frac{\\exp(\\text{logits}_{i,i})}{\\sum_{j=1}^N \\exp(\\text{logits}_{i,j})} $$\n",
                "\n",
                "$$ \\mathcal{L}_T = -\\frac{1}{N} \\sum_{i=1}^N \\log \\frac{\\exp(\\text{logits}_{i,i})}{\\sum_{j=1}^N \\exp(\\text{logits}_{j,i})} $$\n",
                "\n",
                "$$ \\mathcal{L} = \\frac{\\mathcal{L}_I + \\mathcal{L}_T}{2} $$"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install torch torchvision matplotlib transformers"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "from torchvision import transforms\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f'Using device: {device}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Encoders Implementation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class ImageEncoder(nn.Module):\n",
                "    def __init__(self, embed_dim):\n",
                "        super().__init__()\n",
                "        # Simple Conv backbone\n",
                "        self.conv1 = nn.Conv2d(3, 32, 3, stride=2, padding=1)\n",
                "        self.conv2 = nn.Conv2d(32, 64, 3, stride=2, padding=1)\n",
                "        self.conv3 = nn.Conv2d(64, 128, 3, stride=2, padding=1)\n",
                "        self.global_pool = nn.AdaptiveAvgPool2d(1)\n",
                "        self.fc = nn.Linear(128, embed_dim)\n",
                "\n",
                "    def forward(self, x):\n",
                "        x = F.relu(self.conv1(x))\n",
                "        x = F.relu(self.conv2(x))\n",
                "        x = F.relu(self.conv3(x))\n",
                "        x = self.global_pool(x).flatten(1)\n",
                "        return self.fc(x)\n",
                "\n",
                "class TextEncoder(nn.Module):\n",
                "    def __init__(self, vocab_size, embed_dim, max_seq_len=32):\n",
                "        super().__init__()\n",
                "        self.embedding = nn.Embedding(vocab_size, 128)\n",
                "        self.transformer = nn.TransformerEncoder(\n",
                "            nn.TransformerEncoderLayer(d_model=128, nhead=4, batch_first=True),\n",
                "            num_layers=2\n",
                "        )\n",
                "        self.fc = nn.Linear(128, embed_dim)\n",
                "        \n",
                "    def forward(self, x):\n",
                "        x = self.embedding(x)\n",
                "        x = self.transformer(x)\n",
                "        # Avg pooling over sequence\n",
                "        x = x.mean(dim=1)\n",
                "        return self.fc(x)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. CLIP Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class CLIP(nn.Module):\n",
                "    def __init__(self, vocab_size, embed_dim=256):\n",
                "        super().__init__()\n",
                "        self.image_encoder = ImageEncoder(embed_dim)\n",
                "        self.text_encoder = TextEncoder(vocab_size, embed_dim)\n",
                "        self.temperature = nn.Parameter(torch.ones([]) * 0.07)\n",
                "\n",
                "    def forward(self, image, text):\n",
                "        I_e = self.image_encoder(image)\n",
                "        T_e = self.text_encoder(text)\n",
                "        \n",
                "        # L2 Normalize embeddings\n",
                "        I_e = I_e / I_e.norm(dim=-1, keepdim=True)\n",
                "        T_e = T_e / T_e.norm(dim=-1, keepdim=True)\n",
                "        \n",
                "        # Scaled pairwise cosine similarities\n",
                "        logits = (I_e @ T_e.t()) * torch.exp(self.temperature)\n",
                "        return logits"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Training (Small Scale Demo)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Mock Data\n",
                "vocab_size = 1000\n",
                "batch_size = 8\n",
                "embed_dim = 64\n",
                "\n",
                "model = CLIP(vocab_size, embed_dim).to(device)\n",
                "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
                "\n",
                "images = torch.randn(batch_size, 3, 224, 224).to(device)\n",
                "text = torch.randint(0, vocab_size, (batch_size, 20)).to(device)\n",
                "\n",
                "# Labels: The diagonal elements are the correct pairs (image[i] matches text[i])\n",
                "labels = torch.arange(batch_size).to(device)\n",
                "\n",
                "# Forward pass\n",
                "logits_per_image = model(images, text)\n",
                "logits_per_text = logits_per_image.t()\n",
                "\n",
                "# Contrastive Loss\n",
                "loss_i = F.cross_entropy(logits_per_image, labels)\n",
                "loss_t = F.cross_entropy(logits_per_text, labels)\n",
                "loss = (loss_i + loss_t) / 2\n",
                "\n",
                "print(f\"Initial Loss: {loss.item():.4f}\")\n",
                "\n",
                "for _ in range(5):\n",
                "    optimizer.zero_grad()\n",
                "    loss.backward()\n",
                "    optimizer.step()\n",
                "    \n",
                "    logits_per_image = model(images, text)\n",
                "    loss = F.cross_entropy(logits_per_image, labels)\n",
                "    print(f\"Loss: {loss.item():.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Visualization: Contrastive Similarity Matrix"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "with torch.no_grad():\n",
                "    logits = model(images, text)\n",
                "    similarity = F.softmax(logits, dim=1).cpu().numpy()\n",
                "\n",
                "plt.figure(figsize=(6, 5))\n",
                "plt.imshow(similarity, cmap='viridis')\n",
                "plt.colorbar(label='Softmax Probability')\n",
                "plt.xlabel(\"Text Index\")\n",
                "plt.ylabel(\"Image Index\")\n",
                "plt.title(\"Image-Text Similarity Matrix (After a few steps)\")\n",
                "plt.show()\n",
                "\n",
                "print(\"Diagonal should ideally be bright (high similarity).\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}