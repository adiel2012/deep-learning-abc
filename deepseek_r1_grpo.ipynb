{
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "colab": {
            "provenance": [],
            "gpuType": "T4"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        },
        "language_info": {
            "name": "python"
        },
        "accelerator": "GPU"
    },
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# DeepSeek-R1: Group Relative Policy Optimization (GRPO)\n",
                "\n",
                "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/adiel2012/deep-learning-abc/blob/main/deepseek_r1_grpo.ipynb)\n",
                "\n",
                "DeepSeek-R1 introduced a reinforcement learning method called **GRPO** that removes the need for a Critic model (Value Function), making RL efficiency much higher for reasoning tasks.\n",
                "\n",
                "### Key Idea\n",
                "Instead of estimating an \"advantage\" $A_t$ using a separate Critic network (like PPO), GRPO samples a **group** of $G$ outputs for the *same* prompt.\n",
                "\n",
                "The advantage of the $i$-th output is simply its normalized score relative to the group:\n",
                "$$ A_i = \\frac{r_i - \\text{mean}(R)}{\\text{std}(R)} $$\n",
                "\n",
                "Where $R = \\{r_1, r_2, ..., r_G\\}$ are the rewards for the group.\n",
                "\n",
                "This encourages the model to generate answers that are better than its own average attempts."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "import numpy as np\n",
                "\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f'Using device: {device}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Minimal Policy Model\n",
                "A simple language model (like a tiny GPT) that we want to train."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class TinyPolicy(nn.Module):\n",
                "    def __init__(self, vocab_size, d_model):\n",
                "        super().__init__()\n",
                "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
                "        self.net = nn.Sequential(\n",
                "            nn.Linear(d_model, d_model),\n",
                "            nn.ReLU(),\n",
                "            nn.Linear(d_model, vocab_size)\n",
                "        )\n",
                "    \n",
                "    def forward(self, x):\n",
                "        # x: (batch, seq_len)\n",
                "        emb = self.embedding(x)\n",
                "        logits = self.net(emb)\n",
                "        return logits"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. GRPO Loss Function\n",
                "This computes the policy gradient loss similar to PPO but using group-normalized advantages."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def grpo_loss(logits, old_log_probs, actions, advantages, epsilon=0.2):\n",
                "    \"\"\"\n",
                "    logits: (Batch, Seq_Len, Vocab_Size) - Current policy outputs\n",
                "    old_log_probs: (Batch, Seq_Len) - Log probs of actions from old policy (before update)\n",
                "    actions: (Batch, Seq_Len) - The token indices generated\n",
                "    advantages: (Batch,) - Group relative advantage for each sequence\n",
                "    epsilon: PPO clipping parameter\n",
                "    \"\"\"\n",
                "    \n",
                "    # 1. Calculate current log probs\n",
                "    log_probs = F.log_softmax(logits, dim=-1)\n",
                "    # Gather log prob of the actual taken actions\n",
                "    action_log_probs = torch.gather(log_probs, dim=2, index=actions.unsqueeze(-1)).squeeze(-1)\n",
                "    \n",
                "    # 2. Probability Ratio r_t(theta) = exp(log_p_new - log_p_old)\n",
                "    ratio = torch.exp(action_log_probs - old_log_probs)\n",
                "    \n",
                "    # 3. PPO-style Clipped Objective\n",
                "    # We expand advantages to match sequence length for broadcasting if needed,\n",
                "    # or usually we average over the sequence. Here let's assume advantage applies to the whole seq.\n",
                "    adv_expanded = advantages.unsqueeze(1).expand_as(ratio)\n",
                "    \n",
                "    surr1 = ratio * adv_expanded\n",
                "    surr2 = torch.clamp(ratio, 1 - epsilon, 1 + epsilon) * adv_expanded\n",
                "    \n",
                "    # Maximize objective => Minimize -objective\n",
                "    loss = -torch.min(surr1, surr2).mean()\n",
                "    \n",
                "    # Optional: KL Divergence penalty to reference model would be added here in full R1.\n",
                "    # loss += beta * KL(policy || ref)\n",
                "    \n",
                "    return loss"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Training Loop Simulation\n",
                "We verify mathematical correctness with dummy data."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Setup\n",
                "group_size = 4 # G in the paper\n",
                "vocab_size = 100\n",
                "seq_len = 10\n",
                "\n",
                "model = TinyPolicy(vocab_size, 32).to(device)\n",
                "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
                "\n",
                "# SIMULATE A BATCH OF GENERATIONS\n",
                "# Assume we had 1 prompt, and we generated 'group_size' completions.\n",
                "actions = torch.randint(0, vocab_size, (group_size, seq_len)).to(device)\n",
                "\n",
                "# SIMULATE REWARDS\n",
                "# Let's say we have an oracle reward model or verifier (e.g. math solver)\n",
                "# Completion 0 and 2 were \"correct\" (high reward), others wrong.\n",
                "rewards = torch.tensor([1.0, 0.1, 1.0, 0.2]).to(device)\n",
                "\n",
                "# 1. Compute Group Advantages\n",
                "mean_r = rewards.mean()\n",
                "std_r = rewards.std() + 1e-8 # Prevent div by zero\n",
                "advantages = (rewards - mean_r) / std_r\n",
                "\n",
                "print(\"Rewards:\", rewards)\n",
                "print(\"Group Advantages:\", advantages)\n",
                "\n",
                "# 2. Get 'Old' Log Probs (Assuming we just sampled them from current model for this step)\n",
                "with torch.no_grad():\n",
                "    logits_old = model(actions)\n",
                "    log_probs_old_all = F.log_softmax(logits_old, dim=-1)\n",
                "    old_log_probs = torch.gather(log_probs_old_all, dim=2, index=actions.unsqueeze(-1)).squeeze(-1)\n",
                "\n",
                "# 3. Optimization Step\n",
                "logits_new = model(actions)\n",
                "loss = grpo_loss(logits_new, old_log_probs, actions, advantages)\n",
                "\n",
                "optimizer.zero_grad()\n",
                "loss.backward()\n",
                "optimizer.step()\n",
                "\n",
                "print(f\"GRPO Step Loss: {loss.item():.4f}\")"
            ]
        }
    ]
}