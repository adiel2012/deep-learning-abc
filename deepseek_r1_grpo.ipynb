{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# DeepSeek-R1: Group Relative Policy Optimization (GRPO)\n",
                "\n",
                "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/adiel2012/deep-learning-abc/blob/main/deepseek_r1_grpo.ipynb)\n",
                "\n",
                "## 1. Mathematical Foundations: GRPO Algorithm\n",
                "\n",
                "DeepSeek-R1 avoids the need for a Value Function (Critic) by using group-based relative advantages.\n",
                "\n",
                "For each prompt $q$, the model samples a group of $G$ outputs $\\{o_1, o_2, \\dots, o_G\\}$ from the old policy $\\pi_{\\theta_{old}}$.\n",
                "\n",
                "### Group Advantage\n",
                "The advantage for the $i$-th output is computed by normalizing the rewards within the group:\n",
                "\n",
                "$$ A_i = \\frac{r_i - \\text{mean}(\\{r_1, \\dots, r_G\\})}{\\text{std}(\\{r_1, \\dots, r_G\\})} $$\n",
                "\n",
                "### GRPO Objective\n",
                "The objective maximizes the PPO-clipped surrogate using this group advantage, plus a KL penalty to keep the policy close to a reference model $\\pi_{ref}$:\n",
                "\n",
                "$$ \\mathcal{L}_{GRPO}(\\theta) = \\frac{1}{G} \\sum_{i=1}^G \\left( \\min \\left( \\frac{\\pi_\\theta(o_i|q)}{\\pi_{\\theta_{old}}(o_i|q)} A_i, \\text{clip}(\\dots) A_i \\right) - \\beta D_{KL}(\\pi_\\theta || \\pi_{ref}) \\right) $$\n",
                "\n",
                "By removing the critic, GRPO saves significant memory and training stability, especially for reasoning tasks where judging partial steps is hard."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f'Using device: {device}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Policy Model Implementation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class TinyPolicy(nn.Module):\n",
                "    def __init__(self, vocab_size, d_model):\n",
                "        super().__init__()\n",
                "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
                "        self.net = nn.Sequential(\n",
                "            nn.Linear(d_model, d_model),\n",
                "            nn.ReLU(),\n",
                "            nn.Linear(d_model, vocab_size)\n",
                "        )\n",
                "    \n",
                "    def forward(self, x):\n",
                "        # x: (batch, seq_len)\n",
                "        emb = self.embedding(x)\n",
                "        logits = self.net(emb)\n",
                "        return logits"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. GRPO Loss Function"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def grpo_loss(logits, old_log_probs, actions, advantages, epsilon=0.2):\n",
                "    \"\"\"\n",
                "    logits: (Batch, Seq_Len, Vocab_Size) - Current policy outputs\n",
                "    old_log_probs: (Batch, Seq_Len) - Log probs of actions from old policy (before update)\n",
                "    actions: (Batch, Seq_Len) - The token indices generated\n",
                "    advantages: (Batch,) - Group relative advantage for each sequence\n",
                "    epsilon: PPO clipping parameter\n",
                "    \"\"\"\n",
                "    \n",
                "    # 1. Calculate current log probs\n",
                "    log_probs = F.log_softmax(logits, dim=-1)\n",
                "    # Gather log prob of the actual taken actions\n",
                "    action_log_probs = torch.gather(log_probs, dim=2, index=actions.unsqueeze(-1)).squeeze(-1)\n",
                "    \n",
                "    # 2. Probability Ratio r_t(theta) = exp(log_p_new - log_p_old)\n",
                "    ratio = torch.exp(action_log_probs - old_log_probs)\n",
                "    \n",
                "    # 3. PPO-style Clipped Objective\n",
                "    # We expand advantages to match sequence length for broadcasting\n",
                "    # For simplicity, we assume advantage is scalar for whole sequence\n",
                "    adv_expanded = advantages.unsqueeze(1).expand_as(ratio)\n",
                "    \n",
                "    surr1 = ratio * adv_expanded\n",
                "    surr2 = torch.clamp(ratio, 1 - epsilon, 1 + epsilon) * adv_expanded\n",
                "    \n",
                "    loss = -torch.min(surr1, surr2).mean()\n",
                "    \n",
                "    return loss"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Simulation with Advantage Visualization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Setup\n",
                "group_size = 64 # High group size for better visualization\n",
                "vocab_size = 100\n",
                "seq_len = 10\n",
                "\n",
                "model = TinyPolicy(vocab_size, 32).to(device)\n",
                "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
                "\n",
                "# SIMULATE A BATCH\n",
                "# actions = samples from the model (mocked here)\n",
                "actions = torch.randint(0, vocab_size, (group_size, seq_len)).to(device)\n",
                "\n",
                "# SIMULATE REWARDS\n",
                "# Assume a sparse reward distribution (common in reasoning: most wrong (0), some right (1))\n",
                "# Let's say 10% correct.\n",
                "rewards = torch.zeros(group_size).to(device)\n",
                "rewards[:int(group_size * 0.1)] = 1.0\n",
                "rewards = rewards[torch.randperm(group_size)] # Shuffle\n",
                "\n",
                "# 1. Compute Group Advantages\n",
                "mean_r = rewards.mean()\n",
                "std_r = rewards.std() + 1e-8\n",
                "advantages = (rewards - mean_r) / std_r\n",
                "\n",
                "print(f\"Mean Reward: {mean_r:.4f}, Std Reward: {std_r:.4f}\")\n",
                "\n",
                "# 2. Forward Old (Mock)\n",
                "with torch.no_grad():\n",
                "    logits_old = model(actions)\n",
                "    log_probs_old_all = F.log_softmax(logits_old, dim=-1)\n",
                "    old_log_probs = torch.gather(log_probs_old_all, dim=2, index=actions.unsqueeze(-1)).squeeze(-1)\n",
                "\n",
                "# 3. Optimize\n",
                "logits_new = model(actions)\n",
                "loss = grpo_loss(logits_new, old_log_probs, actions, advantages)\n",
                "\n",
                "optimizer.zero_grad()\n",
                "loss.backward()\n",
                "optimizer.step()\n",
                "\n",
                "print(f\"GRPO Loss: {loss.item():.4f}\")\n",
                "\n",
                "# Plot Advantage Distribution\n",
                "plt.figure(figsize=(8, 4))\n",
                "plt.hist(advantages.cpu().numpy(), bins=10, alpha=0.7, color='purple', edgecolor='black')\n",
                "plt.title(f\"Distribution of Advantages $A_i$ (Group Size G={group_size})\")\n",
                "plt.xlabel(\"Advantage Value (Normalized Reward)\")\n",
                "plt.ylabel(\"Frequency\")\n",
                "plt.grid(axis='y', alpha=0.3)\n",
                "plt.show()\n",
                "\n",
                "print(\"Notice how rewards [0, 1] are transformed into advantages centered at 0.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}