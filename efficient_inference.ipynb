{
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "colab": {
            "provenance": [],
            "gpuType": "T4"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        },
        "language_info": {
            "name": "python"
        },
        "accelerator": "GPU"
    },
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Efficient Inference: FlashAttention & Quantization\n",
                "\n",
                "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/adiel2012/deep-learning-abc/blob/main/efficient_inference.ipynb)\n",
                "\n",
                "Modern LLMs rely on efficiency tricks to run fast and fit in memory.\n",
                "\n",
                "## 1. FlashAttention (Conceptual Simulation)\n",
                "\n",
                "**Problem:** Standard Attention computes an $N \\times N$ matrix ($QK^T$), which is huge ($O(N^2)$ memory).\n",
                "**Solution:** FlashAttention computes attention in **tiles** (blocks) without ever materializing the full matrix. It uses **Tiling** and **Recomputation**.\n",
                "\n",
                "Here we simulate the \"Tiling\" logic in Python (real FlashAttention uses CUDA kernels)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "\n",
                "def standard_attention(Q, K, V):\n",
                "    # Memory: O(N^2)\n",
                "    scores = Q @ K.transpose(-2, -1)\n",
                "    P = torch.softmax(scores, dim=-1)\n",
                "    return P @ V\n",
                "\n",
                "def tiled_attention_simulation(Q, K, V, block_size=2):\n",
                "    # Simulation of block-wise attention computation\n",
                "    # Real FlashAttention does this in SRAM with online softmax\n",
                "    N, d = Q.shape\n",
                "    output = torch.zeros_like(Q)\n",
                "    \n",
                "    # Loop over blocks of Q (rows)\n",
                "    for i in range(0, N, block_size):\n",
                "        Q_block = Q[i:i+block_size]\n",
                "        \n",
                "        # In real FA, we would maintain running max/sum for softmax here\n",
                "        # For this simple demo, we just compute the row-chunk exact attention\n",
                "        # against ALL keys (or tiled keys) to show we don't need full N*N at once IF we managed stats.\n",
                "        \n",
                "        # Simplified: We still compute Q_block @ K^T (size block_size * N)\n",
                "        # This is O(block_size * N), much smaller than O(N^2) if block_size is small\n",
                "        scores_block = Q_block @ K.transpose(-2, -1)\n",
                "        P_block = torch.softmax(scores_block, dim=-1)\n",
                "        output[i:i+block_size] = P_block @ V\n",
                "        \n",
                "    return output\n",
                "\n",
                "# Test\n",
                "torch.manual_seed(42)\n",
                "N, d = 8, 4\n",
                "Q = torch.randn(N, d)\n",
                "K = torch.randn(N, d)\n",
                "V = torch.randn(N, d)\n",
                "\n",
                "print(\"Standard:\\n\", standard_attention(Q, K, V))\n",
                "print(\"Tiled (Simulated):\\n\", tiled_attention_simulation(Q, K, V, block_size=4))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Quantization (INT8)\n",
                "\n",
                "Reducing precision from FP32 (4 bytes) to INT8 (1 byte) reduces memory by 4x. \n",
                "\n",
                "### AbsMax Quantization (Symmetric)\n",
                "Map range $[-absmax, absmax]$ to $[-127, 127]$."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def absmax_quantize(x):\n",
                "    scale = 127 / torch.max(torch.abs(x))\n",
                "    x_quant = (x * scale).round().clamp(-127, 127).to(torch.int8)\n",
                "    return x_quant, scale\n",
                "\n",
                "def dequantize(x_quant, scale):\n",
                "    return x_quant.float() / scale\n",
                "\n",
                "weights = torch.tensor([0.1, -0.5, 1.2, -2.5, 0.0])\n",
                "q, s = absmax_quantize(weights)\n",
                "dq = dequantize(q, s)\n",
                "\n",
                "print(\"Original:\", weights)\n",
                "print(\"Quantized (int8):\", q)\n",
                "print(\"Scale:\", s)\n",
                "print(\"Dequantized:\", dq)\n",
                "print(\"Error:\", (weights - dq).abs().mean().item())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### ZeroPoint Quantization (Asymmetric)\n",
                "Map range $[min, max]$ to $[0, 255]$ (unsigned).\n",
                "\n",
                "$$ x_{int} = round( \frac{x}{scale} + zero\\_point ) $$"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def zeropoint_quantize(x):\n",
                "    x_range = x.max() - x.min()\n",
                "    x_range = 1 if x_range == 0 else x_range\n",
                "    \n",
                "    scale = 255 / x_range\n",
                "    zeropoint = (-x.min() * scale).round()\n",
                "    \n",
                "    x_quant = (x * scale + zeropoint).round().clamp(0, 255).to(torch.uint8)\n",
                "    return x_quant, scale, zeropoint\n",
                "\n",
                "def zp_dequantize(x_quant, scale, zeropoint):\n",
                "    return (x_quant.float() - zeropoint) / scale\n",
                "\n",
                "q_zp, s_zp, z_zp = zeropoint_quantize(weights)\n",
                "dq_zp = zp_dequantize(q_zp, s_zp, z_zp)\n",
                "\n",
                "print(\"\\nZeroPoint Quantized:\", q_zp)\n",
                "print(\"Dequantized:\", dq_zp)"
            ]
        }
    ]
}