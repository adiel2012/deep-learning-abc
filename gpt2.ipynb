{
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "colab": {
            "provenance": [],
            "gpuType": "T4"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        },
        "language_info": {
            "name": "python"
        },
        "accelerator": "GPU"
    },
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# GPT-2 from Scratch\n",
                "\n",
                "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/adiel2012/deep-learning-abc/blob/main/gpt2.ipynb)\n",
                "\n",
                "This notebook implements **GPT-2 (Generative Pre-trained Transformer)** from scratch.\n",
                "\n",
                "Key differences from BERT:\n",
                "1. **Decoder-only architecture**: Uses masked self-attention (causal mask) so tokens can only attend to past tokens.\n",
                "2. **Objective**: Causal Language Modeling (predict next token).\n",
                "3. **Layer Normalization**: Pre-norm formulation (LayerNorm is applied *before* the sub-layer)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install torch matplotlib"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "import math\n",
                "\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f'Using device: {device}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Causal Self-Attention\n",
                "\n",
                "The core of GPT is the **causal mask**, which ensures that position $i$ can only attend to positions $j \\le i$."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class CausalSelfAttention(nn.Module):\n",
                "    def __init__(self, d_model, n_heads, max_len):\n",
                "        super().__init__()\n",
                "        self.d_model = d_model\n",
                "        self.n_heads = n_heads\n",
                "        self.d_k = d_model // n_heads\n",
                "        \n",
                "        # Merged Q, K, V projection for efficiency\n",
                "        self.c_attn = nn.Linear(d_model, 3 * d_model)\n",
                "        self.c_proj = nn.Linear(d_model, d_model)\n",
                "        \n",
                "        # Causal mask: Lower triangular matrix of ones\n",
                "        self.register_buffer(\"bias\", torch.tril(torch.ones(max_len, max_len))\n",
                "                                     .view(1, 1, max_len, max_len))\n",
                "        \n",
                "    def forward(self, x):\n",
                "        batch, seq_len, d_model = x.shape\n",
                "        \n",
                "        # Calculate Q, K, V\n",
                "        qkv = self.c_attn(x)\n",
                "        q, k, v = qkv.split(self.d_model, dim=2)\n",
                "        \n",
                "        # Reshape for multi-head attention\n",
                "        q = q.view(batch, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
                "        k = k.view(batch, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
                "        v = v.view(batch, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
                "        \n",
                "        # Scaled dot-product attention\n",
                "        scores = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(self.d_k))\n",
                "        \n",
                "        # Apply CAUSAL mask (mask future tokens)\n",
                "        mask = self.bias[:, :, :seq_len, :seq_len]\n",
                "        scores = scores.masked_fill(mask == 0, float('-inf'))\n",
                "        \n",
                "        # Softmax and output\n",
                "        att = F.softmax(scores, dim=-1)\n",
                "        y = att @ v\n",
                "        \n",
                "        # Reassemble heads\n",
                "        y = y.transpose(1, 2).contiguous().view(batch, seq_len, d_model)\n",
                "        return self.c_proj(y)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. GPT-2 Block (Pre-Norm)\n",
                "\n",
                "GPT-2 uses **Pre-LayerNorm**: `x = x + Sublayer(LayerNorm(x))`.\n",
                "This is more stable than Post-LayerNorm (used in BERT) for deep networks."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class GPTBlock(nn.Module):\n",
                "    def __init__(self, d_model, n_heads, d_ff, max_len):\n",
                "        super().__init__()\n",
                "        self.ln1 = nn.LayerNorm(d_model)\n",
                "        self.attn = CausalSelfAttention(d_model, n_heads, max_len)\n",
                "        self.ln2 = nn.LayerNorm(d_model)\n",
                "        self.mlp = nn.Sequential(\n",
                "            nn.Linear(d_model, d_ff),\n",
                "            nn.GELU(),\n",
                "            nn.Linear(d_ff, d_model)\n",
                "        )\n",
                "        \n",
                "    def forward(self, x):\n",
                "        # Pre-Norm: Norm -> Attention -> Resid\n",
                "        x = x + self.attn(self.ln1(x))\n",
                "        # Pre-Norm: Norm -> MLP -> Resid\n",
                "        x = x + self.mlp(self.ln2(x))\n",
                "        return x"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Full GPT-2 Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class GPT2(nn.Module):\n",
                "    def __init__(self, vocab_size, d_model, n_heads, d_ff, n_layers, max_len):\n",
                "        super().__init__()\n",
                "        self.token_emb = nn.Embedding(vocab_size, d_model)\n",
                "        self.pos_emb = nn.Embedding(max_len, d_model)\n",
                "        self.blocks = nn.ModuleList([\n",
                "            GPTBlock(d_model, n_heads, d_ff, max_len) for _ in range(n_layers)\n",
                "        ])\n",
                "        self.ln_f = nn.LayerNorm(d_model)\n",
                "        self.head = nn.Linear(d_model, vocab_size, bias=False)\n",
                "        \n",
                "        # Weight tying: embedding weights == output weights\n",
                "        self.token_emb.weight = self.head.weight\n",
                "        \n",
                "        self.max_len = max_len\n",
                "\n",
                "    def forward(self, idx):\n",
                "        batch, seq_len = idx.shape\n",
                "        assert seq_len <= self.max_len, f\"Seq len {seq_len} > max {self.max_len}\"\n",
                "        \n",
                "        pos_idx = torch.arange(seq_len, device=idx.device)\n",
                "        \n",
                "        # Token + Pos embeddings\n",
                "        x = self.token_emb(idx) + self.pos_emb(pos_idx)\n",
                "        \n",
                "        # Transformer blocks\n",
                "        for block in self.blocks:\n",
                "            x = block(x)\n",
                "            \n",
                "        x = self.ln_f(x)\n",
                "        logits = self.head(x)\n",
                "        return logits\n",
                "\n",
                "    def generate(self, idx, max_new_tokens, temperature=1.0):\n",
                "        \"\"\"Generate new tokens by repeatedly predicting the next token.\"\"\"\n",
                "        for _ in range(max_new_tokens):\n",
                "            # Crop to max_len if needed\n",
                "            idx_cond = idx if idx.size(1) <= self.max_len else idx[:, -self.max_len:]\n",
                "            \n",
                "            # Forward pass\n",
                "            logits = self(idx_cond)\n",
                "            \n",
                "            # Get last token prediction\n",
                "            logits = logits[:, -1, :] / temperature\n",
                "            probs = F.softmax(logits, dim=-1)\n",
                "            \n",
                "            # Sample\n",
                "            idx_next = torch.multinomial(probs, num_samples=1)\n",
                "            idx = torch.cat((idx, idx_next), dim=1)\n",
                "            \n",
                "        return idx\n",
                "\n",
                "# Initialize GPT-2 (small config for demo)\n",
                "model = GPT2(\n",
                "    vocab_size=1000,\n",
                "    d_model=128,\n",
                "    n_heads=4,\n",
                "    d_ff=256,\n",
                "    n_layers=4,\n",
                "    max_len=64\n",
                ").to(device)\n",
                "\n",
                "print(f\"GPT-2 Initialized with {sum(p.numel() for p in model.parameters()):,} parameters\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Test Generation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate random text (untrained model)\n",
                "start_tokens = torch.zeros((1, 1), dtype=torch.long, device=device)  # Start with token 0\n",
                "generated = model.generate(start_tokens, max_new_tokens=20)\n",
                "\n",
                "print(f\"Generated sequence (indices): {generated[0].tolist()}\")\n",
                "print(\"\\n(Note: The model is random, so output is meaningless noise)\")"
            ]
        }
    ]
}