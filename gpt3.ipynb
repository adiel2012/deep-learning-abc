{
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "colab": {
            "provenance": [],
            "gpuType": "T4"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        },
        "language_info": {
            "name": "python"
        },
        "accelerator": "GPU"
    },
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# GPT-3 (Reduced) from Scratch\n",
                "\n",
                "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/adiel2012/deep-learning-abc/blob/main/gpt3.ipynb)\n",
                "\n",
                "This notebook implements **GPT-3's architecture** from scratch.\n",
                "\n",
                "GPT-3 is very similar to GPT-2 (decoder-only Transformer), with a few key modifications for scale:\n",
                "1. **Alternating Dense and Sparse Attention**: Not implemented here for simplicity (standard attention is used).\n",
                "2. **Larger Scale**: 175B parameters (we implement a reduced version).\n",
                "3. **Few-Shot Learning**: The model is designed to learn from prompts without gradient updates."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install torch matplotlib"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "import math\n",
                "\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f'Using device: {device}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Simplified Sparse Attention Pattern (Mock)\n",
                "\n",
                "GPT-3 uses **sparse attention** in alternating layers to reduce computation. For this implementation, we will stick to standard attention but note where the difference lies."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Standard Causal Attention (Same as GPT-2)\n",
                "class GPT3Attention(nn.Module):\n",
                "    def __init__(self, d_model, n_heads, max_len):\n",
                "        super().__init__()\n",
                "        self.d_model = d_model\n",
                "        self.n_heads = n_heads\n",
                "        self.d_k = d_model // n_heads\n",
                "        \n",
                "        self.c_attn = nn.Linear(d_model, 3 * d_model)\n",
                "        self.c_proj = nn.Linear(d_model, d_model)\n",
                "        \n",
                "        # Causal mask\n",
                "        self.register_buffer(\"bias\", torch.tril(torch.ones(max_len, max_len))\n",
                "                                     .view(1, 1, max_len, max_len))\n",
                "        \n",
                "    def forward(self, x):\n",
                "        # Standard attention implemenation\n",
                "        B, T, C = x.size()\n",
                "        qkv = self.c_attn(x)\n",
                "        q, k, v = qkv.split(self.d_model, dim=2)\n",
                "        \n",
                "        q = q.view(B, T, self.n_heads, self.d_k).transpose(1, 2)\n",
                "        k = k.view(B, T, self.n_heads, self.d_k).transpose(1, 2)\n",
                "        v = v.view(B, T, self.n_heads, self.d_k).transpose(1, 2)\n",
                "        \n",
                "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(self.d_k))\n",
                "        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
                "        att = F.softmax(att, dim=-1)\n",
                "        y = att @ v\n",
                "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
                "        \n",
                "        return self.c_proj(y)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. GPT-3 Architecture\n",
                "\n",
                "The main difference is scale. GPT-3 small (125M) is basically GPT-2 base. \n",
                "GPT-3 175B uses:\n",
                "- 96 layers\n",
                "- d_model = 12288\n",
                "- 96 heads\n",
                "- Context window = 2048 tokens"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class GPT3(nn.Module):\n",
                "    def __init__(self, vocab_size, d_model, n_heads, n_layers, max_len):\n",
                "        super().__init__()\n",
                "        self.token_emb = nn.Embedding(vocab_size, d_model)\n",
                "        self.pos_emb = nn.Embedding(max_len, d_model)\n",
                "        self.drop = nn.Dropout(0.1)\n",
                "        \n",
                "        self.blocks = nn.ModuleList([\n",
                "            GPTBlock(d_model, n_heads, 4 * d_model, max_len) for _ in range(n_layers)\n",
                "        ])\n",
                "        \n",
                "        self.ln_f = nn.LayerNorm(d_model)\n",
                "        self.head = nn.Linear(d_model, vocab_size, bias=False)\n",
                "        self.max_len = max_len\n",
                "\n",
                "    def forward(self, idx):\n",
                "        b, t = idx.size()\n",
                "        t_pos = torch.arange(0, t, dtype=torch.long, device=idx.device)\n",
                "        \n",
                "        x = self.token_emb(idx) + self.pos_emb(t_pos)\n",
                "        x = self.drop(x)\n",
                "        \n",
                "        for block in self.blocks:\n",
                "            x = block(x)\n",
                "            \n",
                "        x = self.ln_f(x)\n",
                "        logits = self.head(x)\n",
                "        return logits\n",
                "    \n",
                "# Reuse GPTBlock from GPT-2 notebook (simulating sharing)\n",
                "class GPTBlock(nn.Module):\n",
                "    def __init__(self, d_model, n_heads, d_ff, max_len):\n",
                "        super().__init__()\n",
                "        self.ln1 = nn.LayerNorm(d_model)\n",
                "        self.attn = GPT3Attention(d_model, n_heads, max_len)\n",
                "        self.ln2 = nn.LayerNorm(d_model)\n",
                "        self.mlp = nn.Sequential(\n",
                "            nn.Linear(d_model, d_ff),\n",
                "            nn.GELU(),\n",
                "            nn.Linear(d_ff, d_model),\n",
                "            nn.Dropout(0.1)\n",
                "        )\n",
                "        \n",
                "    def forward(self, x):\n",
                "        x = x + self.attn(self.ln1(x))\n",
                "        x = x + self.mlp(self.ln2(x))\n",
                "        return x\n",
                "\n",
                "# Instantiating \"GPT-3 Small\" (similar to GPT-2 base)\n",
                "model = GPT3(\n",
                "    vocab_size=50257, # GPT-2/3 tokenizer size\n",
                "    d_model=768,\n",
                "    n_heads=12,\n",
                "    n_layers=12,\n",
                "    max_len=1024       # Reduced context for demo\n",
                ").to(device)\n",
                "\n",
                "print(f\"GPT-3 Small initialized: {sum(p.numel() for p in model.parameters())/1e6:.1f}M params\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Few-Shot Prompting Concept\n",
                "\n",
                "GPT-3's power comes from **in-context learning**. Instead of fine-tuning weights (like BERT), we feed examples in the prompt."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "prompt = \"\"\"\n",
                "Translate English to French:\n",
                "sea otter => loutre de mer\n",
                "peppermint => menthe poivrÃ©e\n",
                "plush giraffe => girafe peluche\n",
                "cheese =>\n",
                "\"\"\"\n",
                "\n",
                "print(\"Few-Shot Prompt Structure:\")\n",
                "print(prompt)\n",
                "print(\"(The model completes the pattern: 'fromage')\")"
            ]
        }
    ]
}