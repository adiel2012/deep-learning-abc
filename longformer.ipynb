{
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "colab": {
            "provenance": [],
            "gpuType": "T4"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        },
        "language_info": {
            "name": "python"
        },
        "accelerator": "GPU"
    },
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Longformer from Scratch (Sparse Attention)\n",
                "\n",
                "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/adiel2012/deep-learning-abc/blob/main/longformer.ipynb)\n",
                "\n",
                "Longformer scales Transformers to long sequences (e.g., 4096 tokens) by replacing $O(N^2)$ full attention with **sparse attention** patterns.\n",
                "\n",
                "It combines three local attention patterns:\n",
                "1. **Sliding Window:** Attend to fixed # of neighbors.\n",
                "2. **Dilated Sliding Window:** Attend to neighbors with gaps.\n",
                "3. **Global Attention:** Specific tokens (like `[CLS]`) attend to everything, and everything attends to them."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install torch matplotlib"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "import math\n",
                "\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f'Using device: {device}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Sliding Window Attention (Naive Implementation)\n",
                "\n",
                "Efficient implementations require custom CUDA kernels. Here we simulate it by masking the full attention matrix."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def create_sliding_window_mask(seq_len, window_size, device):\n",
                "    \"\"\"Creates a mask where each token attends to W/2 neighbors on each side.\"\"\"\n",
                "    # Standard attention mask is 1s everywhere\n",
                "    attn_mask = torch.ones((seq_len, seq_len), device=device)\n",
                "    \n",
                "    # Create band mask\n",
                "    # abs(i - j) <= w/2\n",
                "    rows = torch.arange(seq_len, device=device)[:, None]\n",
                "    cols = torch.arange(seq_len, device=device)[None, :]\n",
                "    dist = torch.abs(rows - cols)\n",
                "    \n",
                "    mask = (dist <= (window_size // 2)).float()\n",
                "    \n",
                "    # Use -inf for masked (0) areas\n",
                "    return mask.masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, 0.0)\n",
                "\n",
                "window_size = 4\n",
                "mask = create_sliding_window_mask(10, window_size, device)\n",
                "\n",
                "print(f\"Sliding Window Mask (Window={window_size}):\")\n",
                "print(mask)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Global Attention (for [CLS])\n",
                "\n",
                "We add global attention indices. Typically index 0 (`[CLS]`) is global."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def add_global_attention(mask, global_indices):\n",
                "    \"\"\"Updates mask so global indices attend to all, and all attend to them.\"\"\"\n",
                "    for idx in global_indices:\n",
                "        mask[idx, :] = 0.0  # Global token sees everything\n",
                "        mask[:, idx] = 0.0  # Everything sees global token\n",
                "    return mask\n",
                "\n",
                "mask = add_global_attention(mask, [0])\n",
                "print(\"Sliding Window + Global Attention at [0]:\")\n",
                "print(mask)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Longformer Self-Attention Layer\n",
                "\n",
                "Combines local (sliding window) and global attention."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class LongformerSelfAttention(nn.Module):\n",
                "    def __init__(self, d_model, n_heads, window_size):\n",
                "        super().__init__()\n",
                "        self.d_model = d_model\n",
                "        self.n_heads = n_heads\n",
                "        self.window_size = window_size\n",
                "        \n",
                "        self.query = nn.Linear(d_model, d_model)\n",
                "        self.key = nn.Linear(d_model, d_model)\n",
                "        self.value = nn.Linear(d_model, d_model)\n",
                "        \n",
                "        # Global projections (often separate parameters in Longformer)\n",
                "        self.query_global = nn.Linear(d_model, d_model)\n",
                "        self.key_global = nn.Linear(d_model, d_model)\n",
                "        self.value_global = nn.Linear(d_model, d_model)\n",
                "        \n",
                "        self.out = nn.Linear(d_model, d_model)\n",
                "\n",
                "    def forward(self, x, global_mask):\n",
                "        # x: [batch, seq_len, d_model]\n",
                "        # global_mask: [batch, seq_len] (1 if global, 0 if local)\n",
                "        \n",
                "        B, T, D = x.shape\n",
                "        \n",
                "        # 1. Local Attention (Sliding Window)\n",
                "        q = self.query(x).view(B, T, self.n_heads, -1).transpose(1, 2)\n",
                "        k = self.key(x).view(B, T, self.n_heads, -1).transpose(1, 2)\n",
                "        v = self.value(x).view(B, T, self.n_heads, -1).transpose(1, 2)\n",
                "        \n",
                "        # Create sliding window mask (naive)\n",
                "        local_mask = create_sliding_window_mask(T, self.window_size, x.device)\n",
                "        \n",
                "        scores_local = (q @ k.transpose(-2, -1)) / math.sqrt(D // self.n_heads)\n",
                "        scores_local = scores_local + local_mask\n",
                "        attn_local = torch.softmax(scores_local, dim=-1)\n",
                "        out_local = attn_local @ v\n",
                "        \n",
                "        # 2. Global Attention\n",
                "        # For simplicity, we compute full attention for global tokens and merge\n",
                "        # Real Longformer implements this efficiently without full matrix\n",
                "        \n",
                "        q_g = self.query_global(x).view(B, T, self.n_heads, -1).transpose(1, 2)\n",
                "        k_g = self.key_global(x).view(B, T, self.n_heads, -1).transpose(1, 2)\n",
                "        v_g = self.value_global(x).view(B, T, self.n_heads, -1).transpose(1, 2)\n",
                "        \n",
                "        # Full attention scores\n",
                "        scores_global = (q_g @ k_g.transpose(-2, -1)) / math.sqrt(D // self.n_heads)\n",
                "        attn_global = torch.softmax(scores_global, dim=-1)\n",
                "        out_global = attn_global @ v_g\n",
                "        \n",
                "        # 3. Merge\n",
                "        # If a token is global, take global output. Else, take local.\n",
                "        # global_mask: [batch, len] -> expand to [batch, heads, len, dim]\n",
                "        mask = global_mask.view(B, 1, T, 1).expand(-1, self.n_heads, -1, D // self.n_heads)\n",
                "        \n",
                "        out = torch.where(mask > 0, out_global, out_local)\n",
                "        out = out.transpose(1, 2).contiguous().view(B, T, D)\n",
                "        \n",
                "        return self.out(out)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test\n",
                "model = LongformerSelfAttention(d_model=64, n_heads=4, window_size=4).to(device)\n",
                "x = torch.randn(1, 10, 64, device=device)\n",
                "g_mask = torch.zeros(1, 10, device=device)\n",
                "g_mask[0, 0] = 1 # [CLS] is global\n",
                "\n",
                "out = model(x, g_mask)\n",
                "print(f\"Longformer Attention Output: {out.shape}\")"
            ]
        }
    ]
}