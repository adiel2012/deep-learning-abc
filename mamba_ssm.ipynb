{
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "colab": {
            "provenance": [],
            "gpuType": "T4"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        },
        "language_info": {
            "name": "python"
        },
        "accelerator": "GPU"
    },
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Mamba: Linear-Time Sequence Modeling with Selective State Spaces\n",
                "\n",
                "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/adiel2012/deep-learning-abc/blob/main/mamba_ssm.ipynb)\n",
                "\n",
                "Mamba (State Space Models) offers Transformer-quality performance with linear time scaling $O(N)$.\n",
                "\n",
                "Key Innovations:\n",
                "1. **Selection Mechanism:** The SSM parameters ($B, C, \\Delta$) are functions of the input $x_t$, allowing the model to \"select\" what to remember/forget.\n",
                "2. **Hardware-aware Algorithm:** Uses a parallel associative scan (prefix sum) to compute the recurrence efficiently on GPU.\n",
                "\n",
                "$$ h_t = \\bar{A}_t h_{t-1} + \\bar{B}_t x_t $$ \n",
                "$$ y_t = C_t h_t $$"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "import math\n",
                "\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f'Using device: {device}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Selective Scan (Simulation)\n",
                "\n",
                "The core of Mamba is the \"Selective Scan\". It's a recurrence where the transition matrices change at every timestep based on the input."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def selective_scan_ref(u, delta, A, B, C, D=None):\n",
                "    \"\"\"\n",
                "    Reference implementation of Selective Scan (Sequential).\n",
                "    u: (B, L, D_in) - Input\n",
                "    delta: (B, L, D_in) - Time step parameter\n",
                "    A: (D_in, N) - State transition (diagonal)\n",
                "    B: (B, L, N) - Input projection (Input dependent)\n",
                "    C: (B, L, N) - Output projection (Input dependent)\n",
                "    D: (D_in) - Residual\n",
                "    \"\"\"\n",
                "    batch_size, seq_len, d_in = u.shape\n",
                "    n = A.shape[1]\n",
                "    \n",
                "    # Discretize A\n",
                "    # deltaA = exp(delta * A)\n",
                "    deltaA = torch.exp(torch.einsum('bld,dn->bldn', delta, A))\n",
                "    \n",
                "    # Discretize B\n",
                "    # deltaB = delta * B\n",
                "    deltaB = torch.einsum('bld,bln->bldn', delta, B)\n",
                "    \n",
                "    # Recurrence\n",
                "    x = torch.zeros((batch_size, d_in, n), device=u.device)\n",
                "    ys = []\n",
                "    \n",
                "    for t in range(seq_len):\n",
                "        x = deltaA[:, t] * x + deltaB[:, t] * u[:, t].unsqueeze(-1)\n",
                "        y = torch.einsum('bdn,b n->bd', x, C[:, t]) # (B, D_in)\n",
                "        ys.append(y)\n",
                "        \n",
                "    y = torch.stack(ys, dim=1) # (B, L, D_in)\n",
                "    \n",
                "    if D is not None:\n",
                "        y = y + u * D\n",
                "        \n",
                "    return y"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Mamba Block\n",
                "\n",
                "A Mamba block combines projections, convolution (for local context), and the Selective Scan."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class MambaBlock(nn.Module):\n",
                "    def __init__(self, d_model, d_state=16, expand=2):\n",
                "        super().__init__()\n",
                "        self.d_model = d_model\n",
                "        self.d_state = d_state\n",
                "        self.d_inner = int(expand * d_model)\n",
                "        \n",
                "        # 1. Input Projections\n",
                "        self.in_proj = nn.Linear(d_model, self.d_inner * 2)\n",
                "        \n",
                "        # 2. Convolution (1D)\n",
                "        self.conv1d = nn.Conv1d(\n",
                "            in_channels=self.d_inner,\n",
                "            out_channels=self.d_inner,\n",
                "            kernel_size=4,\n",
                "            groups=self.d_inner,\n",
                "            padding=3\n",
                "        )\n",
                "        \n",
                "        # 3. Parameters for Selective Scan (Input Dependent!)\n",
                "        self.x_proj = nn.Linear(self.d_inner,  d_state + d_state + self.d_inner)\n",
                "        # The line above projects input to [delta, B, C]\n",
                "        \n",
                "        self.dt_proj = nn.Linear(d_state, self.d_inner, bias=True)\n",
                "        \n",
                "        # Fixed parameters A and D\n",
                "        A = torch.arange(1, d_state + 1, dtype=torch.float32).repeat(self.d_inner, 1)\n",
                "        self.A_log = nn.Parameter(torch.log(A))\n",
                "        self.D = nn.Parameter(torch.ones(self.d_inner))\n",
                "        \n",
                "        # 4. Output Projection\n",
                "        self.out_proj = nn.Linear(self.d_inner, d_model)\n",
                "        \n",
                "    def forward(self, x):\n",
                "        # x: (B, L, D_model)\n",
                "        batch_size, seq_len, _ = x.shape\n",
                "        \n",
                "        x_and_res = self.in_proj(x)\n",
                "        x, res = x_and_res.chunk(2, dim=-1)\n",
                "        \n",
                "        x = x.transpose(1, 2)\n",
                "        x = self.conv1d(x)[:, :, :seq_len]\n",
                "        x = F.silu(x)\n",
                "        x = x.transpose(1, 2) # (B, L, D_inner)\n",
                "        \n",
                "        # Compute input-dependent parameters\n",
                "        # Project x to get delta, B, C\n",
                "        # x_proj outputs: dt (D_inner) + B (D_state) + C (D_state) ? Close -- usually it's:\n",
                "        # For simplification, we map x -> (delta, B, C)\n",
                "        # Let's assume standard Mamba shapes for simplicity\n",
                "        \n",
                "        delta_B_C = self.x_proj(x)\n",
                "        \n",
                "        # Split them\n",
                "        # This part requires careful dimension handling matching the official implementation\n",
                "        # Here we do a simplified version:\n",
                "        # delta: (B, L, D_inner)\n",
                "        # B: (B, L, D_state)\n",
                "        # C: (B, L, D_state)\n",
                "        \n",
                "        delta = delta_B_C[:, :, :self.d_inner]\n",
                "        B = delta_B_C[:, :, self.d_inner : self.d_inner + self.d_state]\n",
                "        C = delta_B_C[:, :, self.d_inner + self.d_state :]\n",
                "        \n",
                "        delta = F.softplus(delta) # Ensure positive time step\n",
                "        A = -torch.exp(self.A_log) # Ensure A is negative for stability\n",
                "        \n",
                "        # Run SSM\n",
                "        y = selective_scan_ref(x, delta, A, B, C, self.D)\n",
                "        \n",
                "        y = y * F.silu(res) # Res connection gating\n",
                "        return self.out_proj(y)\n",
                "\n",
                "# Test\n",
                "block = MambaBlock(d_model=64, d_state=16).to(device)\n",
                "x = torch.randn(2, 32, 64).to(device)\n",
                "out = block(x)\n",
                "print(\"Input:\", x.shape)\n",
                "print(\"Output:\", out.shape)"
            ]
        }
    ]
}