{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Mamba: Linear-Time Sequence Modeling with Selective State Spaces\n",
                "\n",
                "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/adiel2012/deep-learning-abc/blob/main/mamba_ssm.ipynb)\n",
                "\n",
                "## 1. Mathematical Foundations: Continuous to Discrete SSM\n",
                "\n",
                "State Space Models (SSMs) map a 1D function or sequence $x(t) \\in \\mathbb{R}$ to $y(t) \\in \\mathbb{R}$ through a latent state $h(t) \\in \\mathbb{R}^N$.\n",
                "\n",
                "### Continuous System\n",
                "$$ \\dot{h}(t) = \\mathbf{A} h(t) + \\mathbf{B} x(t) $$\n",
                "$$ y(t) = \\mathbf{C} h(t) $$\n",
                "\n",
                "where $\\mathbf{A}$ determines the state evolution and $\\mathbf{B}, \\mathbf{C}$ are projections.\n",
                "\n",
                "### Discretization (Zero-Order Hold)\n",
                "To handle discrete data (text), we discretize the system using a step size $\\Delta_t$ (which is input-dependent in Mamba). Using the Zero-Order Hold (ZOH) assumption:\n",
                "\n",
                "$$ \\bar{\\mathbf{A}}_t = \\exp(\\Delta_t \\mathbf{A}) $$\n",
                "$$ \\bar{\\mathbf{B}}_t = (\\Delta_t \\mathbf{A})^{-1} (\\exp(\\Delta_t \\mathbf{A}) - \\mathbf{I}) \\cdot \\Delta_t \\mathbf{B} \\approx \\Delta_t \\mathbf{B} $$\n",
                "\n",
                "The discrete recurrence becomes:\n",
                "$$ h_t = \\bar{\\mathbf{A}}_t h_{t-1} + \\bar{\\mathbf{B}}_t x_t $$\n",
                "$$ y_t = \\mathbf{C}_t h_t $$\n",
                "\n",
                "### Selectivity\n",
                "In standard SSMs (S4), $\\mathbf{A}, \\mathbf{B}, \\mathbf{C}$ are time-invariant. In Mamba, $\\Delta_t, \\mathbf{B}_t, \\mathbf{C}_t$ are functions of the input $x_t$, allowing the model to selectively remember or ignore information."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install torch torchvision matplotlib"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "import math\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f'Using device: {device}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Selective Scan Implementation (Simulation)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def selective_scan_ref(u, delta, A, B, C, D=None):\n",
                "    \"\"\"\n",
                "    Reference implementation of Selective Scan (Sequential).\n",
                "    u: (B, L, D_in) - Input\n",
                "    delta: (B, L, D_in) - Time step parameter\n",
                "    A: (D_in, N) - State transition (diagonal)\n",
                "    B: (B, L, N) - Input projection (Input dependent)\n",
                "    C: (B, L, N) - Output projection (Input dependent)\n",
                "    D: (D_in) - Residual\n",
                "    \"\"\"\n",
                "    batch_size, seq_len, d_in = u.shape\n",
                "    n = A.shape[1]\n",
                "    \n",
                "    # Discretize A\n",
                "    # deltaA = exp(delta * A)\n",
                "    deltaA = torch.exp(torch.einsum('bld,dn->bldn', delta, A))\n",
                "    \n",
                "    # Discretize B\n",
                "    # deltaB = delta * B\n",
                "    deltaB = torch.einsum('bld,bln->bldn', delta, B)\n",
                "    \n",
                "    # Recurrence\n",
                "    x = torch.zeros((batch_size, d_in, n), device=u.device)\n",
                "    ys = []\n",
                "    \n",
                "    # For visualization of one channel's state\n",
                "    h_trace = []\n",
                "    \n",
                "    for t in range(seq_len):\n",
                "        x = deltaA[:, t] * x + deltaB[:, t] * u[:, t].unsqueeze(-1)\n",
                "        y = torch.einsum('bdn,b n->bd', x, C[:, t]) # (B, D_in)\n",
                "        ys.append(y)\n",
                "        h_trace.append(x[0, 0, :].detach().cpu()) # Store state of batch 0, channel 0\n",
                "        \n",
                "    y = torch.stack(ys, dim=1) # (B, L, D_in)\n",
                "    \n",
                "    if D is not None:\n",
                "        y = y + u * D\n",
                "        \n",
                "    return y, h_trace"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Mamba Block"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class MambaBlock(nn.Module):\n",
                "    def __init__(self, d_model, d_state=16, expand=2):\n",
                "        super().__init__()\n",
                "        self.d_model = d_model\n",
                "        self.d_state = d_state\n",
                "        self.d_inner = int(expand * d_model)\n",
                "        \n",
                "        self.in_proj = nn.Linear(d_model, self.d_inner * 2)\n",
                "        \n",
                "        self.conv1d = nn.Conv1d(\n",
                "            in_channels=self.d_inner,\n",
                "            out_channels=self.d_inner,\n",
                "            kernel_size=4,\n",
                "            groups=self.d_inner,\n",
                "            padding=3\n",
                "        )\n",
                "        \n",
                "        self.x_proj = nn.Linear(self.d_inner,  d_state + d_state + self.d_inner)\n",
                "        self.dt_proj = nn.Linear(d_state, self.d_inner, bias=True)\n",
                "        \n",
                "        A = torch.arange(1, d_state + 1, dtype=torch.float32).repeat(self.d_inner, 1)\n",
                "        self.A_log = nn.Parameter(torch.log(A))\n",
                "        self.D = nn.Parameter(torch.ones(self.d_inner))\n",
                "        self.out_proj = nn.Linear(self.d_inner, d_model)\n",
                "        \n",
                "    def forward(self, x, return_trace=False):\n",
                "        batch_size, seq_len, _ = x.shape\n",
                "        x_and_res = self.in_proj(x)\n",
                "        x, res = x_and_res.chunk(2, dim=-1)\n",
                "        \n",
                "        x = x.transpose(1, 2)\n",
                "        x = self.conv1d(x)[:, :, :seq_len]\n",
                "        x = F.silu(x)\n",
                "        x = x.transpose(1, 2)\n",
                "        \n",
                "        delta_B_C = self.x_proj(x)\n",
                "        delta = delta_B_C[:, :, :self.d_inner]\n",
                "        B = delta_B_C[:, :, self.d_inner : self.d_inner + self.d_state]\n",
                "        C = delta_B_C[:, :, self.d_inner + self.d_state :]\n",
                "        \n",
                "        delta = F.softplus(delta)\n",
                "        A = -torch.exp(self.A_log)\n",
                "        \n",
                "        y, h_trace = selective_scan_ref(x, delta, A, B, C, self.D)\n",
                "        \n",
                "        y = y * F.silu(res)\n",
                "        out = self.out_proj(y)\n",
                "        \n",
                "        if return_trace:\n",
                "            return out, h_trace\n",
                "        return out\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Visualization: Hidden State Dynamics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "block = MambaBlock(d_model=64, d_state=16).to(device)\n",
                "x = torch.randn(1, 50, 64).to(device) # Single sequence\n",
                "out, h_trace = block(x, return_trace=True)\n",
                "\n",
                "# Stack trace: (Seq_Len, D_state)\n",
                "h_trace = torch.stack(h_trace).numpy()\n",
                "\n",
                "plt.figure(figsize=(10, 4))\n",
                "plt.plot(h_trace)\n",
                "plt.title(\"Evolution of Hidden State $h_t$ (16 dimensions) over Time\")\n",
                "plt.xlabel(\"Time Step\")\n",
                "plt.ylabel(\"State Value\")\n",
                "plt.grid(True, alpha=0.3)\n",
                "plt.show()\n",
                "\n",
                "print(\"The hidden state evolves based on input selectivity.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}