{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mixture of Experts (MoE) from Scratch\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/adiel2012/deep-learning-abc/blob/main/mixture_of_experts.ipynb)\n",
    "\n",
    "This notebook implements the Mixture of Experts (MoE) mechanism from scratch, as used in modern LLMs like Mixtral, Switch Transformer, and DeepSeek.\n",
    "\n",
    "**Key idea:** Replace the dense FFN with multiple \"expert\" FFNs and a learned router that sends each token to only 1-2 experts. This enables massive models where only a fraction of parameters are active per token.\n",
    "\n",
    "We cover:\n",
    "1. Dense FFN baseline\n",
    "2. Top-1 routing (Switch Transformer style)\n",
    "3. Top-2 routing (Mixtral style)\n",
    "4. Load balancing loss\n",
    "5. Expert utilization analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Mathematical Foundations\n",
    "\n",
    "### The Scaling Problem\n",
    "\n",
    "Larger models perform better, but compute grows linearly with parameters. A 175B model needs ~175B FLOPs per token.\n",
    "\n",
    "**MoE insight:** Not every token needs every parameter. Replace the dense FFN with $E$ experts and activate only $k$ experts per token:\n",
    "\n",
    "$$\\text{MoE}(x) = \\sum_{i \\in \\text{TopK}} g_i(x) \\cdot \\text{Expert}_i(x)$$\n",
    "\n",
    "where $g(x) = \\text{softmax}(x \\cdot W_{\\text{router}})$ is the router, and TopK selects the $k$ experts with highest router scores.\n",
    "\n",
    "### Example: Mixtral 8x7B\n",
    "- 8 experts, activate 2 per token\n",
    "- Total: 47B parameters → Active: 13B per token\n",
    "- Matches or beats dense 70B models!\n",
    "\n",
    "### Load Balancing\n",
    "\n",
    "Without encouragement, the router collapses to always picking the same experts. The **auxiliary load balancing loss** penalizes uneven expert usage:\n",
    "\n",
    "$$\\mathcal{L}_{\\text{balance}} = E \\cdot \\sum_{i=1}^{E} f_i \\cdot p_i$$\n",
    "\n",
    "where $f_i$ = fraction of tokens routed to expert $i$, and $p_i$ = mean router probability for expert $i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dense FFN Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_ffn(x, W1, b1, W2, b2):\n",
    "    \"\"\"Standard dense FFN: all parameters used for every token.\"\"\"\n",
    "    return torch.relu(x @ W1 + b1) @ W2 + b2\n",
    "\n",
    "# Setup\n",
    "torch.manual_seed(42)\n",
    "d_model = 32\n",
    "d_ff = 128\n",
    "batch, seq_len = 2, 8\n",
    "\n",
    "x = torch.randn(batch, seq_len, d_model, device=device)\n",
    "W1 = torch.randn(d_model, d_ff, device=device) * 0.1\n",
    "b1 = torch.zeros(d_ff, device=device)\n",
    "W2 = torch.randn(d_ff, d_model, device=device) * 0.1\n",
    "b2 = torch.zeros(d_model, device=device)\n",
    "\n",
    "out_dense = dense_ffn(x, W1, b1, W2, b2)\n",
    "dense_params = d_model * d_ff + d_ff + d_ff * d_model + d_model\n",
    "\n",
    "print(f'Dense FFN:')\n",
    "print(f'  Output shape: {out_dense.shape}')\n",
    "print(f'  Total parameters: {dense_params}')\n",
    "print(f'  Active parameters per token: {dense_params} (100%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. MoE with Top-1 Routing (Switch Transformer)\n",
    "\n",
    "Each token goes to exactly one expert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_experts(n_experts, d_model, d_ff, device):\n",
    "    \"\"\"Initialize multiple expert FFN networks.\"\"\"\n",
    "    experts = []\n",
    "    for _ in range(n_experts):\n",
    "        expert = {\n",
    "            'W1': torch.randn(d_model, d_ff, device=device) * 0.1,\n",
    "            'b1': torch.zeros(d_ff, device=device),\n",
    "            'W2': torch.randn(d_ff, d_model, device=device) * 0.1,\n",
    "            'b2': torch.zeros(d_model, device=device),\n",
    "        }\n",
    "        experts.append(expert)\n",
    "    return experts\n",
    "\n",
    "def expert_ffn(x, expert):\n",
    "    \"\"\"Apply a single expert FFN.\"\"\"\n",
    "    return torch.relu(x @ expert['W1'] + expert['b1']) @ expert['W2'] + expert['b2']\n",
    "\n",
    "def moe_top1(x, router_weights, experts):\n",
    "    \"\"\"Mixture of Experts with top-1 routing.\n",
    "    \n",
    "    Args:\n",
    "        x: (batch, seq_len, d_model)\n",
    "        router_weights: (d_model, n_experts) — learned routing matrix\n",
    "        experts: list of expert parameter dicts\n",
    "    \n",
    "    Returns:\n",
    "        output: (batch, seq_len, d_model)\n",
    "        router_probs: (batch*seq_len, n_experts) — routing probabilities\n",
    "        expert_indices: (batch*seq_len,) — which expert was selected\n",
    "    \"\"\"\n",
    "    batch, seq_len, d_model = x.shape\n",
    "    n_experts = len(experts)\n",
    "    \n",
    "    # Flatten batch and sequence dimensions\n",
    "    x_flat = x.view(-1, d_model)  # (batch*seq_len, d_model)\n",
    "    n_tokens = x_flat.shape[0]\n",
    "    \n",
    "    # Router: compute probabilities for each expert\n",
    "    router_logits = x_flat @ router_weights  # (n_tokens, n_experts)\n",
    "    router_probs = torch.softmax(router_logits, dim=-1)  # (n_tokens, n_experts)\n",
    "    \n",
    "    # Select top-1 expert per token\n",
    "    expert_weights, expert_indices = router_probs.max(dim=-1)  # (n_tokens,), (n_tokens,)\n",
    "    \n",
    "    # Route each token to its selected expert\n",
    "    output = torch.zeros_like(x_flat)\n",
    "    for i in range(n_experts):\n",
    "        mask = (expert_indices == i)  # which tokens go to expert i\n",
    "        if mask.any():\n",
    "            expert_input = x_flat[mask]  # (n_selected, d_model)\n",
    "            expert_output = expert_ffn(expert_input, experts[i])\n",
    "            output[mask] = expert_weights[mask].unsqueeze(-1) * expert_output\n",
    "    \n",
    "    return output.view(batch, seq_len, d_model), router_probs, expert_indices\n",
    "\n",
    "# Test\n",
    "torch.manual_seed(42)\n",
    "n_experts = 4\n",
    "d_ff_expert = d_ff  # Each expert is same size as the dense FFN\n",
    "\n",
    "experts = init_experts(n_experts, d_model, d_ff_expert, device)\n",
    "router_W = torch.randn(d_model, n_experts, device=device) * 0.1\n",
    "\n",
    "out_moe, probs, indices = moe_top1(x, router_W, experts)\n",
    "\n",
    "total_moe_params = n_experts * dense_params + d_model * n_experts  # experts + router\n",
    "active_moe_params = dense_params  # only 1 expert active per token\n",
    "\n",
    "print(f'MoE Top-1 ({n_experts} experts):')\n",
    "print(f'  Output shape: {out_moe.shape}')\n",
    "print(f'  Total parameters: {total_moe_params}')\n",
    "print(f'  Active parameters per token: {active_moe_params} ({100*active_moe_params/total_moe_params:.1f}%)')\n",
    "print(f'\\nRouting decisions (expert index per token):')\n",
    "print(f'  {indices.view(batch, seq_len).cpu()}')\n",
    "print(f'\\nRouter probabilities (first 4 tokens):')\n",
    "for t in range(4):\n",
    "    p = probs[t].cpu()\n",
    "    print(f'  Token {t}: {[f\"{v:.3f}\" for v in p.tolist()]} → Expert {indices[t].item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize routing decisions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Router probabilities heatmap\n",
    "im = axes[0].imshow(probs.detach().cpu().numpy(), cmap='YlOrRd', aspect='auto')\n",
    "axes[0].set_xlabel('Expert')\n",
    "axes[0].set_ylabel('Token index (batch*seq_len)')\n",
    "axes[0].set_title('Router Probabilities per Token')\n",
    "plt.colorbar(im, ax=axes[0])\n",
    "\n",
    "# Expert load distribution\n",
    "expert_counts = [(indices == i).sum().item() for i in range(n_experts)]\n",
    "colors = plt.cm.Set2(range(n_experts))\n",
    "axes[1].bar(range(n_experts), expert_counts, color=colors)\n",
    "axes[1].axhline(y=len(indices) / n_experts, color='red', linestyle='--', label=f'Ideal ({len(indices)//n_experts} each)')\n",
    "axes[1].set_xlabel('Expert')\n",
    "axes[1].set_ylabel('Number of tokens')\n",
    "axes[1].set_title('Expert Load Distribution')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. MoE with Top-2 Routing (Mixtral Style)\n",
    "\n",
    "Each token is processed by 2 experts, with outputs weighted by router probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moe_top2(x, router_weights, experts):\n",
    "    \"\"\"Mixture of Experts with top-2 routing (Mixtral style).\n",
    "    \n",
    "    Each token goes to 2 experts; outputs are weighted by\n",
    "    normalized router probabilities.\n",
    "    \"\"\"\n",
    "    batch, seq_len, d_model = x.shape\n",
    "    n_experts = len(experts)\n",
    "    \n",
    "    x_flat = x.view(-1, d_model)\n",
    "    n_tokens = x_flat.shape[0]\n",
    "    \n",
    "    # Router\n",
    "    router_logits = x_flat @ router_weights\n",
    "    router_probs = torch.softmax(router_logits, dim=-1)\n",
    "    \n",
    "    # Select top-2 experts per token\n",
    "    top2_weights, top2_indices = router_probs.topk(2, dim=-1)  # (n_tokens, 2)\n",
    "    \n",
    "    # Normalize weights for selected experts (they should sum to 1)\n",
    "    top2_weights = top2_weights / top2_weights.sum(dim=-1, keepdim=True)\n",
    "    \n",
    "    # Route tokens to experts\n",
    "    output = torch.zeros_like(x_flat)\n",
    "    for k in range(2):  # for each of the 2 selected experts\n",
    "        for i in range(n_experts):\n",
    "            mask = (top2_indices[:, k] == i)\n",
    "            if mask.any():\n",
    "                expert_input = x_flat[mask]\n",
    "                expert_output = expert_ffn(expert_input, experts[i])\n",
    "                output[mask] += top2_weights[mask, k].unsqueeze(-1) * expert_output\n",
    "    \n",
    "    return output.view(batch, seq_len, d_model), router_probs, top2_indices, top2_weights\n",
    "\n",
    "# Test\n",
    "torch.manual_seed(42)\n",
    "n_experts = 8  # Mixtral uses 8 experts\n",
    "experts_8 = init_experts(n_experts, d_model, d_ff_expert, device)\n",
    "router_W_8 = torch.randn(d_model, n_experts, device=device) * 0.1\n",
    "\n",
    "out_top2, probs_8, top2_idx, top2_w = moe_top2(x, router_W_8, experts_8)\n",
    "\n",
    "total_params_8 = n_experts * dense_params + d_model * n_experts\n",
    "active_params_8 = 2 * dense_params  # 2 experts active per token\n",
    "\n",
    "print(f'MoE Top-2 ({n_experts} experts, like Mixtral):')\n",
    "print(f'  Output shape: {out_top2.shape}')\n",
    "print(f'  Total parameters: {total_params_8}')\n",
    "print(f'  Active per token: {active_params_8} ({100*active_params_8/total_params_8:.1f}%)')\n",
    "print(f'\\nRouting decisions (top-2 experts per token):')\n",
    "for t in range(min(8, batch * seq_len)):\n",
    "    e1, e2 = top2_idx[t].cpu().tolist()\n",
    "    w1, w2 = top2_w[t].cpu().tolist()\n",
    "    print(f'  Token {t}: Expert {e1} (w={w1:.3f}) + Expert {e2} (w={w2:.3f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Balancing Loss\n",
    "\n",
    "Without load balancing, the router tends to collapse — always sending tokens to the same 1-2 experts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_balancing_loss(router_probs, expert_indices, n_experts):\n",
    "    \"\"\"Compute auxiliary load balancing loss.\n",
    "    \n",
    "    Encourages uniform expert utilization.\n",
    "    Loss = E * sum(f_i * p_i) where:\n",
    "      f_i = fraction of tokens routed to expert i\n",
    "      p_i = mean router probability for expert i\n",
    "    \n",
    "    Minimum when all experts get equal traffic.\n",
    "    \"\"\"\n",
    "    n_tokens = router_probs.shape[0]\n",
    "    \n",
    "    # f_i: fraction of tokens assigned to each expert\n",
    "    # For top-2, a token counts for both selected experts\n",
    "    if expert_indices.dim() == 2:  # top-k\n",
    "        f = torch.zeros(n_experts, device=router_probs.device)\n",
    "        for k in range(expert_indices.shape[1]):\n",
    "            for i in range(n_experts):\n",
    "                f[i] += (expert_indices[:, k] == i).float().sum()\n",
    "        f = f / (n_tokens * expert_indices.shape[1])\n",
    "    else:  # top-1\n",
    "        f = torch.zeros(n_experts, device=router_probs.device)\n",
    "        for i in range(n_experts):\n",
    "            f[i] = (expert_indices == i).float().mean()\n",
    "    \n",
    "    # p_i: mean router probability for each expert\n",
    "    p = router_probs.mean(dim=0)  # (n_experts,)\n",
    "    \n",
    "    # Loss\n",
    "    loss = n_experts * (f * p).sum()\n",
    "    \n",
    "    return loss, f, p\n",
    "\n",
    "# Compute for our top-2 routing\n",
    "loss, f, p = load_balancing_loss(probs_8, top2_idx, n_experts)\n",
    "print(f'Load Balancing Loss: {loss.item():.4f}')\n",
    "print(f'  (Ideal = 1.0 when perfectly balanced)')\n",
    "print(f'\\nExpert utilization (f_i = fraction of tokens):')\n",
    "for i in range(n_experts):\n",
    "    bar = '█' * int(f[i].item() * 80)\n",
    "    print(f'  Expert {i}: {f[i].item():.3f} {bar}')\n",
    "print(f'\\nMean router probability (p_i):')\n",
    "for i in range(n_experts):\n",
    "    bar = '█' * int(p[i].item() * 80)\n",
    "    print(f'  Expert {i}: {p[i].item():.3f} {bar}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate: what happens with and without load balancing\n",
    "# Simulate routing collapse vs balanced routing\n",
    "n_tokens_sim = 1000\n",
    "n_experts_sim = 8\n",
    "\n",
    "# Scenario 1: Collapsed routing (most tokens go to expert 0)\n",
    "collapsed_probs = torch.zeros(n_tokens_sim, n_experts_sim, device=device)\n",
    "collapsed_probs[:, 0] = 0.8\n",
    "collapsed_probs[:, 1:] = 0.2 / (n_experts_sim - 1)\n",
    "collapsed_idx = torch.zeros(n_tokens_sim, dtype=torch.long, device=device)\n",
    "collapsed_idx[n_tokens_sim//2:] = 1\n",
    "\n",
    "# Scenario 2: Balanced routing\n",
    "balanced_probs = torch.ones(n_tokens_sim, n_experts_sim, device=device) / n_experts_sim\n",
    "balanced_idx = torch.arange(n_tokens_sim, device=device) % n_experts_sim\n",
    "\n",
    "loss_collapsed, f_c, _ = load_balancing_loss(collapsed_probs, collapsed_idx, n_experts_sim)\n",
    "loss_balanced, f_b, _ = load_balancing_loss(balanced_probs, balanced_idx, n_experts_sim)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "colors = plt.cm.Set2(range(n_experts_sim))\n",
    "axes[0].bar(range(n_experts_sim), f_c.cpu().numpy(), color=colors)\n",
    "axes[0].set_title(f'Collapsed Routing\\nLoss = {loss_collapsed.item():.2f}')\n",
    "axes[0].set_xlabel('Expert')\n",
    "axes[0].set_ylabel('Fraction of tokens')\n",
    "axes[0].axhline(y=1/n_experts_sim, color='red', linestyle='--')\n",
    "\n",
    "axes[1].bar(range(n_experts_sim), f_b.cpu().numpy(), color=colors)\n",
    "axes[1].set_title(f'Balanced Routing\\nLoss = {loss_balanced.item():.2f}')\n",
    "axes[1].set_xlabel('Expert')\n",
    "axes[1].set_ylabel('Fraction of tokens')\n",
    "axes[1].axhline(y=1/n_experts_sim, color='red', linestyle='--', label='Ideal')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.suptitle('Load Balancing: Collapsed vs Balanced Expert Usage', fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f'Loss ratio: collapsed/balanced = {loss_collapsed.item()/loss_balanced.item():.1f}x')\n",
    "print('→ Higher loss penalizes uneven routing, pushing toward balance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Scaling Analysis\n",
    "\n",
    "How MoE enables efficient scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare dense vs MoE parameter efficiency\n",
    "d_model_real = 4096\n",
    "d_ff_real = 14336  # Typical for 7B model\n",
    "\n",
    "dense_ffn_params = 2 * d_model_real * d_ff_real  # W1 + W2 (ignoring biases)\n",
    "\n",
    "print('=' * 70)\n",
    "print('SCALING: Dense vs MoE Parameter Efficiency')\n",
    "print(f'd_model={d_model_real}, d_ff={d_ff_real}')\n",
    "print('=' * 70)\n",
    "\n",
    "configs = [\n",
    "    ('Dense', 1, 1),\n",
    "    ('MoE 4x (top-1)', 4, 1),\n",
    "    ('MoE 8x (top-2)', 8, 2),\n",
    "    ('MoE 16x (top-2)', 16, 2),\n",
    "]\n",
    "\n",
    "print(f'{\"Config\":<20} {\"Total Params\":<18} {\"Active Params\":<18} {\"Active %\":<10} {\"Speedup\":<10}')\n",
    "print('-' * 70)\n",
    "\n",
    "for name, n_exp, top_k in configs:\n",
    "    total = n_exp * dense_ffn_params + d_model_real * n_exp  # experts + router\n",
    "    active = top_k * dense_ffn_params\n",
    "    pct = 100 * active / total\n",
    "    speedup = total / active\n",
    "    print(f'{name:<20} {total/1e6:>12.1f}M  {active/1e6:>12.1f}M  {pct:>7.1f}%  {speedup:>7.1f}x')\n",
    "\n",
    "print('-' * 70)\n",
    "print('\\nMixtral 8x7B equivalent:')\n",
    "print(f'  47B total params, 13B active → 3.6x more \"knowledge\" per FLOP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the parameter vs compute tradeoff\n",
    "expert_counts = [1, 2, 4, 8, 16, 32, 64]\n",
    "base_params = 7  # billion params for one expert set\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Total params vs active params\n",
    "total_p = [n * base_params for n in expert_counts]\n",
    "active_top1 = [base_params for _ in expert_counts]\n",
    "active_top2 = [2 * base_params for _ in expert_counts]\n",
    "\n",
    "axes[0].plot(expert_counts, total_p, 'o-', linewidth=2, label='Total parameters', color='C0')\n",
    "axes[0].plot(expert_counts, active_top1, 's--', linewidth=2, label='Active (top-1)', color='C1')\n",
    "axes[0].plot(expert_counts, active_top2, '^--', linewidth=2, label='Active (top-2)', color='C2')\n",
    "axes[0].set_xlabel('Number of Experts')\n",
    "axes[0].set_ylabel('Parameters (B)')\n",
    "axes[0].set_title('MoE: Total vs Active Parameters')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Efficiency ratio\n",
    "efficiency_top1 = [t / a for t, a in zip(total_p, active_top1)]\n",
    "efficiency_top2 = [t / a for t, a in zip(total_p, active_top2)]\n",
    "\n",
    "axes[1].plot(expert_counts, efficiency_top1, 'o-', linewidth=2, label='Top-1 routing', color='C1')\n",
    "axes[1].plot(expert_counts, efficiency_top2, 's-', linewidth=2, label='Top-2 routing', color='C2')\n",
    "axes[1].set_xlabel('Number of Experts')\n",
    "axes[1].set_ylabel('Knowledge/Compute Ratio')\n",
    "axes[1].set_title('MoE Efficiency: More Experts = More Knowledge per FLOP')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "print('=' * 75)\n",
    "print('COMPARISON: MoE Routing Strategies')\n",
    "print('=' * 75)\n",
    "print(f'{\"Property\":<25} {\"Top-1 (Switch)\":<20} {\"Top-2 (Mixtral)\":<20}')\n",
    "print('-' * 75)\n",
    "print(f'{\"Experts active/token\":<25} {\"1\":<20} {\"2\":<20}')\n",
    "print(f'{\"Compute per token\":<25} {\"1x FFN\":<20} {\"2x FFN\":<20}')\n",
    "print(f'{\"Quality\":<25} {\"Good\":<20} {\"Better\":<20}')\n",
    "print(f'{\"Load balancing\":<25} {\"Critical\":<20} {\"Important\":<20}')\n",
    "print(f'{\"Key model\":<25} {\"Switch Transformer\":<20} {\"Mixtral 8x7B\":<20}')\n",
    "print(f'{\"Year\":<25} {\"2021\":<20} {\"2023\":<20}')\n",
    "print('=' * 75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook we implemented from scratch:\n",
    "\n",
    "1. **Dense FFN baseline** — all parameters active for every token\n",
    "\n",
    "2. **Top-1 MoE (Switch Transformer)** — each token routed to 1 expert. Maximum efficiency, slight quality tradeoff.\n",
    "\n",
    "3. **Top-2 MoE (Mixtral style)** — each token routed to 2 experts with weighted outputs. Better quality than top-1.\n",
    "\n",
    "4. **Load balancing loss** — auxiliary loss that penalizes uneven expert utilization, preventing routing collapse.\n",
    "\n",
    "**Key insight:** MoE decouples model capacity (total parameters) from compute cost (active parameters). Mixtral 8x7B has 47B total parameters but only uses 13B per token — matching dense models 3-4x its active size."
   ]
  }
 ]
}