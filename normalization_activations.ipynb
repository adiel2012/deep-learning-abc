{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization & Activation Improvements: Pre-LN, RMSNorm, GeLU, SwiGLU\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/adiel2012/deep-learning-abc/blob/main/normalization_activations.ipynb)\n",
    "\n",
    "This notebook implements from scratch the key normalization and activation improvements that make modern Transformers trainable and performant:\n",
    "\n",
    "**Normalization:**\n",
    "1. **Post-LayerNorm** — original (unstable for deep models)\n",
    "2. **Pre-LayerNorm** — apply norm *before* sublayers (current standard)\n",
    "3. **RMSNorm** — simpler and faster (used in LLaMA, Mistral)\n",
    "\n",
    "**Activations:**\n",
    "4. **ReLU** → **GeLU** → **SwiGLU** progression\n",
    "\n",
    "We compare training stability, gradient flow, and computational cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Mathematical Foundations\n",
    "\n",
    "### Layer Normalization\n",
    "\n",
    "LayerNorm normalizes across the feature dimension for each token independently:\n",
    "\n",
    "$$\\text{LayerNorm}(x) = \\gamma \\cdot \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} + \\beta$$\n",
    "\n",
    "where $\\mu = \\frac{1}{d} \\sum_i x_i$ and $\\sigma^2 = \\frac{1}{d} \\sum_i (x_i - \\mu)^2$.\n",
    "\n",
    "### RMSNorm\n",
    "\n",
    "RMSNorm simplifies by removing the mean centering:\n",
    "\n",
    "$$\\text{RMSNorm}(x) = \\gamma \\cdot \\frac{x}{\\text{RMS}(x) + \\epsilon}, \\quad \\text{RMS}(x) = \\sqrt{\\frac{1}{d} \\sum_i x_i^2}$$\n",
    "\n",
    "No $\\beta$ (bias), no $\\mu$ (mean subtraction) → ~10-30% faster.\n",
    "\n",
    "### Pre-LN vs Post-LN\n",
    "\n",
    "**Post-LN** (original): `x + LayerNorm(Sublayer(x))`\n",
    "\n",
    "**Pre-LN** (modern): `x + Sublayer(LayerNorm(x))`\n",
    "\n",
    "Pre-LN places the residual connection as a direct path, making gradients flow more easily through deep networks.\n",
    "\n",
    "### Activation Functions\n",
    "\n",
    "$$\\text{ReLU}(x) = \\max(0, x)$$\n",
    "\n",
    "$$\\text{GeLU}(x) = x \\cdot \\Phi(x) \\approx 0.5 \\cdot x \\cdot \\left(1 + \\tanh\\left(\\sqrt{\\frac{2}{\\pi}}(x + 0.044715 x^3)\\right)\\right)$$\n",
    "\n",
    "$$\\text{Swish}(x) = x \\cdot \\sigma(x)$$\n",
    "\n",
    "$$\\text{SwiGLU}(x, W, V) = \\text{Swish}(xW) \\odot (xV)$$\n",
    "\n",
    "SwiGLU uses a **gating mechanism**: one linear projection controls *what* information passes, another controls *how much*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. LayerNorm vs RMSNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_norm(x, gamma, beta, eps=1e-5):\n",
    "    \"\"\"Standard Layer Normalization.\"\"\"\n",
    "    mean = x.mean(dim=-1, keepdim=True)\n",
    "    var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "    x_norm = (x - mean) / torch.sqrt(var + eps)\n",
    "    return gamma * x_norm + beta\n",
    "\n",
    "def rms_norm(x, gamma, eps=1e-5):\n",
    "    \"\"\"RMS Normalization — no mean centering, no bias.\"\"\"\n",
    "    rms = torch.sqrt(torch.mean(x ** 2, dim=-1, keepdim=True) + eps)\n",
    "    return gamma * (x / rms)\n",
    "\n",
    "# Compare outputs\n",
    "torch.manual_seed(42)\n",
    "d_model = 8\n",
    "x = torch.randn(2, 4, d_model, device=device)  # (batch, seq, d_model)\n",
    "gamma = torch.ones(d_model, device=device)\n",
    "beta = torch.zeros(d_model, device=device)\n",
    "\n",
    "ln_out = layer_norm(x, gamma, beta)\n",
    "rms_out = rms_norm(x, gamma)\n",
    "\n",
    "print('Input x[0,0]:', x[0, 0].cpu())\n",
    "print('LayerNorm:   ', ln_out[0, 0].cpu())\n",
    "print('RMSNorm:     ', rms_out[0, 0].cpu())\n",
    "\n",
    "print(f'\\nLayerNorm stats: mean={ln_out[0,0].mean():.6f}, std={ln_out[0,0].std():.4f}')\n",
    "print(f'RMSNorm stats:   mean={rms_out[0,0].mean():.6f}, RMS={torch.sqrt((rms_out[0,0]**2).mean()):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify: LayerNorm output has mean=0, std=1\n",
    "#         RMSNorm output has RMS=1 (but mean may not be 0)\n",
    "print('LayerNorm guarantees:')\n",
    "print(f'  Output mean ≈ 0: {ln_out.mean(dim=-1)[0].cpu()}')\n",
    "print(f'  Output std  ≈ 1: {ln_out.std(dim=-1, unbiased=False)[0].cpu()}')\n",
    "\n",
    "print('\\nRMSNorm guarantees:')\n",
    "print(f'  Output RMS  ≈ 1: {torch.sqrt((rms_out**2).mean(dim=-1))[0].cpu()}')\n",
    "print(f'  Output mean:     {rms_out.mean(dim=-1)[0].cpu()}  (NOT necessarily 0)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Speed comparison (conceptual — count operations)\n",
    "print('Operation count comparison (for d features):')\n",
    "print('\\nLayerNorm:')\n",
    "print('  1. Compute mean:     d additions + 1 division')\n",
    "print('  2. Subtract mean:    d subtractions')\n",
    "print('  3. Compute variance: d multiplications + d additions + 1 division')\n",
    "print('  4. Normalize:        d subtractions + d divisions')\n",
    "print('  5. Scale + shift:    d multiplications + d additions')\n",
    "print('  Total: ~6d operations')\n",
    "\n",
    "print('\\nRMSNorm:')\n",
    "print('  1. Compute x²:      d multiplications')\n",
    "print('  2. Mean of x²:      d additions + 1 division')\n",
    "print('  3. Square root:     1 operation')\n",
    "print('  4. Normalize:       d divisions')\n",
    "print('  5. Scale:           d multiplications')\n",
    "print('  Total: ~4d operations')\n",
    "print('\\n→ RMSNorm is ~33% fewer operations (no mean subtraction, no bias addition)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Post-LN vs Pre-LN Transformer Blocks\n",
    "\n",
    "The placement of normalization has a dramatic effect on training stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_attention(x, W_Q, W_K, W_V):\n",
    "    \"\"\"Simplified single-head attention for demonstration.\"\"\"\n",
    "    Q, K, V = x @ W_Q, x @ W_K, x @ W_V\n",
    "    d_k = Q.shape[-1]\n",
    "    scores = torch.bmm(Q, K.transpose(1, 2)) / math.sqrt(d_k)\n",
    "    weights = torch.softmax(scores, dim=-1)\n",
    "    return torch.bmm(weights, V)\n",
    "\n",
    "def ffn(x, W1, b1, W2, b2, activation_fn):\n",
    "    \"\"\"Feed-forward network with configurable activation.\"\"\"\n",
    "    return activation_fn(x @ W1 + b1) @ W2 + b2\n",
    "\n",
    "def post_ln_block(x, attn_params, ffn_params, norm_params, activation_fn):\n",
    "    \"\"\"Post-LayerNorm: x + LayerNorm(Sublayer(x))\"\"\"\n",
    "    W_Q, W_K, W_V = attn_params\n",
    "    W1, b1, W2, b2 = ffn_params\n",
    "    gamma1, beta1, gamma2, beta2 = norm_params\n",
    "    \n",
    "    # Attention sublayer with Post-LN\n",
    "    attn_out = simple_attention(x, W_Q, W_K, W_V)\n",
    "    x = layer_norm(x + attn_out, gamma1, beta1)  # norm AFTER residual\n",
    "    \n",
    "    # FFN sublayer with Post-LN\n",
    "    ffn_out = ffn(x, W1, b1, W2, b2, activation_fn)\n",
    "    x = layer_norm(x + ffn_out, gamma2, beta2)  # norm AFTER residual\n",
    "    \n",
    "    return x\n",
    "\n",
    "def pre_ln_block(x, attn_params, ffn_params, norm_params, activation_fn):\n",
    "    \"\"\"Pre-LayerNorm: x + Sublayer(LayerNorm(x))\"\"\"\n",
    "    W_Q, W_K, W_V = attn_params\n",
    "    W1, b1, W2, b2 = ffn_params\n",
    "    gamma1, beta1, gamma2, beta2 = norm_params\n",
    "    \n",
    "    # Attention sublayer with Pre-LN\n",
    "    x_norm = layer_norm(x, gamma1, beta1)  # norm BEFORE sublayer\n",
    "    attn_out = simple_attention(x_norm, W_Q, W_K, W_V)\n",
    "    x = x + attn_out  # clean residual path\n",
    "    \n",
    "    # FFN sublayer with Pre-LN\n",
    "    x_norm = layer_norm(x, gamma2, beta2)  # norm BEFORE sublayer\n",
    "    ffn_out = ffn(x_norm, W1, b1, W2, b2, activation_fn)\n",
    "    x = x + ffn_out  # clean residual path\n",
    "    \n",
    "    return x\n",
    "\n",
    "print('Post-LN: x → Sublayer → Add → Norm → next')\n",
    "print('Pre-LN:  x → Norm → Sublayer → Add → next')\n",
    "print('\\nPre-LN gives the gradient a clean highway through residual connections.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_block_params(d_model, d_ff, device):\n",
    "    \"\"\"Initialize parameters for one transformer block.\"\"\"\n",
    "    scale = 0.02\n",
    "    attn_params = (\n",
    "        torch.randn(d_model, d_model, device=device) * scale,\n",
    "        torch.randn(d_model, d_model, device=device) * scale,\n",
    "        torch.randn(d_model, d_model, device=device) * scale,\n",
    "    )\n",
    "    ffn_params = (\n",
    "        torch.randn(d_model, d_ff, device=device) * scale,\n",
    "        torch.zeros(d_ff, device=device),\n",
    "        torch.randn(d_ff, d_model, device=device) * scale,\n",
    "        torch.zeros(d_model, device=device),\n",
    "    )\n",
    "    norm_params = (\n",
    "        torch.ones(d_model, device=device),\n",
    "        torch.zeros(d_model, device=device),\n",
    "        torch.ones(d_model, device=device),\n",
    "        torch.zeros(d_model, device=device),\n",
    "    )\n",
    "    return attn_params, ffn_params, norm_params\n",
    "\n",
    "# Compare activation magnitudes through deep stacks\n",
    "torch.manual_seed(42)\n",
    "d_model = 32\n",
    "d_ff = 128\n",
    "n_layers = 20\n",
    "batch, seq_len = 2, 8\n",
    "relu = torch.relu\n",
    "\n",
    "x = torch.randn(batch, seq_len, d_model, device=device)\n",
    "\n",
    "# Track norms through layers\n",
    "post_ln_norms = [x.norm().item()]\n",
    "pre_ln_norms = [x.norm().item()]\n",
    "\n",
    "x_post = x.clone()\n",
    "x_pre = x.clone()\n",
    "\n",
    "for layer in range(n_layers):\n",
    "    attn_p, ffn_p, norm_p = init_block_params(d_model, d_ff, device)\n",
    "    \n",
    "    x_post = post_ln_block(x_post, attn_p, ffn_p, norm_p, relu)\n",
    "    post_ln_norms.append(x_post.norm().item())\n",
    "    \n",
    "    x_pre = pre_ln_block(x_pre, attn_p, ffn_p, norm_p, relu)\n",
    "    pre_ln_norms.append(x_pre.norm().item())\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.plot(post_ln_norms, 'o-', linewidth=2, label='Post-LN', color='red')\n",
    "ax.plot(pre_ln_norms, 's-', linewidth=2, label='Pre-LN', color='blue')\n",
    "ax.set_xlabel('Layer')\n",
    "ax.set_ylabel('Activation Norm')\n",
    "ax.set_title('Activation Magnitude Through Deep Transformer Stack\\n(Pre-LN grows smoothly, Post-LN can be erratic)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Pre-RMSNorm Block (Modern Standard)\n",
    "\n",
    "The modern LLM standard combines Pre-LN placement with RMSNorm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_rmsnorm_block(x, attn_params, ffn_params, gamma1, gamma2, activation_fn):\n",
    "    \"\"\"Modern transformer block: Pre-RMSNorm (used in LLaMA, Mistral).\"\"\"\n",
    "    W_Q, W_K, W_V = attn_params\n",
    "    W1, b1, W2, b2 = ffn_params\n",
    "    \n",
    "    # RMSNorm before attention\n",
    "    x_norm = rms_norm(x, gamma1)\n",
    "    attn_out = simple_attention(x_norm, W_Q, W_K, W_V)\n",
    "    x = x + attn_out\n",
    "    \n",
    "    # RMSNorm before FFN\n",
    "    x_norm = rms_norm(x, gamma2)\n",
    "    ffn_out = ffn(x_norm, W1, b1, W2, b2, activation_fn)\n",
    "    x = x + ffn_out\n",
    "    \n",
    "    return x\n",
    "\n",
    "# Track norms through layers\n",
    "torch.manual_seed(42)\n",
    "x_rms = x.clone()\n",
    "rms_norms_list = [x_rms.norm().item()]\n",
    "\n",
    "for layer in range(n_layers):\n",
    "    attn_p, ffn_p, _ = init_block_params(d_model, d_ff, device)\n",
    "    gamma1 = torch.ones(d_model, device=device)\n",
    "    gamma2 = torch.ones(d_model, device=device)\n",
    "    x_rms = pre_rmsnorm_block(x_rms, attn_p, ffn_p, gamma1, gamma2, relu)\n",
    "    rms_norms_list.append(x_rms.norm().item())\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.plot(post_ln_norms, 'o-', linewidth=2, label='Post-LayerNorm', color='red')\n",
    "ax.plot(pre_ln_norms, 's-', linewidth=2, label='Pre-LayerNorm', color='blue')\n",
    "ax.plot(rms_norms_list, '^-', linewidth=2, label='Pre-RMSNorm', color='green')\n",
    "ax.set_xlabel('Layer')\n",
    "ax.set_ylabel('Activation Norm')\n",
    "ax.set_title('Normalization Strategy Comparison (20 layers)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Activation Functions: ReLU → GeLU → SwiGLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    \"\"\"ReLU: max(0, x) — original Transformer activation.\"\"\"\n",
    "    return torch.clamp(x, min=0)\n",
    "\n",
    "def gelu(x):\n",
    "    \"\"\"GeLU: x * Phi(x) — smooth probabilistic activation.\n",
    "    Used in BERT, GPT-2, GPT-3.\"\"\"\n",
    "    return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * x ** 3)))\n",
    "\n",
    "def swish(x):\n",
    "    \"\"\"Swish: x * sigmoid(x) — smooth self-gated activation.\"\"\"\n",
    "    return x * torch.sigmoid(x)\n",
    "\n",
    "# Visualize all three\n",
    "x_range = torch.linspace(-4, 4, 200, device=device)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Activation functions\n",
    "activations = [\n",
    "    ('ReLU', relu(x_range)),\n",
    "    ('GeLU', gelu(x_range)),\n",
    "    ('Swish', swish(x_range)),\n",
    "]\n",
    "\n",
    "for name, y in activations:\n",
    "    axes[0].plot(x_range.cpu(), y.cpu(), linewidth=2, label=name)\n",
    "axes[0].axhline(y=0, color='gray', linestyle='--', alpha=0.3)\n",
    "axes[0].axvline(x=0, color='gray', linestyle='--', alpha=0.3)\n",
    "axes[0].set_xlabel('x')\n",
    "axes[0].set_ylabel('f(x)')\n",
    "axes[0].set_title('Activation Functions')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Gradients (computed manually for clarity)\n",
    "x_grad = x_range.clone().requires_grad_(True)\n",
    "for name, fn in [('ReLU', relu), ('GeLU', gelu), ('Swish', swish)]:\n",
    "    y = fn(x_grad)\n",
    "    grad = torch.autograd.grad(y.sum(), x_grad, create_graph=True)[0]\n",
    "    axes[1].plot(x_range.cpu().detach(), grad.cpu().detach(), linewidth=2, label=name)\n",
    "    x_grad = x_range.clone().requires_grad_(True)\n",
    "\n",
    "axes[1].axhline(y=0, color='gray', linestyle='--', alpha=0.3)\n",
    "axes[1].axvline(x=0, color='gray', linestyle='--', alpha=0.3)\n",
    "axes[1].set_xlabel('x')\n",
    "axes[1].set_ylabel(\"f'(x)\")\n",
    "axes[1].set_title('Gradients — GeLU/Swish are smooth (no hard cutoff at 0)')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('Key differences:')\n",
    "print('  ReLU:  Hard cutoff at 0 — \"dead neurons\" get zero gradient forever')\n",
    "print('  GeLU:  Smooth transition — small negative values get small (nonzero) gradients')\n",
    "print('  Swish: Similar to GeLU, slightly different shape (basis for SwiGLU)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. SwiGLU — The Modern FFN\n",
    "\n",
    "SwiGLU replaces the standard FFN with a gated linear unit:\n",
    "\n",
    "**Standard FFN**: $\\text{FFN}(x) = \\text{ReLU}(xW_1 + b_1)W_2 + b_2$\n",
    "\n",
    "**SwiGLU FFN**: $\\text{SwiGLU}(x) = (\\text{Swish}(xW_{\\text{gate}}) \\odot xW_{\\text{up}}) W_{\\text{down}}$\n",
    "\n",
    "The key idea: one projection ($W_{\\text{gate}}$) learns *what to let through*, another ($W_{\\text{up}}$) learns *what values to pass*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ffn_relu(x, W1, b1, W2, b2):\n",
    "    \"\"\"Original FFN: ReLU(xW1 + b1)W2 + b2\"\"\"\n",
    "    return relu(x @ W1 + b1) @ W2 + b2\n",
    "\n",
    "def ffn_gelu(x, W1, b1, W2, b2):\n",
    "    \"\"\"GeLU FFN: GeLU(xW1 + b1)W2 + b2\"\"\"\n",
    "    return gelu(x @ W1 + b1) @ W2 + b2\n",
    "\n",
    "def ffn_swiglu(x, W_gate, W_up, W_down):\n",
    "    \"\"\"SwiGLU FFN: Swish(x @ W_gate) * (x @ W_up) @ W_down\n",
    "    \n",
    "    Note: SwiGLU uses 3 weight matrices (no bias) instead of 2 matrices + 2 biases.\n",
    "    To keep parameter count similar, d_ff is typically reduced by 2/3.\n",
    "    \"\"\"\n",
    "    gate = swish(x @ W_gate)  # Controls what passes\n",
    "    up = x @ W_up              # Values to pass\n",
    "    return (gate * up) @ W_down\n",
    "\n",
    "# Compare\n",
    "torch.manual_seed(42)\n",
    "d_model = 16\n",
    "d_ff = 64  # Standard FFN expansion factor\n",
    "d_ff_swiglu = int(d_ff * 2 / 3)  # SwiGLU uses smaller d_ff (3 matrices instead of 2)\n",
    "\n",
    "x = torch.randn(2, 4, d_model, device=device)\n",
    "\n",
    "# Standard FFN params\n",
    "W1 = torch.randn(d_model, d_ff, device=device) * 0.1\n",
    "b1 = torch.zeros(d_ff, device=device)\n",
    "W2 = torch.randn(d_ff, d_model, device=device) * 0.1\n",
    "b2 = torch.zeros(d_model, device=device)\n",
    "\n",
    "# SwiGLU params\n",
    "W_gate = torch.randn(d_model, d_ff_swiglu, device=device) * 0.1\n",
    "W_up = torch.randn(d_model, d_ff_swiglu, device=device) * 0.1\n",
    "W_down = torch.randn(d_ff_swiglu, d_model, device=device) * 0.1\n",
    "\n",
    "out_relu = ffn_relu(x, W1, b1, W2, b2)\n",
    "out_gelu = ffn_gelu(x, W1, b1, W2, b2)\n",
    "out_swiglu = ffn_swiglu(x, W_gate, W_up, W_down)\n",
    "\n",
    "print('FFN output shapes:')\n",
    "print(f'  ReLU FFN:  {out_relu.shape}')\n",
    "print(f'  GeLU FFN:  {out_gelu.shape}')\n",
    "print(f'  SwiGLU FFN: {out_swiglu.shape}')\n",
    "\n",
    "# Parameter count comparison\n",
    "relu_params = d_model * d_ff + d_ff + d_ff * d_model + d_model  # W1, b1, W2, b2\n",
    "swiglu_params = d_model * d_ff_swiglu * 3  # W_gate, W_up, W_down (no biases)\n",
    "\n",
    "print(f'\\nParameter counts:')\n",
    "print(f'  ReLU/GeLU FFN (d_ff={d_ff}):  {relu_params}')\n",
    "print(f'  SwiGLU FFN (d_ff={d_ff_swiglu}): {swiglu_params}')\n",
    "print(f'  Ratio: {swiglu_params/relu_params:.2f}x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the gating mechanism in SwiGLU\n",
    "torch.manual_seed(42)\n",
    "x_demo = torch.randn(1, 8, d_model, device=device)\n",
    "\n",
    "gate_values = swish(x_demo @ W_gate)  # How much to let through\n",
    "up_values = x_demo @ W_up             # What to let through\n",
    "gated_output = gate_values * up_values # The product\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 4))\n",
    "\n",
    "titles = [\n",
    "    f'Gate: Swish(x @ W_gate)\\nControls \"how much\"',\n",
    "    f'Up: x @ W_up\\nControls \"what\"',\n",
    "    f'Output: gate * up\\nGated result',\n",
    "]\n",
    "data = [gate_values, up_values, gated_output]\n",
    "\n",
    "for ax, title, d in zip(axes, titles, data):\n",
    "    im = ax.imshow(d[0].detach().cpu().numpy(), cmap='RdBu', aspect='auto')\n",
    "    ax.set_xlabel(f'Hidden dim (d_ff={d_ff_swiglu})')\n",
    "    ax.set_ylabel('Token position')\n",
    "    ax.set_title(title)\n",
    "    plt.colorbar(im, ax=ax)\n",
    "\n",
    "plt.suptitle('SwiGLU Gating Mechanism', fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Modern Transformer Block\n",
    "\n",
    "Putting it all together: the modern LLM block uses **Pre-RMSNorm + SwiGLU FFN**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modern_transformer_block(x, attn_params, swiglu_params, gamma1, gamma2):\n",
    "    \"\"\"Modern transformer block: Pre-RMSNorm + SwiGLU (LLaMA-style).\"\"\"\n",
    "    W_Q, W_K, W_V = attn_params\n",
    "    W_gate, W_up, W_down = swiglu_params\n",
    "    \n",
    "    # Pre-RMSNorm attention\n",
    "    x_norm = rms_norm(x, gamma1)\n",
    "    attn_out = simple_attention(x_norm, W_Q, W_K, W_V)\n",
    "    x = x + attn_out\n",
    "    \n",
    "    # Pre-RMSNorm SwiGLU FFN\n",
    "    x_norm = rms_norm(x, gamma2)\n",
    "    ffn_out = ffn_swiglu(x_norm, W_gate, W_up, W_down)\n",
    "    x = x + ffn_out\n",
    "    \n",
    "    return x\n",
    "\n",
    "def original_transformer_block(x, attn_params, ffn_params, norm_params):\n",
    "    \"\"\"Original 2017 transformer block: Post-LayerNorm + ReLU FFN.\"\"\"\n",
    "    return post_ln_block(x, attn_params, ffn_params, norm_params, relu)\n",
    "\n",
    "# Compare the two architectures\n",
    "torch.manual_seed(42)\n",
    "d_model = 32\n",
    "d_ff = 128\n",
    "d_ff_sg = int(d_ff * 2 / 3)\n",
    "n_layers = 20\n",
    "\n",
    "x = torch.randn(2, 8, d_model, device=device)\n",
    "x_orig = x.clone()\n",
    "x_modern = x.clone()\n",
    "\n",
    "orig_norms = [x_orig.norm().item()]\n",
    "modern_norms = [x_modern.norm().item()]\n",
    "\n",
    "for layer in range(n_layers):\n",
    "    scale = 0.02\n",
    "    \n",
    "    # Original block params\n",
    "    attn_p = (\n",
    "        torch.randn(d_model, d_model, device=device) * scale,\n",
    "        torch.randn(d_model, d_model, device=device) * scale,\n",
    "        torch.randn(d_model, d_model, device=device) * scale,\n",
    "    )\n",
    "    ffn_p = (\n",
    "        torch.randn(d_model, d_ff, device=device) * scale,\n",
    "        torch.zeros(d_ff, device=device),\n",
    "        torch.randn(d_ff, d_model, device=device) * scale,\n",
    "        torch.zeros(d_model, device=device),\n",
    "    )\n",
    "    norm_p = (\n",
    "        torch.ones(d_model, device=device),\n",
    "        torch.zeros(d_model, device=device),\n",
    "        torch.ones(d_model, device=device),\n",
    "        torch.zeros(d_model, device=device),\n",
    "    )\n",
    "    \n",
    "    # SwiGLU params\n",
    "    swiglu_p = (\n",
    "        torch.randn(d_model, d_ff_sg, device=device) * scale,\n",
    "        torch.randn(d_model, d_ff_sg, device=device) * scale,\n",
    "        torch.randn(d_ff_sg, d_model, device=device) * scale,\n",
    "    )\n",
    "    gamma1 = torch.ones(d_model, device=device)\n",
    "    gamma2 = torch.ones(d_model, device=device)\n",
    "    \n",
    "    x_orig = original_transformer_block(x_orig, attn_p, ffn_p, norm_p)\n",
    "    orig_norms.append(x_orig.norm().item())\n",
    "    \n",
    "    x_modern = modern_transformer_block(x_modern, attn_p, swiglu_p, gamma1, gamma2)\n",
    "    modern_norms.append(x_modern.norm().item())\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.plot(orig_norms, 'o-', linewidth=2, label='Original (Post-LN + ReLU)', color='red')\n",
    "ax.plot(modern_norms, 's-', linewidth=2, label='Modern (Pre-RMSNorm + SwiGLU)', color='green')\n",
    "ax.set_xlabel('Layer')\n",
    "ax.set_ylabel('Activation Norm')\n",
    "ax.set_title('Original vs Modern Transformer Block\\n(20 layers, forward pass)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary comparison\n",
    "print('=' * 80)\n",
    "print('EVOLUTION: Normalization & Activation in Transformers')\n",
    "print('=' * 80)\n",
    "print(f'{\"Component\":<20} {\"Original (2017)\":<25} {\"Modern (2024)\":<25}')\n",
    "print('-' * 80)\n",
    "print(f'{\"Norm placement\":<20} {\"Post-LN\":<25} {\"Pre-LN\":<25}')\n",
    "print(f'{\"Norm type\":<20} {\"LayerNorm\":<25} {\"RMSNorm\":<25}')\n",
    "print(f'{\"Activation\":<20} {\"ReLU\":<25} {\"SwiGLU\":<25}')\n",
    "print(f'{\"FFN structure\":<20} {\"Linear→ReLU→Linear\":<25} {\"Gate+Up→SwiGLU→Down\":<25}')\n",
    "print(f'{\"Norm params\":<20} {\"gamma + beta\":<25} {\"gamma only\":<25}')\n",
    "print(f'{\"Max stable depth\":<20} {\"~6-12 layers\":<25} {\"100+ layers\":<25}')\n",
    "print(f'{\"Used in\":<20} {\"Original Transformer\":<25} {\"LLaMA, Mistral, Gemma\":<25}')\n",
    "print('=' * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook we implemented from scratch:\n",
    "\n",
    "**Normalization:**\n",
    "1. **LayerNorm** — normalizes by mean and variance, with learnable $\\gamma$ and $\\beta$\n",
    "2. **RMSNorm** — normalizes by RMS only, no mean centering, no bias. 10-30% faster.\n",
    "3. **Pre-LN** — places normalization *before* sublayers, giving residual connections a clean gradient path. Enables training 100+ layer models.\n",
    "\n",
    "**Activations:**\n",
    "4. **ReLU → GeLU** — smooth activation eliminates \"dead neuron\" problem\n",
    "5. **SwiGLU** — gated linear unit with Swish activation. Two projections: one gates \"how much\", one provides \"what\". More expressive per parameter.\n",
    "\n",
    "**Modern standard (LLaMA, Mistral, Gemma):** Pre-RMSNorm + SwiGLU FFN"
   ]
  }
 ]
}