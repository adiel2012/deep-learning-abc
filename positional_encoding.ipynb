{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Positional Encoding from Scratch\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/adiel2012/deep-learning-abc/blob/main/positional_encoding.ipynb)\n",
    "\n",
    "Attention treats its input as a **set** — it has no notion of token order. The sentence \"cat sat on mat\" and \"mat sat on cat\" produce identical attention scores if the embeddings are the same.\n",
    "\n",
    "**Positional encoding** injects order information so the model knows *where* each token appears in the sequence.\n",
    "\n",
    "This notebook covers:\n",
    "1. Why position matters\n",
    "2. The math behind sinusoidal encoding\n",
    "3. Implementation from raw ops\n",
    "4. Visualizations that build intuition\n",
    "5. Learned positional embeddings (the alternative)\n",
    "\n",
    "> **Prerequisites:** [attention_from_scratch.ipynb](https://colab.research.google.com/github/adiel2012/deep-learning-abc/blob/main/attention_from_scratch.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (Colab already has torch, but this ensures compatibility)\n",
    "!pip install torch matplotlib -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Mathematical Foundations\n",
    "\n",
    "---\n",
    "\n",
    "### 0.1 The Problem: Attention is Permutation-Invariant\n",
    "\n",
    "Consider the self-attention output for token $i$:\n",
    "\n",
    "$$\\text{output}_i = \\sum_j \\text{softmax}\\!\\left(\\frac{q_i \\cdot k_j}{\\sqrt{d_k}}\\right) v_j$$\n",
    "\n",
    "If we shuffle the input tokens, the dot products $q_i \\cdot k_j$ don't change (the same pairs still exist), so the outputs are just a permutation of the original outputs. The model **cannot distinguish** \"the cat sat\" from \"sat cat the\".\n",
    "\n",
    "We need to **break this symmetry** by encoding position.\n",
    "\n",
    "---\n",
    "\n",
    "### 0.2 Sinusoidal Positional Encoding\n",
    "\n",
    "The original Transformer (Vaswani et al., 2017) uses a fixed encoding based on sine and cosine waves at different frequencies:\n",
    "\n",
    "$$PE_{(pos, 2i)} = \\sin\\!\\left(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\right)$$\n",
    "\n",
    "$$PE_{(pos, 2i+1)} = \\cos\\!\\left(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\right)$$\n",
    "\n",
    "where:\n",
    "- $pos$ = position in the sequence ($0, 1, 2, \\ldots$)\n",
    "- $i$ = dimension index ($0, 1, \\ldots, d_{\\text{model}}/2 - 1$)\n",
    "- $d_{\\text{model}}$ = embedding dimension\n",
    "\n",
    "Each dimension gets a sinusoid with a different **wavelength**, ranging from $2\\pi$ (dimension 0) to $10000 \\cdot 2\\pi$ (last dimension).\n",
    "\n",
    "---\n",
    "\n",
    "### 0.3 Why Sinusoids?\n",
    "\n",
    "**Property 1: Unique encoding.** Each position gets a unique pattern across all dimensions — like a binary counter but with smooth, continuous values.\n",
    "\n",
    "**Property 2: Relative positions via linear transformation.** For any fixed offset $k$:\n",
    "\n",
    "$$PE_{pos+k} = M_k \\cdot PE_{pos}$$\n",
    "\n",
    "where $M_k$ is a rotation matrix that depends only on $k$, not on $pos$. This means the model can learn to attend to relative positions (\"2 tokens ago\") because the relationship between any two positions is a simple linear function.\n",
    "\n",
    "Proof sketch for a single frequency $\\omega$:\n",
    "\n",
    "$$\\begin{bmatrix} \\sin(\\omega(pos+k)) \\\\ \\cos(\\omega(pos+k)) \\end{bmatrix} = \\begin{bmatrix} \\cos(\\omega k) & \\sin(\\omega k) \\\\ -\\sin(\\omega k) & \\cos(\\omega k) \\end{bmatrix} \\begin{bmatrix} \\sin(\\omega \\cdot pos) \\\\ \\cos(\\omega \\cdot pos) \\end{bmatrix}$$\n",
    "\n",
    "This is just the angle-addition identity from trigonometry.\n",
    "\n",
    "**Property 3: Bounded values.** All values are in $[-1, 1]$, same scale as typical normalized embeddings.\n",
    "\n",
    "**Property 4: Extrapolation.** Since it's a formula (not a lookup table), it works for sequence lengths longer than those seen during training.\n",
    "\n",
    "---\n",
    "\n",
    "### 0.4 The Frequency Spectrum\n",
    "\n",
    "The denominator $10000^{2i/d_{\\text{model}}}$ creates a geometric progression of wavelengths:\n",
    "\n",
    "| Dimension $i$ | Wavelength | Intuition |\n",
    "|:---:|:---:|:---|\n",
    "| 0 | $2\\pi \\approx 6.3$ | Changes rapidly — encodes fine position |\n",
    "| mid | ~$630$ | Medium frequency |\n",
    "| last | $20000\\pi \\approx 62{,}832$ | Changes very slowly — encodes coarse position |\n",
    "\n",
    "Low dimensions act like the \"ones digit\" of a number (fast-changing), while high dimensions act like the \"thousands digit\" (slow-changing). Together they uniquely identify each position, similar to how digits in a number uniquely identify a value.\n",
    "\n",
    "---\n",
    "\n",
    "### 0.5 How It's Used\n",
    "\n",
    "The positional encoding is simply **added** to the token embedding:\n",
    "\n",
    "$$\\text{input}_i = \\text{Embedding}(\\text{token}_i) + PE_i$$\n",
    "\n",
    "Addition (rather than concatenation) keeps the dimension unchanged and lets the model learn to use position and content jointly through the same projections.\n",
    "\n",
    "---\n",
    "\n",
    "Now let's implement and visualize all of this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Problem: Attention Ignores Order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def softmax(x):\n    x_max = x.max(dim=-1, keepdim=True).values\n    exp_x = torch.exp(x - x_max)\n    return exp_x / exp_x.sum(dim=-1, keepdim=True)\n\ndef attention_weights(X):\n    \"\"\"Compute self-attention weights (no projection, for demonstration).\"\"\"\n    scores = X @ X.T / math.sqrt(X.shape[-1])\n    return softmax(scores)\n\n# Three token embeddings (4 dims each)\nembeddings = torch.tensor([\n    [1.0, 0.0, 0.5, 0.2],   # \"cat\"\n    [0.0, 1.0, 0.3, 0.8],   # \"sat\"\n    [0.5, 0.5, 1.0, 0.1],   # \"down\"\n], device=device)\n\n# Original order: cat, sat, down\noriginal = embeddings[[0, 1, 2]]\n# Shuffled order: down, cat, sat\nshuffled = embeddings[[2, 0, 1]]\n\nw_orig = attention_weights(original)\nw_shuf = attention_weights(shuffled)\n\nprint(\"Original order [cat, sat, down] — attention weights:\")\nprint(w_orig)\nprint(\"\\nShuffled order [down, cat, sat] — attention weights:\")\nprint(w_shuf)\nprint(\"\\nThe weights are just a permutation of each other!\")\nprint(\"Attention has NO idea which token came first.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Sinusoidal Positional Encoding — Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sinusoidal_positional_encoding(max_len, d_model, device=None):\n",
    "    \"\"\"\n",
    "    Compute sinusoidal positional encoding from scratch.\n",
    "    \n",
    "    PE(pos, 2i)   = sin(pos / 10000^(2i/d_model))\n",
    "    PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\n",
    "    \n",
    "    Returns: (max_len, d_model) tensor\n",
    "    \"\"\"\n",
    "    pe = torch.zeros(max_len, d_model, device=device)\n",
    "    \n",
    "    # Position indices: [0, 1, 2, ..., max_len-1]\n",
    "    pos = torch.arange(0, max_len, device=device).unsqueeze(1)  # (max_len, 1)\n",
    "    \n",
    "    # Dimension indices for pairs: [0, 2, 4, ...]\n",
    "    i = torch.arange(0, d_model, 2, device=device).float()     # (d_model/2,)\n",
    "    \n",
    "    # Compute the denominator: 10000^(2i/d_model)\n",
    "    # Using exp-log trick: 10000^(2i/d) = exp(2i/d * ln(10000))\n",
    "    div_term = torch.exp(i * -(math.log(10000.0) / d_model))   # (d_model/2,)\n",
    "    \n",
    "    # Compute angles: pos / 10000^(2i/d_model)\n",
    "    angles = pos * div_term  # (max_len, d_model/2) via broadcasting\n",
    "    \n",
    "    # Even dimensions: sin, odd dimensions: cos\n",
    "    pe[:, 0::2] = torch.sin(angles)\n",
    "    pe[:, 1::2] = torch.cos(angles)\n",
    "    \n",
    "    return pe\n",
    "\n",
    "\n",
    "# Generate encoding for 50 positions, 16 dimensions\n",
    "max_len = 50\n",
    "d_model = 16\n",
    "PE = sinusoidal_positional_encoding(max_len, d_model, device=device)\n",
    "\n",
    "print(f\"PE shape: {PE.shape}  (max_len, d_model)\")\n",
    "print(f\"\\nPosition 0 encoding: {PE[0]}\")\n",
    "print(f\"Position 1 encoding: {PE[1]}\")\n",
    "print(f\"\\nAll values in [-1, 1]: min={PE.min():.4f}, max={PE.max():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualizing the Encoding\n",
    "\n",
    "### 3.1 Heatmap: All positions × all dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "im = ax.imshow(PE.cpu().numpy(), cmap='RdBu_r', aspect='auto', vmin=-1, vmax=1)\n",
    "ax.set_xlabel('Dimension')\n",
    "ax.set_ylabel('Position')\n",
    "ax.set_title('Sinusoidal Positional Encoding')\n",
    "plt.colorbar(im, ax=ax, label='Value')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading the heatmap:**\n",
    "- Left columns (low dimensions) oscillate rapidly — they encode fine-grained position\n",
    "- Right columns (high dimensions) change slowly — they encode coarse position\n",
    "- Each row (position) has a unique pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Individual Sinusoids at Different Frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positions = torch.arange(max_len)\n",
    "dims_to_show = [0, 2, 4, 8, 14]  # even dims (sin channels)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "for dim in dims_to_show:\n",
    "    ax.plot(positions.numpy(), PE[:, dim].cpu().numpy(), label=f'dim {dim} (sin)')\n",
    "\n",
    "ax.set_xlabel('Position')\n",
    "ax.set_ylabel('PE value')\n",
    "ax.set_title('Sinusoids at Different Frequencies')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Low-index dimensions complete many cycles over the sequence (high frequency), while high-index dimensions barely change (low frequency)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Similarity Between Positions\n",
    "\n",
    "If the encoding works well, nearby positions should be more similar than distant ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosine similarity between all pairs of positions\n",
    "PE_norm = PE / PE.norm(dim=-1, keepdim=True)\n",
    "similarity = (PE_norm @ PE_norm.T).cpu().numpy()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 6))\n",
    "im = ax.imshow(similarity, cmap='viridis', aspect='equal')\n",
    "ax.set_xlabel('Position')\n",
    "ax.set_ylabel('Position')\n",
    "ax.set_title('Cosine Similarity Between Position Encodings')\n",
    "plt.colorbar(im, ax=ax)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Nearby positions are more similar (bright diagonal band).\")\n",
    "print(\"Distant positions are less similar (darker off-diagonal).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 The \"Binary Counter\" Analogy\n",
    "\n",
    "Think of positions in binary:\n",
    "\n",
    "| Position | Binary | Dim 0 (fast) | Dim 1 | Dim 2 | Dim 3 (slow) |\n",
    "|:---:|:---:|:---:|:---:|:---:|:---:|\n",
    "| 0 | 0000 | 0 | 0 | 0 | 0 |\n",
    "| 1 | 0001 | 1 | 0 | 0 | 0 |\n",
    "| 2 | 0010 | 0 | 1 | 0 | 0 |\n",
    "| 3 | 0011 | 1 | 1 | 0 | 0 |\n",
    "| 4 | 0100 | 0 | 0 | 1 | 0 |\n",
    "\n",
    "The lowest bit toggles every step, the next bit every 2 steps, etc. Sinusoidal encoding does the same thing but with **continuous** waves instead of discrete bits — dim 0 oscillates fastest, dim $d_{\\text{model}}-1$ oscillates slowest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Relative Position via Dot Product\n",
    "\n",
    "A key property: the dot product $PE_{pos} \\cdot PE_{pos+k}$ depends mainly on the **offset** $k$, not the absolute position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dot product between PE[pos] and PE[pos+k] for different starting positions\n",
    "offsets = range(0, 20)\n",
    "start_positions = [0, 5, 10, 20]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "\n",
    "for start in start_positions:\n",
    "    dots = []\n",
    "    for k in offsets:\n",
    "        if start + k < max_len:\n",
    "            dot = (PE[start] * PE[start + k]).sum().item()\n",
    "            dots.append(dot)\n",
    "        else:\n",
    "            dots.append(float('nan'))\n",
    "    ax.plot(list(offsets), dots, 'o-', markersize=4, label=f'start={start}')\n",
    "\n",
    "ax.set_xlabel('Offset k')\n",
    "ax.set_ylabel('Dot product PE[pos] · PE[pos+k]')\n",
    "ax.set_title('Dot Product Depends on Offset, Not Absolute Position')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"All curves nearly overlap — the dot product is a function of the offset k,\")\n",
    "print(\"regardless of where in the sequence we start.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Adding Positional Encoding to Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate a small model\n",
    "d_model = 16\n",
    "seq_len = 6\n",
    "vocab_size = 100\n",
    "\n",
    "# Fake token embeddings (random, as if from nn.Embedding)\n",
    "token_embeddings = torch.randn(seq_len, d_model, device=device) * 0.1\n",
    "\n",
    "# Get positional encoding for these positions\n",
    "PE = sinusoidal_positional_encoding(seq_len, d_model, device=device)\n",
    "\n",
    "# Add them together — this is exactly what the Transformer does\n",
    "input_to_attention = token_embeddings + PE\n",
    "\n",
    "print(\"Token embeddings shape:\", token_embeddings.shape)\n",
    "print(\"Positional encoding shape:\", PE.shape)\n",
    "print(\"Combined input shape:\", input_to_attention.shape)\n",
    "\n",
    "print(\"\\nToken embedding (pos 0):\", token_embeddings[0, :4].tolist())\n",
    "print(\"Positional enc  (pos 0):\", PE[0, :4].tolist())\n",
    "print(\"Sum             (pos 0):\", input_to_attention[0, :4].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Does it fix the permutation problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Same 3 tokens as before, but with 4 dimensions (must be even for sin/cos pairs)\nembeddings = torch.tensor([\n    [1.0, 0.0, 0.5, 0.2],   # \"cat\"\n    [0.0, 1.0, 0.3, 0.8],   # \"sat\"\n    [0.5, 0.5, 1.0, 0.1],   # \"down\"\n], device=device)\n\nPE_3 = sinusoidal_positional_encoding(3, 4, device=device)\n\n# Original: cat(pos0), sat(pos1), down(pos2)\noriginal_with_pe = embeddings + PE_3\n# Shuffled: down(pos0), cat(pos1), sat(pos2)\nshuffled_with_pe = embeddings[[2, 0, 1]] + PE_3\n\nw_orig = attention_weights(original_with_pe)\nw_shuf = attention_weights(shuffled_with_pe)\n\nprint(\"WITH positional encoding:\")\nprint(\"\\nOriginal [cat, sat, down]:\")\nprint(w_orig)\nprint(\"\\nShuffled [down, cat, sat]:\")\nprint(w_shuf)\nprint(\"\\nThe weights are now DIFFERENT — order matters!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Scaling with $\\sqrt{d_{\\text{model}}}$\n",
    "\n",
    "In practice, token embeddings are often multiplied by $\\sqrt{d_{\\text{model}}}$ before adding the positional encoding:\n",
    "\n",
    "$$\\text{input}_i = \\sqrt{d_{\\text{model}}} \\cdot \\text{Embedding}(\\text{token}_i) + PE_i$$\n",
    "\n",
    "Why? The embedding vectors have variance $\\approx 1/d_{\\text{model}}$ after initialization, while $PE$ has values in $[-1, 1]$. Without scaling, the positional signal would dominate. Multiplying by $\\sqrt{d_{\\text{model}}}$ brings both to a similar scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 64\n",
    "\n",
    "# Typical embedding variance after initialization\n",
    "emb = torch.randn(10, d_model, device=device)  # default init ~ N(0,1)\n",
    "pe = sinusoidal_positional_encoding(10, d_model, device=device)\n",
    "\n",
    "print(f\"Embedding norm (per token):       {emb.norm(dim=-1).mean():.2f}\")\n",
    "print(f\"PE norm (per position):           {pe.norm(dim=-1).mean():.2f}\")\n",
    "print(f\"Scaled embedding norm (×√d):      {(emb * math.sqrt(d_model)).norm(dim=-1).mean():.2f}\")\n",
    "print(f\"\\nAfter scaling, both are ~{pe.norm(dim=-1).mean():.1f}, so neither dominates.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Learned Positional Embeddings\n",
    "\n",
    "An alternative: instead of a fixed formula, **learn** a position embedding for each position, just like token embeddings.\n",
    "\n",
    "$$PE_{\\text{learned}} = \\text{nn.Embedding}(\\text{max\\_len},\\; d_{\\text{model}})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "max_len = 50\n",
    "d_model = 16\n",
    "\n",
    "# Learned: just an embedding table indexed by position\n",
    "learned_pe = nn.Embedding(max_len, d_model).to(device)\n",
    "\n",
    "positions = torch.arange(max_len, device=device)\n",
    "pe_vectors = learned_pe(positions)  # (max_len, d_model)\n",
    "\n",
    "print(f\"Learned PE shape: {pe_vectors.shape}\")\n",
    "print(f\"Parameters: {max_len} × {d_model} = {max_len * d_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison\n",
    "\n",
    "| | Sinusoidal (fixed) | Learned |\n",
    "|---|---|---|\n",
    "| **Parameters** | 0 | max_len × d_model |\n",
    "| **Extrapolation** | Works for unseen lengths | Cannot — no embedding for unseen positions |\n",
    "| **Performance** | Slightly worse on short sequences | Slightly better when max_len is known |\n",
    "| **Used by** | Original Transformer, some LLMs | BERT, GPT-2, most modern models |\n",
    "\n",
    "In practice, learned embeddings perform as well or slightly better, but sinusoidal encodings are simpler and generalize to longer sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Full Example: Embeddings + PE → Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 16\n",
    "seq_len = 4\n",
    "tokens = [\"The\", \"cat\", \"sat\", \"down\"]\n",
    "\n",
    "# Step 1: Token embeddings (simulated)\n",
    "torch.manual_seed(42)\n",
    "X = torch.randn(seq_len, d_model, device=device) * 0.1\n",
    "\n",
    "# Step 2: Add positional encoding\n",
    "PE = sinusoidal_positional_encoding(seq_len, d_model, device=device)\n",
    "X_with_pos = math.sqrt(d_model) * X + PE\n",
    "\n",
    "# Step 3: Self-attention (simplified — no projection for clarity)\n",
    "scores = X_with_pos @ X_with_pos.T / math.sqrt(d_model)\n",
    "weights_with_pe = softmax(scores)\n",
    "\n",
    "# Compare: without PE\n",
    "scores_no_pe = X @ X.T / math.sqrt(d_model)\n",
    "weights_no_pe = softmax(scores_no_pe)\n",
    "\n",
    "# Visualize side by side\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "for ax, w, title in [(ax1, weights_no_pe, 'Without PE'),\n",
    "                      (ax2, weights_with_pe, 'With PE')]:\n",
    "    w_np = w.detach().cpu().numpy()\n",
    "    im = ax.imshow(w_np, cmap='Blues', vmin=0, vmax=1)\n",
    "    ax.set_xticks(range(len(tokens)))\n",
    "    ax.set_yticks(range(len(tokens)))\n",
    "    ax.set_xticklabels(tokens)\n",
    "    ax.set_yticklabels(tokens)\n",
    "    ax.set_xlabel('Key')\n",
    "    ax.set_ylabel('Query')\n",
    "    ax.set_title(title)\n",
    "    for row in range(len(tokens)):\n",
    "        for col in range(len(tokens)):\n",
    "            ax.text(col, row, f'{w_np[row, col]:.2f}',\n",
    "                    ha='center', va='center', fontsize=9)\n",
    "\n",
    "plt.suptitle('Attention Weights: Effect of Positional Encoding', y=1.02, fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Without PE: weights are nearly uniform (no position info).\")\n",
    "print(\"With PE: weights vary — the model can distinguish positions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary\n",
    "\n",
    "| Concept | Key Idea |\n",
    "|---------|----------|\n",
    "| **Problem** | Self-attention is permutation-invariant — it ignores token order |\n",
    "| **Solution** | Add a position-dependent signal to each embedding |\n",
    "| **Sinusoidal PE** | Fixed formula using sin/cos at geometrically-spaced frequencies |\n",
    "| **Why sin/cos?** | Unique per position, enables relative position via linear transform, bounded, extrapolates |\n",
    "| **Learned PE** | Alternative: learn an embedding per position (more flexible, can't extrapolate) |\n",
    "| **Usage** | $\\text{input} = \\sqrt{d_{\\text{model}}} \\cdot \\text{Embed}(\\text{token}) + PE$ |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}