{
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "colab": {
            "provenance": [],
            "gpuType": "T4"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        },
        "language_info": {
            "name": "python"
        },
        "accelerator": "GPU"
    },
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# RLHF & Direct Preference Optimization (DPO)\n",
                "\n",
                "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/adiel2012/deep-learning-abc/blob/main/rlhf_dpo.ipynb)\n",
                "\n",
                "Aligning language models with human preferences using DPO (a stable alternative to PPO).\n",
                "\n",
                "Key Concepts:\n",
                "1. **Preference Data:** Tuples of $(x, y_w, y_l)$ where $x$ is prompt, $y_w$ is winning response, $y_l$ is losing response.\n",
                "2. **Policy vs Reference:** We maximize the probability of winning responses while staying close to the original (reference) model to prevent mode collapse.\n",
                "3. **DPO Loss:** Analytically solves the RL objective without a Reward Model or PPO.\n",
                "\n",
                "$$ \\mathcal{L}_{DPO} = -\\mathbb{E}_{(x, y_w, y_l) \\sim D} \\left[ \\log \\sigma \\left( \\beta \\log \\frac{\\pi_\\theta(y_w|x)}{\\pi_{ref}(y_w|x)} - \\beta \\log \\frac{\\pi_\\theta(y_l|x)}{\\pi_{ref}(y_l|x)} \\right) \\right] $$"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f'Using device: {device}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. DPO Loss Implementation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def dpo_loss(policy_chosen_logps, policy_rejected_logps, ref_chosen_logps, ref_rejected_logps, beta=0.1):\n",
                "    \"\"\"\n",
                "    Computes DPO loss.\n",
                "    Input logps: (batch_size,)\n",
                "    \"\"\"\n",
                "    \n",
                "    # Calculate log-ratios\n",
                "    # pi_theta(y|x) / pi_ref(y|x) in log space is: log(pi_theta) - log(pi_ref)\n",
                "    chosen_logratios = policy_chosen_logps - ref_chosen_logps\n",
                "    rejected_logratios = policy_rejected_logps - ref_rejected_logps\n",
                "    \n",
                "    # Estimate preference log-odds\n",
                "    logits = chosen_logratios - rejected_logratios\n",
                "    \n",
                "    # Loss = -log(sigmoid(beta * logits))\n",
                "    # using softplus for stability: -log(sigmoid(x)) = softplus(-x)\n",
                "    losses = -F.logsigmoid(beta * logits)\n",
                "    \n",
                "    # Rewards (implicit) for tracking\n",
                "    chosen_rewards = beta * chosen_logratios.detach()\n",
                "    rejected_rewards = beta * rejected_logratios.detach()\n",
                "    \n",
                "    return losses.mean(), chosen_rewards.mean(), rejected_rewards.mean()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Mock Training Loop"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Mock Model (Tiny GPT)\n",
                "class TinyGPT(nn.Module):\n",
                "    def __init__(self, vocab_size, d_model):\n",
                "        super().__init__()\n",
                "        self.token_emb = nn.Embedding(vocab_size, d_model)\n",
                "        self.lm_head = nn.Linear(d_model, vocab_size)\n",
                "        \n",
                "    def forward(self, idx):\n",
                "        x = self.token_emb(idx)\n",
                "        return self.lm_head(x)\n",
                "\n",
                "vocab_size = 100\n",
                "d_model = 32\n",
                "policy_model = TinyGPT(vocab_size, d_model).to(device)\n",
                "ref_model = TinyGPT(vocab_size, d_model).to(device)\n",
                "# Ref model is frozen\n",
                "for p in ref_model.parameters():\n",
                "    p.requires_grad = False\n",
                "\n",
                "optimizer = torch.optim.AdamW(policy_model.parameters(), lr=1e-3)\n",
                "\n",
                "# Mock Batch of Data (Indices)\n",
                "batch_size = 4\n",
                "seq_len = 10\n",
                "chosen_ids = torch.randint(0, vocab_size, (batch_size, seq_len)).to(device)\n",
                "rejected_ids = torch.randint(0, vocab_size, (batch_size, seq_len)).to(device)\n",
                "\n",
                "def get_logps(model, input_ids):\n",
                "    logits = model(input_ids)\n",
                "    # Calculate log_softmax\n",
                "    log_probs = F.log_softmax(logits, dim=-1)\n",
                "    \n",
                "    # Gather log_prob of the actual tokens in the sequence\n",
                "    # input_ids: (B, L)\n",
                "    # log_probs: (B, L, V)\n",
                "    gathered_log_probs = torch.gather(log_probs, 2, input_ids.unsqueeze(-1)).squeeze(-1)\n",
                "    \n",
                "    return gathered_log_probs.sum(dim=-1)\n",
                "\n",
                "print(\"Training DPO Step...\")\n",
                "# 1. Forward pass policy\n",
                "policy_chosen_logps = get_logps(policy_model, chosen_ids)\n",
                "policy_rejected_logps = get_logps(policy_model, rejected_ids)\n",
                "\n",
                "# 2. Forward pass reference (no grad)\n",
                "with torch.no_grad():\n",
                "    ref_chosen_logps = get_logps(ref_model, chosen_ids)\n",
                "    ref_rejected_logps = get_logps(ref_model, rejected_ids)\n",
                "\n",
                "# 3. Loss\n",
                "loss, r_chosen, r_rejected = dpo_loss(\n",
                "    policy_chosen_logps, policy_rejected_logps,\n",
                "    ref_chosen_logps, ref_rejected_logps\n",
                ")\n",
                "\n",
                "optimizer.zero_grad()\n",
                "loss.backward()\n",
                "optimizer.step()\n",
                "\n",
                "print(f\"Loss: {loss.item():.4f}\")\n",
                "print(f\"Reward (Chosen): {r_chosen.item():.4f}\")\n",
                "print(f\"Reward (Rejected): {r_rejected.item():.4f}\")"
            ]
        }
    ]
}