{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# RLHF & Direct Preference Optimization (DPO)\n",
                "\n",
                "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/adiel2012/deep-learning-abc/blob/main/rlhf_dpo.ipynb)\n",
                "\n",
                "## 1. Mathematical Derivation of DPO\n",
                "\n",
                "The standard RLHF objective maximizes the expected reward while penalizing deviation from the reference model:\n",
                "\n",
                "$$ \\max_{\\pi} \\mathbb{E}_{x \\sim \\mathcal{D}, y \\sim \\pi(\\cdot|x)} [r(x, y)] - \\beta \\mathbb{D}_{KL}[\\pi(y|x) || \\pi_{ref}(y|x)] $$\n",
                "\n",
                "The optimal solution to this maximation problem has a closed form (Gibbs distribution):\n",
                "\n",
                "$$ \\pi^*(y|x) = \\frac{1}{Z(x)} \\pi_{ref}(y|x) \\exp\\left( \\frac{1}{\\beta} r(x,y) \\right) $$\n",
                "\n",
                "Rafailov et al. (2023) rearranged this to express the reward function in terms of the optimal policy:\n",
                "\n",
                "$$ r(x,y) = \\beta \\log \\frac{\\pi^*(y|x)}{\\pi_{ref}(y|x)} + \\beta \\log Z(x) $$\n",
                "\n",
                "Substituting this into the Bradley-Terry preference model $P(y_w \\succ y_l | x) = \\sigma(r(x, y_w) - r(x, y_l))$, the partition function $Z(x)$ cancels out, yielding the DPO loss:\n",
                "\n",
                "$$ \\mathcal{L}_{DPO}(\\pi_\\theta; \\pi_{ref}) = -\\mathbb{E}_{(x, y_w, y_l) \\sim \\mathcal{D}} \\left[ \\log \\sigma \\left( \\beta \\log \\frac{\\pi_\\theta(y_w|x)}{\\pi_{ref}(y_w|x)} - \\beta \\log \\frac{\\pi_\\theta(y_l|x)}{\\pi_{ref}(y_l|x)} \\right) \\right] $$"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install torch torchvision matplotlib"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f'Using device: {device}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. DPO Loss Implementation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def dpo_loss(policy_chosen_logps, policy_rejected_logps, ref_chosen_logps, ref_rejected_logps, beta=0.1):\n",
                "    \"\"\"\n",
                "    Computes DPO loss.\n",
                "    Input logps: (batch_size,)\n",
                "    \"\"\"\n",
                "    \n",
                "    # Calculate log-ratios\n",
                "    # pi_theta(y|x) / pi_ref(y|x) in log space is: log(pi_theta) - log(pi_ref)\n",
                "    chosen_logratios = policy_chosen_logps - ref_chosen_logps\n",
                "    rejected_logratios = policy_rejected_logps - ref_rejected_logps\n",
                "    \n",
                "    # Estimate preference log-odds (implicit reward difference)\n",
                "    logits = chosen_logratios - rejected_logratios\n",
                "    \n",
                "    # Loss = -log(sigmoid(beta * logits))\n",
                "    losses = -F.logsigmoid(beta * logits)\n",
                "    \n",
                "    # Rewards (implicit) for tracking\n",
                "    chosen_rewards = beta * chosen_logratios.detach()\n",
                "    rejected_rewards = beta * rejected_logratios.detach()\n",
                "    \n",
                "    return losses.mean(), chosen_rewards.mean(), rejected_rewards.mean()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Mock Training Loop with Margin Visualization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Mock Model (Tiny GPT)\n",
                "class TinyGPT(nn.Module):\n",
                "    def __init__(self, vocab_size, d_model):\n",
                "        super().__init__()\n",
                "        self.token_emb = nn.Embedding(vocab_size, d_model)\n",
                "        self.lm_head = nn.Linear(d_model, vocab_size)\n",
                "        \n",
                "    def forward(self, idx):\n",
                "        x = self.token_emb(idx)\n",
                "        return self.lm_head(x)\n",
                "\n",
                "vocab_size = 100\n",
                "d_model = 32\n",
                "policy_model = TinyGPT(vocab_size, d_model).to(device)\n",
                "ref_model = TinyGPT(vocab_size, d_model).to(device)\n",
                "# Ref model is frozen and identical init (policy starts same as ref)\n",
                "ref_model.load_state_dict(policy_model.state_dict())\n",
                "for p in ref_model.parameters():\n",
                "    p.requires_grad = False\n",
                "\n",
                "optimizer = torch.optim.AdamW(policy_model.parameters(), lr=0.05) # High LR for demo\n",
                "\n",
                "# Mock Data\n",
                "batch_size = 16\n",
                "seq_len = 10\n",
                "chosen_ids = torch.randint(0, vocab_size, (batch_size, seq_len)).to(device)\n",
                "rejected_ids = torch.randint(0, vocab_size, (batch_size, seq_len)).to(device)\n",
                "\n",
                "def get_logps(model, input_ids):\n",
                "    logits = model(input_ids)\n",
                "    log_probs = F.log_softmax(logits, dim=-1)\n",
                "    gathered_log_probs = torch.gather(log_probs, 2, input_ids.unsqueeze(-1)).squeeze(-1)\n",
                "    return gathered_log_probs.sum(dim=-1)\n",
                "\n",
                "margins = []\n",
                "\n",
                "print(\"Training DPO Step...\")\n",
                "for i in range(50):\n",
                "    # 1. Forward pass policy\n",
                "    policy_chosen_logps = get_logps(policy_model, chosen_ids)\n",
                "    policy_rejected_logps = get_logps(policy_model, rejected_ids)\n",
                "\n",
                "    # 2. Forward pass reference (no grad)\n",
                "    with torch.no_grad():\n",
                "        ref_chosen_logps = get_logps(ref_model, chosen_ids)\n",
                "        ref_rejected_logps = get_logps(ref_model, rejected_ids)\n",
                "\n",
                "    # 3. Loss\n",
                "    loss, r_chosen, r_rejected = dpo_loss(\n",
                "        policy_chosen_logps, policy_rejected_logps,\n",
                "        ref_chosen_logps, ref_rejected_logps\n",
                "    )\n",
                "\n",
                "    optimizer.zero_grad()\n",
                "    loss.backward()\n",
                "    optimizer.step()\n",
                "    \n",
                "    margin = r_chosen - r_rejected\n",
                "    margins.append(margin.item())\n",
                "    \n",
                "    if i % 10 == 0:\n",
                "        print(f\"Step {i}: Loss {loss.item():.4f}, Margin {margin.item():.4f}\")\n",
                "\n",
                "plt.figure(figsize=(8, 4))\n",
                "plt.plot(margins)\n",
                "plt.title(\"Reward Margin ($r_{chosen} - r_{rejected}$)\")\n",
                "plt.xlabel(\"Step\")\n",
                "plt.ylabel(\"Margin\")\n",
                "plt.grid(True)\n",
                "plt.show()\n",
                "print(\"Positive margin means model prefers chosen response over rejected.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}