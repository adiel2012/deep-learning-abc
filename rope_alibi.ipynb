{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Positional Encoding Improvements: RoPE & ALiBi\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/adiel2012/deep-learning-abc/blob/main/rope_alibi.ipynb)\n",
    "\n",
    "This notebook implements from scratch the two most important positional encoding improvements since the original Transformer:\n",
    "\n",
    "1. **RoPE** (Rotary Positional Embeddings) — rotates Q and K vectors in 2D subspaces\n",
    "2. **ALiBi** (Attention with Linear Biases) — adds distance-based penalties to attention scores\n",
    "\n",
    "We compare them with the original sinusoidal PE and visualize their key properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Mathematical Foundations\n",
    "\n",
    "### Why Improve Positional Encoding?\n",
    "\n",
    "The original Transformer used fixed sinusoidal positional encodings added to token embeddings:\n",
    "\n",
    "$$\\text{PE}(pos, 2i) = \\sin\\left(\\frac{pos}{10000^{2i/d}}\\right), \\quad \\text{PE}(pos, 2i+1) = \\cos\\left(\\frac{pos}{10000^{2i/d}}\\right)$$\n",
    "\n",
    "**Problems with sinusoidal PE:**\n",
    "- Encodes **absolute** position — token at position 5 always gets the same encoding\n",
    "- Poor **extrapolation** to longer sequences than seen during training\n",
    "- Position info gets diluted through layers\n",
    "\n",
    "### RoPE: Encoding Position via Rotation\n",
    "\n",
    "RoPE (Su et al., 2021) encodes position by **rotating** query and key vectors rather than adding a vector.\n",
    "\n",
    "For a pair of dimensions $(x_0, x_1)$ at position $m$, RoPE applies a 2D rotation:\n",
    "\n",
    "$$\\begin{pmatrix} x_0' \\\\ x_1' \\end{pmatrix} = \\begin{pmatrix} \\cos(m\\theta) & -\\sin(m\\theta) \\\\ \\sin(m\\theta) & \\cos(m\\theta) \\end{pmatrix} \\begin{pmatrix} x_0 \\\\ x_1 \\end{pmatrix}$$\n",
    "\n",
    "where $\\theta_i = 10000^{-2i/d}$ (same frequency scheme as sinusoidal PE).\n",
    "\n",
    "**Key property:** The dot product between two rotated vectors depends only on the **relative** distance:\n",
    "\n",
    "$$\\langle R_m \\mathbf{q}, R_n \\mathbf{k} \\rangle = \\langle R_{m-n} \\mathbf{q}, \\mathbf{k} \\rangle$$\n",
    "\n",
    "This gives us relative position information for free!\n",
    "\n",
    "### ALiBi: No Embeddings, Just Bias\n",
    "\n",
    "ALiBi (Press et al., 2022) takes a radically different approach — it adds no positional information to the embeddings at all. Instead, it adds a **linear bias** to the attention scores:\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}} + \\mathbf{B}\\right) V$$\n",
    "\n",
    "where $B_{ij} = -m \\cdot |i - j|$ and $m$ is a head-specific slope.\n",
    "\n",
    "The slopes follow a geometric sequence: for $h$ heads, $m_k = 2^{-8k/h}$ for $k = 1, \\dots, h$.\n",
    "\n",
    "**Key property:** Nearby tokens get small penalties, distant tokens get large penalties. Each head has a different \"distance sensitivity.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Original Sinusoidal PE (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sinusoidal_pe(max_len, d_model, device=None):\n",
    "    \"\"\"Original sinusoidal positional encoding from 'Attention Is All You Need'.\"\"\"\n",
    "    pe = torch.zeros(max_len, d_model, device=device)\n",
    "    position = torch.arange(0, max_len, dtype=torch.float, device=device).unsqueeze(1)\n",
    "    div_term = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float, device=device) * -(math.log(10000.0) / d_model))\n",
    "    pe[:, 0::2] = torch.sin(position * div_term)\n",
    "    pe[:, 1::2] = torch.cos(position * div_term)\n",
    "    return pe\n",
    "\n",
    "# Quick test\n",
    "pe = sinusoidal_pe(10, 8, device=device)\n",
    "print('Sinusoidal PE shape:', pe.shape)\n",
    "print('PE[0]:', pe[0].cpu())\n",
    "print('PE[1]:', pe[1].cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. RoPE — Rotary Positional Embeddings\n",
    "\n",
    "RoPE works by pairing dimensions $(0,1), (2,3), (4,5), \\ldots$ and applying a 2D rotation to each pair. The rotation angle increases with position and varies by dimension pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rope_frequencies(d_model, max_len, base=10000.0, device=None):\n",
    "    \"\"\"Compute RoPE rotation angles for each position and dimension pair.\n",
    "    \n",
    "    Returns:\n",
    "        freqs: (max_len, d_model//2) — rotation angle for each (position, dimension_pair)\n",
    "    \"\"\"\n",
    "    # theta_i = base^(-2i/d) for i = 0, 1, ..., d/2-1\n",
    "    dim_indices = torch.arange(0, d_model, 2, dtype=torch.float, device=device)\n",
    "    theta = 1.0 / (base ** (dim_indices / d_model))  # (d_model//2,)\n",
    "    \n",
    "    # positions\n",
    "    positions = torch.arange(max_len, dtype=torch.float, device=device)  # (max_len,)\n",
    "    \n",
    "    # outer product: angle = position * theta\n",
    "    freqs = torch.outer(positions, theta)  # (max_len, d_model//2)\n",
    "    return freqs\n",
    "\n",
    "# Visualize the rotation frequencies\n",
    "freqs = rope_frequencies(16, 64, device=device)\n",
    "print('Frequencies shape:', freqs.shape)\n",
    "print('Angles at position 0:', freqs[0].cpu())\n",
    "print('Angles at position 1:', freqs[1].cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_rope(x, freqs):\n",
    "    \"\"\"Apply rotary positional embeddings to input tensor.\n",
    "    \n",
    "    Args:\n",
    "        x: (..., seq_len, d_model) — query or key vectors\n",
    "        freqs: (seq_len, d_model//2) — rotation angles\n",
    "    \n",
    "    Returns:\n",
    "        Rotated tensor of same shape as x\n",
    "    \"\"\"\n",
    "    d_model = x.shape[-1]\n",
    "    \n",
    "    # Split into pairs: (x0, x1), (x2, x3), ...\n",
    "    x_pairs = x.view(*x.shape[:-1], d_model // 2, 2)  # (..., seq_len, d_model//2, 2)\n",
    "    x_even = x_pairs[..., 0]  # (..., seq_len, d_model//2)\n",
    "    x_odd = x_pairs[..., 1]   # (..., seq_len, d_model//2)\n",
    "    \n",
    "    # Get cos and sin of rotation angles\n",
    "    cos_f = torch.cos(freqs)  # (seq_len, d_model//2)\n",
    "    sin_f = torch.sin(freqs)  # (seq_len, d_model//2)\n",
    "    \n",
    "    # Apply 2D rotation: [cos -sin; sin cos] @ [x_even; x_odd]\n",
    "    out_even = x_even * cos_f - x_odd * sin_f\n",
    "    out_odd  = x_even * sin_f + x_odd * cos_f\n",
    "    \n",
    "    # Interleave back\n",
    "    out = torch.stack([out_even, out_odd], dim=-1)  # (..., seq_len, d_model//2, 2)\n",
    "    return out.view(*x.shape)  # (..., seq_len, d_model)\n",
    "\n",
    "# Test: rotation preserves vector magnitude\n",
    "x = torch.randn(1, 8, 16, device=device)  # (batch, seq_len, d_model)\n",
    "freqs = rope_frequencies(16, 8, device=device)\n",
    "x_rotated = apply_rope(x, freqs)\n",
    "\n",
    "print('Original norms:', torch.norm(x, dim=-1).cpu())\n",
    "print('Rotated norms: ', torch.norm(x_rotated, dim=-1).cpu())\n",
    "print('Norms preserved:', torch.allclose(torch.norm(x, dim=-1), torch.norm(x_rotated, dim=-1), atol=1e-5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verifying: RoPE Encodes Relative Position\n",
    "\n",
    "The key mathematical property: the dot product of two RoPE-rotated vectors depends only on their **distance**, not their absolute positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate that q_m @ k_n depends only on (m-n)\n",
    "d_model = 16\n",
    "max_len = 20\n",
    "freqs = rope_frequencies(d_model, max_len, device=device)\n",
    "\n",
    "# Fixed q and k vectors (same for all positions)\n",
    "torch.manual_seed(42)\n",
    "q_vec = torch.randn(d_model, device=device)\n",
    "k_vec = torch.randn(d_model, device=device)\n",
    "\n",
    "# Compute dot product for different absolute positions but same relative distance\n",
    "print('Dot products for SAME relative distance (gap=3) at different absolute positions:')\n",
    "for m in [0, 3, 7, 12]:\n",
    "    n = m + 3  # always gap of 3\n",
    "    q_rot = apply_rope(q_vec.unsqueeze(0), freqs[m:m+1])\n",
    "    k_rot = apply_rope(k_vec.unsqueeze(0), freqs[n:n+1])\n",
    "    dot = (q_rot * k_rot).sum().item()\n",
    "    print(f'  pos ({m},{n}): dot = {dot:.6f}')\n",
    "\n",
    "print('\\nDot products for DIFFERENT relative distances (from position 0):')\n",
    "for gap in [0, 1, 3, 5, 10]:\n",
    "    q_rot = apply_rope(q_vec.unsqueeze(0), freqs[0:1])\n",
    "    k_rot = apply_rope(k_vec.unsqueeze(0), freqs[gap:gap+1])\n",
    "    dot = (q_rot * k_rot).sum().item()\n",
    "    print(f'  gap={gap}: dot = {dot:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize RoPE rotation frequencies\n",
    "d_model = 64\n",
    "max_len = 128\n",
    "freqs = rope_frequencies(d_model, max_len, device=device)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Heatmap of rotation angles\n",
    "im = axes[0].imshow(freqs.cpu().numpy(), aspect='auto', cmap='RdBu')\n",
    "axes[0].set_xlabel('Dimension pair index')\n",
    "axes[0].set_ylabel('Position')\n",
    "axes[0].set_title('RoPE Rotation Angles')\n",
    "plt.colorbar(im, ax=axes[0])\n",
    "\n",
    "# Show cosine of angles (what actually multiplies the vectors)\n",
    "cos_vals = torch.cos(freqs)\n",
    "im2 = axes[1].imshow(cos_vals.cpu().numpy(), aspect='auto', cmap='RdBu')\n",
    "axes[1].set_xlabel('Dimension pair index')\n",
    "axes[1].set_ylabel('Position')\n",
    "axes[1].set_title('cos(angle) — Rotation Factor')\n",
    "plt.colorbar(im2, ax=axes[1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation:** Low-index dimension pairs rotate fast (high frequency), high-index pairs rotate slowly (low frequency). This mirrors the sinusoidal PE design — but applied as rotation, not addition.\n",
    "\n",
    "## 3. RoPE Attention — Putting It All Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_with_rope(Q, K, V, d_model, device=None):\n",
    "    \"\"\"Scaled dot-product attention with RoPE applied to Q and K.\n",
    "    \n",
    "    Args:\n",
    "        Q, K: (batch, seq_len, d_model)\n",
    "        V: (batch, seq_len, d_model)\n",
    "    \"\"\"\n",
    "    seq_len = Q.shape[1]\n",
    "    d_k = Q.shape[-1]\n",
    "    \n",
    "    # Compute rotation frequencies\n",
    "    freqs = rope_frequencies(d_model, seq_len, device=device)\n",
    "    \n",
    "    # Apply RoPE to Q and K (NOT to V — V carries content, not position)\n",
    "    Q_rot = apply_rope(Q, freqs)\n",
    "    K_rot = apply_rope(K, freqs)\n",
    "    \n",
    "    # Standard scaled dot-product attention\n",
    "    scores = torch.bmm(Q_rot, K_rot.transpose(1, 2)) / math.sqrt(d_k)\n",
    "    weights = torch.softmax(scores, dim=-1)\n",
    "    output = torch.bmm(weights, V)\n",
    "    \n",
    "    return output, weights\n",
    "\n",
    "# Test\n",
    "torch.manual_seed(42)\n",
    "batch, seq_len, d_model = 1, 10, 16\n",
    "Q = torch.randn(batch, seq_len, d_model, device=device)\n",
    "K = torch.randn(batch, seq_len, d_model, device=device)\n",
    "V = torch.randn(batch, seq_len, d_model, device=device)\n",
    "\n",
    "out, attn_w = attention_with_rope(Q, K, V, d_model, device=device)\n",
    "print('Output shape:', out.shape)\n",
    "print('Attention weights shape:', attn_w.shape)\n",
    "\n",
    "# Visualize attention pattern\n",
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "im = ax.imshow(attn_w[0].detach().cpu().numpy(), cmap='Blues')\n",
    "ax.set_xlabel('Key position')\n",
    "ax.set_ylabel('Query position')\n",
    "ax.set_title('Attention Weights with RoPE')\n",
    "plt.colorbar(im, ax=ax)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RoPE's Natural Decay Property\n",
    "\n",
    "One key advantage of RoPE: attention scores naturally decay as the distance between tokens increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show attention score decay with distance in RoPE\n",
    "d_model = 64\n",
    "max_len = 100\n",
    "freqs = rope_frequencies(d_model, max_len, device=device)\n",
    "\n",
    "# Use identical q and k to isolate positional effect\n",
    "torch.manual_seed(0)\n",
    "q = torch.randn(d_model, device=device)\n",
    "k = q.clone()\n",
    "\n",
    "# Compute dot product as a function of distance\n",
    "distances = list(range(max_len))\n",
    "dots = []\n",
    "for dist in distances:\n",
    "    q_rot = apply_rope(q.unsqueeze(0), freqs[0:1])  # position 0\n",
    "    k_rot = apply_rope(k.unsqueeze(0), freqs[dist:dist+1])  # position dist\n",
    "    dots.append((q_rot * k_rot).sum().item())\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "ax.plot(distances, dots, linewidth=2)\n",
    "ax.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax.set_xlabel('Distance between tokens')\n",
    "ax.set_ylabel('Dot product (attention score)')\n",
    "ax.set_title('RoPE: Natural Attention Decay with Distance')\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ALiBi — Attention with Linear Biases\n",
    "\n",
    "ALiBi takes a completely different approach: instead of encoding positions into the embeddings, it adds a distance-based penalty directly to the attention scores.\n",
    "\n",
    "$$\\text{score}_{ij} = \\frac{q_i \\cdot k_j}{\\sqrt{d_k}} - m \\cdot |i - j|$$\n",
    "\n",
    "where $m$ is a per-head slope. Slopes follow a geometric sequence: $m_k = 2^{-8k/h}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alibi_slopes(n_heads):\n",
    "    \"\"\"Compute ALiBi slopes for each attention head.\n",
    "    \n",
    "    For n_heads = 8: slopes = [1/2, 1/4, 1/8, 1/16, 1/32, 1/64, 1/128, 1/256]\n",
    "    \"\"\"\n",
    "    # Base ratio: 2^(-8/n_heads)\n",
    "    ratio = 2 ** (-8 / n_heads)\n",
    "    slopes = [ratio ** (i + 1) for i in range(n_heads)]\n",
    "    return torch.tensor(slopes)\n",
    "\n",
    "def alibi_bias(seq_len, n_heads, device=None):\n",
    "    \"\"\"Compute the ALiBi bias matrix for all heads.\n",
    "    \n",
    "    Returns:\n",
    "        bias: (n_heads, seq_len, seq_len) — negative distance penalties\n",
    "    \"\"\"\n",
    "    slopes = alibi_slopes(n_heads).to(device)  # (n_heads,)\n",
    "    \n",
    "    # Distance matrix: |i - j|\n",
    "    positions = torch.arange(seq_len, device=device)\n",
    "    dist = (positions.unsqueeze(0) - positions.unsqueeze(1)).abs().float()  # (seq_len, seq_len)\n",
    "    \n",
    "    # bias = -slope * distance, per head\n",
    "    bias = -slopes.view(n_heads, 1, 1) * dist.unsqueeze(0)  # (n_heads, seq_len, seq_len)\n",
    "    return bias\n",
    "\n",
    "# Test\n",
    "slopes = alibi_slopes(8)\n",
    "print('ALiBi slopes for 8 heads:', slopes)\n",
    "print('Sum:', slopes.sum().item())\n",
    "\n",
    "bias = alibi_bias(6, 4, device=device)\n",
    "print('\\nBias matrix for head 0 (steepest slope):')\n",
    "print(bias[0].cpu())\n",
    "print('\\nBias matrix for head 3 (gentlest slope):')\n",
    "print(bias[3].cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize ALiBi bias matrices across heads\n",
    "n_heads = 8\n",
    "seq_len = 32\n",
    "bias = alibi_bias(seq_len, n_heads, device=device)\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "slopes = alibi_slopes(n_heads)\n",
    "\n",
    "for h in range(n_heads):\n",
    "    ax = axes[h // 4, h % 4]\n",
    "    im = ax.imshow(bias[h].cpu().numpy(), cmap='RdBu', vmin=bias.min().item(), vmax=0)\n",
    "    ax.set_title(f'Head {h} (slope={slopes[h]:.4f})')\n",
    "    ax.set_xlabel('Key pos')\n",
    "    ax.set_ylabel('Query pos')\n",
    "\n",
    "plt.suptitle('ALiBi Bias Matrices — Each Head Has Different Distance Sensitivity', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_with_alibi(Q, K, V, n_heads, device=None):\n",
    "    \"\"\"Multi-head attention with ALiBi positional biases.\n",
    "    \n",
    "    Args:\n",
    "        Q, K, V: (batch, seq_len, d_model)\n",
    "        n_heads: number of attention heads\n",
    "    \"\"\"\n",
    "    batch, seq_len, d_model = Q.shape\n",
    "    d_k = d_model // n_heads\n",
    "    \n",
    "    # Reshape for multi-head: (batch, n_heads, seq_len, d_k)\n",
    "    Q = Q.view(batch, seq_len, n_heads, d_k).transpose(1, 2)\n",
    "    K = K.view(batch, seq_len, n_heads, d_k).transpose(1, 2)\n",
    "    V = V.view(batch, seq_len, n_heads, d_k).transpose(1, 2)\n",
    "    \n",
    "    # Attention scores\n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)  # (batch, n_heads, seq, seq)\n",
    "    \n",
    "    # Add ALiBi bias\n",
    "    bias = alibi_bias(seq_len, n_heads, device=device)  # (n_heads, seq, seq)\n",
    "    scores = scores + bias.unsqueeze(0)  # broadcast over batch\n",
    "    \n",
    "    weights = torch.softmax(scores, dim=-1)\n",
    "    output = torch.matmul(weights, V)  # (batch, n_heads, seq_len, d_k)\n",
    "    \n",
    "    # Reshape back\n",
    "    output = output.transpose(1, 2).contiguous().view(batch, seq_len, d_model)\n",
    "    return output, weights\n",
    "\n",
    "# Test\n",
    "torch.manual_seed(42)\n",
    "batch, seq_len, d_model, n_heads = 1, 12, 16, 4\n",
    "Q = torch.randn(batch, seq_len, d_model, device=device)\n",
    "K = torch.randn(batch, seq_len, d_model, device=device)\n",
    "V = torch.randn(batch, seq_len, d_model, device=device)\n",
    "\n",
    "out, weights = attention_with_alibi(Q, K, V, n_heads, device=device)\n",
    "print('Output shape:', out.shape)\n",
    "print('Weights shape:', weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize ALiBi attention weights per head\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "slopes = alibi_slopes(n_heads)\n",
    "\n",
    "for h in range(n_heads):\n",
    "    ax = axes[h]\n",
    "    im = ax.imshow(weights[0, h].detach().cpu().numpy(), cmap='Blues')\n",
    "    ax.set_title(f'Head {h} (slope={slopes[h]:.4f})')\n",
    "    ax.set_xlabel('Key pos')\n",
    "    ax.set_ylabel('Query pos')\n",
    "\n",
    "plt.suptitle('ALiBi Attention Weights — Steeper Slopes = More Local Attention', fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Length Extrapolation Comparison\n",
    "\n",
    "A key advantage of both RoPE and ALiBi is the ability to handle sequences **longer** than those seen during training. Let's simulate this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_no_pe(Q, K, V):\n",
    "    \"\"\"Standard attention without any positional encoding.\"\"\"\n",
    "    d_k = Q.shape[-1]\n",
    "    scores = torch.bmm(Q, K.transpose(1, 2)) / math.sqrt(d_k)\n",
    "    weights = torch.softmax(scores, dim=-1)\n",
    "    output = torch.bmm(weights, V)\n",
    "    return output, weights\n",
    "\n",
    "def attention_sinusoidal(Q, K, V, d_model, device=None):\n",
    "    \"\"\"Attention with sinusoidal PE added to Q and K.\"\"\"\n",
    "    seq_len = Q.shape[1]\n",
    "    pe = sinusoidal_pe(seq_len, d_model, device=device)\n",
    "    Q_pe = Q + pe.unsqueeze(0)\n",
    "    K_pe = K + pe.unsqueeze(0)\n",
    "    return attention_no_pe(Q_pe, K_pe, V)\n",
    "\n",
    "# Simulate: train on seq_len=16, test on seq_len=64\n",
    "torch.manual_seed(42)\n",
    "d_model = 32\n",
    "train_len = 16\n",
    "test_len = 64\n",
    "\n",
    "# Generate test data at long length\n",
    "Q = torch.randn(1, test_len, d_model, device=device)\n",
    "K = torch.randn(1, test_len, d_model, device=device)\n",
    "V = torch.randn(1, test_len, d_model, device=device)\n",
    "\n",
    "# All three methods at extended length\n",
    "_, w_sine = attention_sinusoidal(Q, K, V, d_model, device=device)\n",
    "_, w_rope = attention_with_rope(Q, K, V, d_model, device=device)\n",
    "\n",
    "# For ALiBi we use single-head for fair comparison\n",
    "bias_1head = alibi_bias(test_len, 1, device=device)  # (1, seq, seq)\n",
    "scores = torch.bmm(Q, K.transpose(1, 2)) / math.sqrt(d_model)\n",
    "scores_alibi = scores + bias_1head\n",
    "w_alibi = torch.softmax(scores_alibi, dim=-1)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "methods = [('Sinusoidal PE', w_sine), ('RoPE', w_rope), ('ALiBi', w_alibi)]\n",
    "for ax, (name, w) in zip(axes, methods):\n",
    "    im = ax.imshow(w[0].detach().cpu().numpy(), cmap='Blues', aspect='auto')\n",
    "    ax.set_xlabel('Key position')\n",
    "    ax.set_ylabel('Query position')\n",
    "    ax.set_title(f'{name} (seq_len={test_len})')\n",
    "    ax.axvline(x=train_len, color='red', linestyle='--', alpha=0.7, label=f'Train boundary ({train_len})')\n",
    "    ax.axhline(y=train_len, color='red', linestyle='--', alpha=0.7)\n",
    "    ax.legend(fontsize=8)\n",
    "    plt.colorbar(im, ax=ax)\n",
    "\n",
    "plt.suptitle('Length Extrapolation: Trained on 16 tokens, Testing on 64', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:**\n",
    "- **Sinusoidal PE**: Attention patterns are similar everywhere — no strong locality bias, which can cause problems with long-range extrapolation.\n",
    "- **RoPE**: Natural decay with distance means attention remains well-behaved beyond training length.\n",
    "- **ALiBi**: Strongest locality bias — the linear penalty naturally limits attention scope, making extrapolation smooth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Comparison Summary\n",
    "\n",
    "Let's put the key properties side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantitative comparison: attention entropy (spread) at different sequence lengths\n",
    "def attention_entropy(weights):\n",
    "    \"\"\"Shannon entropy of attention weights — higher = more spread out.\"\"\"\n",
    "    # Clamp to avoid log(0)\n",
    "    w = weights.clamp(min=1e-10)\n",
    "    return -(w * w.log()).sum(dim=-1).mean().item()\n",
    "\n",
    "seq_lengths = [8, 16, 32, 64, 128]\n",
    "d_model = 32\n",
    "torch.manual_seed(42)\n",
    "\n",
    "entropies = {'Sinusoidal': [], 'RoPE': [], 'ALiBi': []}\n",
    "\n",
    "for sl in seq_lengths:\n",
    "    Q = torch.randn(1, sl, d_model, device=device)\n",
    "    K = torch.randn(1, sl, d_model, device=device)\n",
    "    V = torch.randn(1, sl, d_model, device=device)\n",
    "    \n",
    "    _, w = attention_sinusoidal(Q, K, V, d_model, device=device)\n",
    "    entropies['Sinusoidal'].append(attention_entropy(w))\n",
    "    \n",
    "    _, w = attention_with_rope(Q, K, V, d_model, device=device)\n",
    "    entropies['RoPE'].append(attention_entropy(w))\n",
    "    \n",
    "    bias = alibi_bias(sl, 1, device=device)\n",
    "    scores = torch.bmm(Q, K.transpose(1, 2)) / math.sqrt(d_model)\n",
    "    w = torch.softmax(scores + bias, dim=-1)\n",
    "    entropies['ALiBi'].append(attention_entropy(w))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "for name, ent in entropies.items():\n",
    "    ax.plot(seq_lengths, ent, 'o-', linewidth=2, markersize=8, label=name)\n",
    "\n",
    "ax.set_xlabel('Sequence Length')\n",
    "ax.set_ylabel('Attention Entropy (nats)')\n",
    "ax.set_title('Attention Spread vs Sequence Length\\n(Lower = more focused, Higher = more diffuse)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('Key takeaway:')\n",
    "print('  - Sinusoidal PE: entropy grows fastest → attention becomes diluted at long range')\n",
    "print('  - RoPE: moderate growth → natural decay helps maintain focus')\n",
    "print('  - ALiBi: slowest growth → strong locality bias keeps attention concentrated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary table\n",
    "print('=' * 80)\n",
    "print('COMPARISON: Positional Encoding Methods')\n",
    "print('=' * 80)\n",
    "print(f'{\"Property\":<30} {\"Sinusoidal\":<18} {\"RoPE\":<18} {\"ALiBi\":<18}')\n",
    "print('-' * 80)\n",
    "print(f'{\"Position type\":<30} {\"Absolute\":<18} {\"Relative\":<18} {\"Relative\":<18}')\n",
    "print(f'{\"Where applied\":<30} {\"Added to embed\":<18} {\"Rotates Q,K\":<18} {\"Bias on scores\":<18}')\n",
    "print(f'{\"Extra parameters\":<30} {\"0\":<18} {\"0\":<18} {\"0\":<18}')\n",
    "print(f'{\"Extrapolation\":<30} {\"Poor\":<18} {\"Good\":<18} {\"Excellent\":<18}')\n",
    "print(f'{\"Used in\":<30} {\"Original TF\":<18} {\"LLaMA,Mistral\":<18} {\"BLOOM,MPT\":<18}')\n",
    "print(f'{\"Status (2024)\":<30} {\"Legacy\":<18} {\"Industry std\":<18} {\"Niche\":<18}')\n",
    "print('=' * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook we implemented from scratch:\n",
    "\n",
    "1. **RoPE (Rotary Positional Embeddings)**\n",
    "   - Pairs dimensions and applies 2D rotations at position-dependent angles\n",
    "   - Key formula: $R(m\\theta)$ rotation matrix applied to Q and K\n",
    "   - Dot product depends only on relative distance → relative position encoding\n",
    "   - Natural decay with distance\n",
    "   - Used in LLaMA, Mistral, PaLM, GPT-NeoX — current industry standard\n",
    "\n",
    "2. **ALiBi (Attention with Linear Biases)**\n",
    "   - Adds $-m \\cdot |i-j|$ bias directly to attention scores\n",
    "   - Per-head slopes create multi-scale distance sensitivity\n",
    "   - No modification to embeddings at all\n",
    "   - Excellent length extrapolation (train on 2K → inference on 32K+)\n",
    "   - Used in BLOOM, MPT\n",
    "\n",
    "**Key insight:** Both methods encode *relative* position rather than absolute position, enabling better generalization to longer sequences. RoPE achieves this through geometric rotation; ALiBi through arithmetic penalty."
   ]
  }
 ]
}