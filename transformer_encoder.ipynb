{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Encoder from Scratch\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/adiel2012/deep-learning-abc/blob/main/transformer_encoder.ipynb)\n",
    "\n",
    "This notebook builds the **encoder** side of the Transformer from [\"Attention Is All You Need\"](https://arxiv.org/abs/1706.03762) (Vaswani et al., 2017), using only low-level tensor operations.\n",
    "\n",
    "We assemble every component step by step:\n",
    "\n",
    "```\n",
    "Input Tokens\n",
    "    │\n",
    "    ▼\n",
    "Token Embedding + Positional Encoding\n",
    "    │\n",
    "    ▼\n",
    "┌─────────────────────────────────┐\n",
    "│  Encoder Layer  (×N)           │\n",
    "│  ┌───────────────────────────┐ │\n",
    "│  │ Multi-Head Self-Attention │ │\n",
    "│  └─────────────┬─────────────┘ │\n",
    "│          Add & LayerNorm       │\n",
    "│  ┌─────────────┴─────────────┐ │\n",
    "│  │ Feed-Forward Network      │ │\n",
    "│  └─────────────┬─────────────┘ │\n",
    "│          Add & LayerNorm       │\n",
    "└────────────────┬────────────────┘\n",
    "                 │\n",
    "                 ▼\n",
    "          Encoder Output\n",
    "```\n",
    "\n",
    "> **Prerequisites:** [attention_from_scratch.ipynb](https://colab.research.google.com/github/adiel2012/deep-learning-abc/blob/main/attention_from_scratch.ipynb) and [positional_encoding.ipynb](https://colab.research.google.com/github/adiel2012/deep-learning-abc/blob/main/positional_encoding.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (Colab already has torch, but this ensures compatibility)\n",
    "!pip install torch matplotlib -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Mathematical Foundations\n",
    "\n",
    "The encoder introduces two new building blocks on top of attention: **Layer Normalization** and the **Position-wise Feed-Forward Network**. It also uses **residual connections** to enable training deep stacks.\n",
    "\n",
    "---\n",
    "\n",
    "### 0.1 Residual Connections\n",
    "\n",
    "A residual (skip) connection adds the input of a sub-layer directly to its output:\n",
    "\n",
    "$$\\text{output} = x + \\text{SubLayer}(x)$$\n",
    "\n",
    "**Why?** In deep networks, gradients can vanish as they flow backward through many layers. The skip connection provides a \"gradient highway\" — even if the sub-layer's gradient is tiny, the gradient through the identity path ($\\frac{\\partial x}{\\partial x} = 1$) is always 1.\n",
    "\n",
    "This means the sub-layer only needs to learn the **residual** (the difference between the desired output and the input), which is typically easier than learning the full transformation.\n",
    "\n",
    "---\n",
    "\n",
    "### 0.2 Layer Normalization\n",
    "\n",
    "Layer Norm normalizes each token's feature vector independently:\n",
    "\n",
    "$$\\text{LayerNorm}(x) = \\gamma \\odot \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} + \\beta$$\n",
    "\n",
    "where, for a single token vector $x \\in \\mathbb{R}^{d_{\\text{model}}}$:\n",
    "\n",
    "$$\\mu = \\frac{1}{d_{\\text{model}}} \\sum_{i=1}^{d_{\\text{model}}} x_i \\qquad \\sigma^2 = \\frac{1}{d_{\\text{model}}} \\sum_{i=1}^{d_{\\text{model}}} (x_i - \\mu)^2$$\n",
    "\n",
    "- $\\gamma, \\beta \\in \\mathbb{R}^{d_{\\text{model}}}$ are learnable scale and shift parameters\n",
    "- $\\epsilon$ is a small constant for numerical stability (typically $10^{-5}$)\n",
    "- $\\odot$ denotes element-wise multiplication\n",
    "\n",
    "**Why not Batch Norm?** Batch Norm normalizes across the batch dimension, which doesn't work well for variable-length sequences and small batches. Layer Norm normalizes across the feature dimension of each individual token, making it independent of batch size and sequence length.\n",
    "\n",
    "**In the Transformer:** Layer Norm is applied **after** each sub-layer (post-norm), combined with the residual:\n",
    "\n",
    "$$\\text{output} = \\text{LayerNorm}(x + \\text{SubLayer}(x))$$\n",
    "\n",
    "---\n",
    "\n",
    "### 0.3 Position-wise Feed-Forward Network (FFN)\n",
    "\n",
    "Each encoder layer has a two-layer MLP applied **independently** to each token:\n",
    "\n",
    "$$\\text{FFN}(x) = W_2 \\cdot \\text{ReLU}(W_1 x + b_1) + b_2$$\n",
    "\n",
    "where:\n",
    "- $W_1 \\in \\mathbb{R}^{d_{ff} \\times d_{\\text{model}}}$ — projects up to a wider hidden space\n",
    "- $W_2 \\in \\mathbb{R}^{d_{\\text{model}} \\times d_{ff}}$ — projects back down\n",
    "- Typically $d_{ff} = 4 \\times d_{\\text{model}}$ (expansion factor of 4)\n",
    "\n",
    "**Why \"position-wise\"?** The same weights are applied to every position independently — there's no interaction between tokens. Attention handles inter-token mixing; the FFN handles per-token feature transformation.\n",
    "\n",
    "**Why expand then compress?** The wider hidden layer gives the network more capacity to learn complex nonlinear transformations before compressing back to $d_{\\text{model}}$.\n",
    "\n",
    "---\n",
    "\n",
    "### 0.4 The Full Encoder Layer\n",
    "\n",
    "Putting it all together, one encoder layer computes:\n",
    "\n",
    "$$z = \\text{LayerNorm}\\big(x + \\text{MultiHeadAttention}(x)\\big)$$\n",
    "$$\\text{output} = \\text{LayerNorm}\\big(z + \\text{FFN}(z)\\big)$$\n",
    "\n",
    "The encoder stacks $N$ of these layers (the paper uses $N=6$). Each layer refines the representations: attention captures token interactions, the FFN transforms features, residuals preserve information, and layer norm stabilizes training.\n",
    "\n",
    "---\n",
    "\n",
    "### 0.5 Encoder Hyperparameters (from the paper)\n",
    "\n",
    "| Parameter | Symbol | Base model value |\n",
    "|-----------|--------|------------------|\n",
    "| Model dimension | $d_{\\text{model}}$ | 512 |\n",
    "| Number of heads | $h$ | 8 |\n",
    "| Key/value dimension per head | $d_k = d_v = d_{\\text{model}}/h$ | 64 |\n",
    "| FFN inner dimension | $d_{ff}$ | 2048 |\n",
    "| Number of layers | $N$ | 6 |\n",
    "| Dropout rate | $p$ | 0.1 |\n",
    "\n",
    "We'll use smaller values in this notebook for clarity.\n",
    "\n",
    "---\n",
    "\n",
    "Now let's build each piece."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Helper Functions from Previous Notebooks\n",
    "\n",
    "We reuse softmax, scaled dot-product attention, multi-head attention, and positional encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Row-wise softmax from scratch.\"\"\"\n",
    "    x_max = x.max(dim=-1, keepdim=True).values\n",
    "    exp_x = torch.exp(x - x_max)\n",
    "    return exp_x / exp_x.sum(dim=-1, keepdim=True)\n",
    "\n",
    "\n",
    "def sinusoidal_positional_encoding(max_len, d_model, device=None):\n",
    "    \"\"\"Sinusoidal positional encoding (see positional_encoding.ipynb).\"\"\"\n",
    "    pe = torch.zeros(max_len, d_model, device=device)\n",
    "    pos = torch.arange(0, max_len, device=device).unsqueeze(1)\n",
    "    i = torch.arange(0, d_model, 2, device=device).float()\n",
    "    div_term = torch.exp(i * -(math.log(10000.0) / d_model))\n",
    "    angles = pos * div_term\n",
    "    pe[:, 0::2] = torch.sin(angles)\n",
    "    pe[:, 1::2] = torch.cos(angles)\n",
    "    return pe\n",
    "\n",
    "\n",
    "print(\"Helper functions loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "We use small dimensions to keep outputs readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model hyperparameters (small for demonstration)\n",
    "d_model = 16     # embedding / model dimension\n",
    "n_heads = 2      # number of attention heads\n",
    "d_ff = 64        # feed-forward inner dimension (4 × d_model)\n",
    "n_layers = 2     # number of encoder layers\n",
    "vocab_size = 100  # vocabulary size\n",
    "seq_len = 5      # sequence length\n",
    "\n",
    "d_k = d_model // n_heads  # dimension per head\n",
    "\n",
    "print(f\"d_model={d_model}, n_heads={n_heads}, d_k={d_k}, d_ff={d_ff}, n_layers={n_layers}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Input: Token Embedding + Positional Encoding\n",
    "\n",
    "$$\\text{input} = \\sqrt{d_{\\text{model}}} \\cdot \\text{Embed}(\\text{tokens}) + PE$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding matrix: (vocab_size, d_model) — raw parameter\n",
    "W_embed = torch.randn(vocab_size, d_model, device=device) * 0.1\n",
    "\n",
    "# Simulate tokens: [\"The\", \"cat\", \"sat\", \"on\", \"mat\"]\n",
    "token_ids = torch.tensor([5, 12, 31, 7, 42], device=device)\n",
    "\n",
    "# Look up embeddings (equivalent to nn.Embedding)\n",
    "token_emb = W_embed[token_ids]  # (seq_len, d_model)\n",
    "\n",
    "# Add positional encoding\n",
    "PE = sinusoidal_positional_encoding(seq_len, d_model, device=device)\n",
    "X = math.sqrt(d_model) * token_emb + PE\n",
    "\n",
    "print(\"Token embeddings shape:\", token_emb.shape)\n",
    "print(\"Positional encoding shape:\", PE.shape)\n",
    "print(\"Encoder input shape:\", X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Layer Normalization (from scratch)\n",
    "\n",
    "$$\\text{LayerNorm}(x) = \\gamma \\odot \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} + \\beta$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_norm(x, gamma, beta, eps=1e-5):\n",
    "    \"\"\"\n",
    "    Layer normalization from scratch.\n",
    "    \n",
    "    x:     (seq_len, d_model)\n",
    "    gamma: (d_model,) — learnable scale\n",
    "    beta:  (d_model,) — learnable shift\n",
    "    \"\"\"\n",
    "    # Compute mean and variance across the feature dimension (last dim)\n",
    "    mean = x.mean(dim=-1, keepdim=True)          # (seq_len, 1)\n",
    "    var = x.var(dim=-1, keepdim=True, unbiased=False)  # (seq_len, 1)\n",
    "    \n",
    "    # Normalize\n",
    "    x_norm = (x - mean) / torch.sqrt(var + eps)  # (seq_len, d_model)\n",
    "    \n",
    "    # Scale and shift\n",
    "    return gamma * x_norm + beta\n",
    "\n",
    "\n",
    "# Initialize learnable parameters\n",
    "gamma = torch.ones(d_model, device=device)   # scale (init to 1)\n",
    "beta = torch.zeros(d_model, device=device)   # shift (init to 0)\n",
    "\n",
    "# Test it\n",
    "X_normed = layer_norm(X, gamma, beta)\n",
    "\n",
    "print(\"Before LayerNorm:\")\n",
    "print(f\"  mean per token: {X.mean(dim=-1)}\")\n",
    "print(f\"  std per token:  {X.std(dim=-1)}\")\n",
    "print(\"\\nAfter LayerNorm:\")\n",
    "print(f\"  mean per token: {X_normed.mean(dim=-1)}\")\n",
    "print(f\"  std per token:  {X_normed.std(dim=-1)}\")\n",
    "print(\"\\nEach token now has mean≈0 and std≈1.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Multi-Head Self-Attention (recap)\n",
    "\n",
    "Same as in `attention_from_scratch.ipynb` — the efficient batched version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_head_attention(X, W_Q, W_K, W_V, W_O, n_heads):\n",
    "    \"\"\"\n",
    "    Multi-head self-attention from raw ops.\n",
    "    \n",
    "    X:    (seq_len, d_model)\n",
    "    W_Q, W_K, W_V, W_O: (d_model, d_model)\n",
    "    \n",
    "    Returns: output (seq_len, d_model), weights (n_heads, seq_len, seq_len)\n",
    "    \"\"\"\n",
    "    seq_len, d_model = X.shape\n",
    "    d_k = d_model // n_heads\n",
    "    \n",
    "    # Project\n",
    "    Q = X @ W_Q\n",
    "    K = X @ W_K\n",
    "    V = X @ W_V\n",
    "    \n",
    "    # Split into heads: (seq_len, d_model) -> (n_heads, seq_len, d_k)\n",
    "    Q = Q.view(seq_len, n_heads, d_k).transpose(0, 1)\n",
    "    K = K.view(seq_len, n_heads, d_k).transpose(0, 1)\n",
    "    V = V.view(seq_len, n_heads, d_k).transpose(0, 1)\n",
    "    \n",
    "    # Scaled dot-product attention\n",
    "    scores = Q @ K.transpose(-2, -1) / math.sqrt(d_k)\n",
    "    weights = softmax(scores)\n",
    "    attn_out = weights @ V\n",
    "    \n",
    "    # Merge heads\n",
    "    attn_out = attn_out.transpose(0, 1).contiguous().view(seq_len, d_model)\n",
    "    \n",
    "    # Output projection\n",
    "    output = attn_out @ W_O\n",
    "    \n",
    "    return output, weights\n",
    "\n",
    "print(\"multi_head_attention() defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Position-wise Feed-Forward Network\n",
    "\n",
    "$$\\text{FFN}(x) = W_2 \\cdot \\text{ReLU}(W_1 x + b_1) + b_2$$\n",
    "\n",
    "Applied independently to each token position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    \"\"\"ReLU activation from scratch.\"\"\"\n",
    "    return torch.clamp(x, min=0)\n",
    "\n",
    "\n",
    "def feed_forward(x, W1, b1, W2, b2):\n",
    "    \"\"\"\n",
    "    Position-wise feed-forward network.\n",
    "    \n",
    "    x:  (seq_len, d_model)\n",
    "    W1: (d_model, d_ff)\n",
    "    b1: (d_ff,)\n",
    "    W2: (d_ff, d_model)\n",
    "    b2: (d_model,)\n",
    "    \n",
    "    Returns: (seq_len, d_model)\n",
    "    \"\"\"\n",
    "    # Expand: d_model -> d_ff\n",
    "    hidden = relu(x @ W1 + b1)   # (seq_len, d_ff)\n",
    "    \n",
    "    # Compress: d_ff -> d_model\n",
    "    output = hidden @ W2 + b2    # (seq_len, d_model)\n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "# Test it\n",
    "W1_test = torch.randn(d_model, d_ff, device=device) * 0.1\n",
    "b1_test = torch.zeros(d_ff, device=device)\n",
    "W2_test = torch.randn(d_ff, d_model, device=device) * 0.1\n",
    "b2_test = torch.zeros(d_model, device=device)\n",
    "\n",
    "ffn_out = feed_forward(X, W1_test, b1_test, W2_test, b2_test)\n",
    "print(f\"FFN input shape:  {X.shape}  (seq_len, d_model)\")\n",
    "print(f\"FFN output shape: {ffn_out.shape}  (seq_len, d_model)\")\n",
    "print(f\"\\nInternally: {d_model} -> {d_ff} -> {d_model}\")\n",
    "print(f\"Each token is transformed independently (same weights, no cross-token interaction).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Single Encoder Layer\n",
    "\n",
    "One encoder layer = Multi-Head Attention + Add & Norm + FFN + Add & Norm.\n",
    "\n",
    "$$z = \\text{LayerNorm}(x + \\text{MHA}(x))$$\n",
    "$$\\text{output} = \\text{LayerNorm}(z + \\text{FFN}(z))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_encoder_layer(d_model, n_heads, d_ff, device):\n",
    "    \"\"\"Initialize all parameters for one encoder layer.\"\"\"\n",
    "    scale = 0.1  # small init for stability\n",
    "    params = {\n",
    "        # Multi-head attention weights\n",
    "        'W_Q': torch.randn(d_model, d_model, device=device) * scale,\n",
    "        'W_K': torch.randn(d_model, d_model, device=device) * scale,\n",
    "        'W_V': torch.randn(d_model, d_model, device=device) * scale,\n",
    "        'W_O': torch.randn(d_model, d_model, device=device) * scale,\n",
    "        # LayerNorm 1 (after attention)\n",
    "        'gamma1': torch.ones(d_model, device=device),\n",
    "        'beta1': torch.zeros(d_model, device=device),\n",
    "        # Feed-forward weights\n",
    "        'W1': torch.randn(d_model, d_ff, device=device) * scale,\n",
    "        'b1': torch.zeros(d_ff, device=device),\n",
    "        'W2': torch.randn(d_ff, d_model, device=device) * scale,\n",
    "        'b2': torch.zeros(d_model, device=device),\n",
    "        # LayerNorm 2 (after FFN)\n",
    "        'gamma2': torch.ones(d_model, device=device),\n",
    "        'beta2': torch.zeros(d_model, device=device),\n",
    "    }\n",
    "    return params\n",
    "\n",
    "\n",
    "def encoder_layer(x, params, n_heads):\n",
    "    \"\"\"\n",
    "    One Transformer encoder layer.\n",
    "    \n",
    "    x: (seq_len, d_model)\n",
    "    Returns: (seq_len, d_model), attention_weights (n_heads, seq_len, seq_len)\n",
    "    \"\"\"\n",
    "    # --- Sub-layer 1: Multi-Head Self-Attention ---\n",
    "    attn_out, attn_weights = multi_head_attention(\n",
    "        x, params['W_Q'], params['W_K'], params['W_V'], params['W_O'], n_heads\n",
    "    )\n",
    "    # Residual connection + LayerNorm\n",
    "    z = layer_norm(x + attn_out, params['gamma1'], params['beta1'])\n",
    "    \n",
    "    # --- Sub-layer 2: Feed-Forward Network ---\n",
    "    ffn_out = feed_forward(z, params['W1'], params['b1'], params['W2'], params['b2'])\n",
    "    # Residual connection + LayerNorm\n",
    "    output = layer_norm(z + ffn_out, params['gamma2'], params['beta2'])\n",
    "    \n",
    "    return output, attn_weights\n",
    "\n",
    "\n",
    "# Test one layer\n",
    "layer_params = init_encoder_layer(d_model, n_heads, d_ff, device)\n",
    "out, weights = encoder_layer(X, layer_params, n_heads)\n",
    "\n",
    "print(f\"Input shape:  {X.shape}\")\n",
    "print(f\"Output shape: {out.shape}  — same as input (residual connections preserve shape)\")\n",
    "print(f\"Attention weights shape: {weights.shape}  — (n_heads, seq_len, seq_len)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data flow through the encoder layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trace the data flow step by step\n",
    "print(\"=== Encoder Layer Data Flow ===\")\n",
    "print(f\"\\n1. Input x:                          {X.shape}\")\n",
    "\n",
    "attn_out, attn_w = multi_head_attention(\n",
    "    X, layer_params['W_Q'], layer_params['W_K'],\n",
    "    layer_params['W_V'], layer_params['W_O'], n_heads\n",
    ")\n",
    "print(f\"2. Multi-Head Attention output:       {attn_out.shape}\")\n",
    "\n",
    "residual1 = X + attn_out\n",
    "print(f\"3. After residual (x + MHA(x)):       {residual1.shape}\")\n",
    "\n",
    "z = layer_norm(residual1, layer_params['gamma1'], layer_params['beta1'])\n",
    "print(f\"4. After LayerNorm 1:                 {z.shape}\")\n",
    "\n",
    "ffn_out = feed_forward(z, layer_params['W1'], layer_params['b1'],\n",
    "                       layer_params['W2'], layer_params['b2'])\n",
    "print(f\"5. FFN output:                        {ffn_out.shape}\")\n",
    "print(f\"   (internally: {d_model} -> {d_ff} -> {d_model})\")\n",
    "\n",
    "residual2 = z + ffn_out\n",
    "print(f\"6. After residual (z + FFN(z)):        {residual2.shape}\")\n",
    "\n",
    "final = layer_norm(residual2, layer_params['gamma2'], layer_params['beta2'])\n",
    "print(f\"7. After LayerNorm 2 (final output):  {final.shape}\")\n",
    "print(f\"\\nShape is preserved at every step: {X.shape} in, {final.shape} out.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Full Encoder (N stacked layers)\n",
    "\n",
    "The complete encoder is:\n",
    "1. Token embedding + positional encoding\n",
    "2. N identical encoder layers stacked sequentially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_encoder(vocab_size, d_model, n_heads, d_ff, n_layers, device):\n",
    "    \"\"\"Initialize all parameters for the full encoder.\"\"\"\n",
    "    params = {\n",
    "        'W_embed': torch.randn(vocab_size, d_model, device=device) * 0.1,\n",
    "        'layers': [init_encoder_layer(d_model, n_heads, d_ff, device)\n",
    "                   for _ in range(n_layers)]\n",
    "    }\n",
    "    return params\n",
    "\n",
    "\n",
    "def encoder_forward(token_ids, params, d_model, n_heads):\n",
    "    \"\"\"\n",
    "    Full encoder forward pass.\n",
    "    \n",
    "    token_ids: (seq_len,) integer tensor\n",
    "    Returns: (seq_len, d_model) encoder output, list of attention weights per layer\n",
    "    \"\"\"\n",
    "    seq_len = token_ids.shape[0]\n",
    "    \n",
    "    # 1. Token embedding + positional encoding\n",
    "    token_emb = params['W_embed'][token_ids]\n",
    "    PE = sinusoidal_positional_encoding(seq_len, d_model, device=token_ids.device)\n",
    "    x = math.sqrt(d_model) * token_emb + PE\n",
    "    \n",
    "    # 2. Pass through N encoder layers\n",
    "    all_attn_weights = []\n",
    "    for i, layer_params in enumerate(params['layers']):\n",
    "        x, attn_weights = encoder_layer(x, layer_params, n_heads)\n",
    "        all_attn_weights.append(attn_weights)\n",
    "    \n",
    "    return x, all_attn_weights\n",
    "\n",
    "\n",
    "# Build and run the encoder\n",
    "enc_params = init_encoder(vocab_size, d_model, n_heads, d_ff, n_layers, device)\n",
    "\n",
    "token_ids = torch.tensor([5, 12, 31, 7, 42], device=device)\n",
    "enc_output, all_weights = encoder_forward(token_ids, enc_params, d_model, n_heads)\n",
    "\n",
    "print(f\"Token IDs: {token_ids.tolist()}\")\n",
    "print(f\"Encoder output shape: {enc_output.shape}\")\n",
    "print(f\"Number of layers: {len(all_weights)}\")\n",
    "print(f\"Attention weights per layer: {all_weights[0].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Parameter Count\n",
    "\n",
    "Let's count exactly how many parameters our encoder has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_params(enc_params):\n",
    "    total = 0\n",
    "    \n",
    "    # Embedding\n",
    "    emb_count = enc_params['W_embed'].numel()\n",
    "    print(f\"Embedding:       {emb_count:>8,}  ({list(enc_params['W_embed'].shape)})\")\n",
    "    total += emb_count\n",
    "    \n",
    "    # Each layer\n",
    "    for i, lp in enumerate(enc_params['layers']):\n",
    "        layer_total = 0\n",
    "        details = []\n",
    "        for name, param in lp.items():\n",
    "            n = param.numel()\n",
    "            layer_total += n\n",
    "            details.append(f\"{name}: {list(param.shape)}\")\n",
    "        print(f\"Layer {i}:         {layer_total:>8,}  ({', '.join(details[:4])}...)\")\n",
    "        total += layer_total\n",
    "    \n",
    "    print(f\"{'':─<50}\")\n",
    "    print(f\"Total:           {total:>8,}\")\n",
    "    return total\n",
    "\n",
    "total = count_params(enc_params)\n",
    "print(f\"\\nFor comparison, the original Transformer encoder: ~44M parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Visualizing Attention Across Layers\n",
    "\n",
    "Each layer's attention pattern can capture different relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [\"The\", \"cat\", \"sat\", \"on\", \"mat\"]\n",
    "\n",
    "fig, axes = plt.subplots(n_layers, n_heads, figsize=(5 * n_heads, 4 * n_layers))\n",
    "if n_layers == 1:\n",
    "    axes = [axes]\n",
    "if n_heads == 1:\n",
    "    axes = [[ax] for ax in axes]\n",
    "\n",
    "for layer_idx in range(n_layers):\n",
    "    for head_idx in range(n_heads):\n",
    "        ax = axes[layer_idx][head_idx]\n",
    "        w = all_weights[layer_idx][head_idx].detach().cpu().numpy()\n",
    "        \n",
    "        im = ax.imshow(w, cmap='Blues', vmin=0, vmax=1)\n",
    "        ax.set_xticks(range(len(tokens)))\n",
    "        ax.set_yticks(range(len(tokens)))\n",
    "        ax.set_xticklabels(tokens, fontsize=9)\n",
    "        ax.set_yticklabels(tokens, fontsize=9)\n",
    "        ax.set_title(f'Layer {layer_idx}, Head {head_idx}', fontsize=11)\n",
    "        \n",
    "        if head_idx == 0:\n",
    "            ax.set_ylabel('Query')\n",
    "        if layer_idx == n_layers - 1:\n",
    "            ax.set_xlabel('Key')\n",
    "        \n",
    "        for row in range(len(tokens)):\n",
    "            for col in range(len(tokens)):\n",
    "                ax.text(col, row, f'{w[row, col]:.2f}',\n",
    "                        ha='center', va='center', fontsize=8)\n",
    "\n",
    "plt.suptitle('Encoder Attention Weights by Layer and Head', y=1.02, fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Different layers and heads learn to attend to different patterns.\")\n",
    "print(\"With random weights, patterns are uniform — they specialize during training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. How Representations Evolve Through Layers\n",
    "\n",
    "Let's track how token representations change as they pass through each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect intermediate representations\n",
    "representations = []\n",
    "\n",
    "# Input embedding + PE\n",
    "token_emb = enc_params['W_embed'][token_ids]\n",
    "PE = sinusoidal_positional_encoding(seq_len, d_model, device=device)\n",
    "x = math.sqrt(d_model) * token_emb + PE\n",
    "representations.append(('Input (Emb+PE)', x.clone()))\n",
    "\n",
    "# Each encoder layer\n",
    "for i, lp in enumerate(enc_params['layers']):\n",
    "    x, _ = encoder_layer(x, lp, n_heads)\n",
    "    representations.append((f'After Layer {i}', x.clone()))\n",
    "\n",
    "# Compute cosine similarity between tokens at each stage\n",
    "fig, axes = plt.subplots(1, len(representations), figsize=(5 * len(representations), 4))\n",
    "\n",
    "for idx, (name, rep) in enumerate(representations):\n",
    "    rep_norm = rep / rep.norm(dim=-1, keepdim=True)\n",
    "    sim = (rep_norm @ rep_norm.T).detach().cpu().numpy()\n",
    "    \n",
    "    ax = axes[idx]\n",
    "    im = ax.imshow(sim, cmap='RdYlGn', vmin=-1, vmax=1)\n",
    "    ax.set_xticks(range(len(tokens)))\n",
    "    ax.set_yticks(range(len(tokens)))\n",
    "    ax.set_xticklabels(tokens, fontsize=9)\n",
    "    ax.set_yticklabels(tokens, fontsize=9)\n",
    "    ax.set_title(name, fontsize=11)\n",
    "    \n",
    "    for row in range(len(tokens)):\n",
    "        for col in range(len(tokens)):\n",
    "            ax.text(col, row, f'{sim[row, col]:.2f}',\n",
    "                    ha='center', va='center', fontsize=8)\n",
    "\n",
    "plt.suptitle('Token Similarity at Each Stage', y=1.02, fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Each layer mixes information between tokens via attention,\")\n",
    "print(\"changing their pairwise similarities.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. The Residual Connection Effect\n",
    "\n",
    "Let's see what happens without residual connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_layer_no_residual(x, params, n_heads):\n",
    "    \"\"\"Encoder layer WITHOUT residual connections.\"\"\"\n",
    "    attn_out, _ = multi_head_attention(\n",
    "        x, params['W_Q'], params['W_K'], params['W_V'], params['W_O'], n_heads\n",
    "    )\n",
    "    z = layer_norm(attn_out, params['gamma1'], params['beta1'])  # no x +\n",
    "    \n",
    "    ffn_out = feed_forward(z, params['W1'], params['b1'], params['W2'], params['b2'])\n",
    "    output = layer_norm(ffn_out, params['gamma2'], params['beta2'])  # no z +\n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "# Compare norms through 10 layers (with vs without residuals)\n",
    "n_test_layers = 10\n",
    "test_layers = [init_encoder_layer(d_model, n_heads, d_ff, device) for _ in range(n_test_layers)]\n",
    "\n",
    "x_with = X.clone()\n",
    "x_without = X.clone()\n",
    "norms_with = [x_with.norm().item()]\n",
    "norms_without = [x_without.norm().item()]\n",
    "\n",
    "for lp in test_layers:\n",
    "    x_with, _ = encoder_layer(x_with, lp, n_heads)\n",
    "    x_without = encoder_layer_no_residual(x_without, lp, n_heads)\n",
    "    norms_with.append(x_with.norm().item())\n",
    "    norms_without.append(x_without.norm().item())\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "ax.plot(norms_with, 'o-', label='With residual connections')\n",
    "ax.plot(norms_without, 's--', label='Without residual connections')\n",
    "ax.set_xlabel('Layer')\n",
    "ax.set_ylabel('Output norm')\n",
    "ax.set_title('Signal Propagation: Residual vs No Residual')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"With residuals:    norm stays stable (~{norms_with[-1]:.2f})\")\n",
    "print(f\"Without residuals: norm may drift or collapse ({norms_without[-1]:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Complete Architecture Diagram\n",
    "\n",
    "Here's everything we built, mapped to the original paper:\n",
    "\n",
    "```\n",
    "                    \"Attention Is All You Need\" — Encoder\n",
    "                    ════════════════════════════════════\n",
    "\n",
    "  token_ids ──► W_embed[token_ids] ──► ×√d_model ──► + PE ──► x\n",
    "                                                              │\n",
    "         ┌────────────────────────────────────────────────────┐\n",
    "         │  Encoder Layer (×N)                                │\n",
    "         │                                                    │\n",
    "         │  x ──┬──► MHA(x) ──► + ◄── x  ──► LayerNorm ──► z │\n",
    "         │      │               ▲                             │\n",
    "         │      │          (residual)                         │\n",
    "         │                                                    │\n",
    "         │  z ──┬──► FFN(z) ──► + ◄── z  ──► LayerNorm ──► out│\n",
    "         │      │               ▲                             │\n",
    "         │      │          (residual)                         │\n",
    "         └────────────────────────────────────────────────────┘\n",
    "                                                              │\n",
    "                                                              ▼\n",
    "                                                      encoder_output\n",
    "```\n",
    "\n",
    "Every box in this diagram corresponds to a function we implemented from raw tensor ops."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Summary\n",
    "\n",
    "| Component | Function | Parameters | Purpose |\n",
    "|-----------|----------|------------|---------|\n",
    "| **Token Embedding** | `W_embed[ids]` | vocab × d_model | Convert token IDs to vectors |\n",
    "| **Positional Encoding** | `sinusoidal_pe()` | 0 (fixed) | Inject position information |\n",
    "| **Multi-Head Attention** | `multi_head_attention()` | 4 × d_model² | Token interaction (who attends to whom) |\n",
    "| **Layer Normalization** | `layer_norm()` | 2 × d_model | Stabilize activations |\n",
    "| **Feed-Forward Network** | `feed_forward()` | 2 × d_model × d_ff + biases | Per-token nonlinear transformation |\n",
    "| **Residual Connection** | `x + sublayer(x)` | 0 | Gradient highway, information preservation |\n",
    "\n",
    "All implemented with only: `@` (matmul), `+`, `*`, `/`, `torch.exp`, `torch.clamp`, `torch.sqrt`, `.view()`, `.transpose()`.\n",
    "\n",
    "No `nn.Module`, no `nn.Linear`, no `nn.LayerNorm`, no `nn.TransformerEncoder`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
