{
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "colab": {
            "provenance": [],
            "gpuType": "T4"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        },
        "language_info": {
            "name": "python"
        },
        "accelerator": "GPU"
    },
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Transformer-XL from Scratch\n",
                "\n",
                "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/adiel2012/deep-learning-abc/blob/main/transformer_xl.ipynb)\n",
                "\n",
                "Transformer-XL addresses the **fixed context length limit** of standard Transformers.\n",
                "\n",
                "Key Innovations:\n",
                "1. **Segment-Level Recurrence**: Reuse hidden states from the previous segment as *extended memory* for the current segment (like RNNs but for tokens).\n",
                "2. **Relative Positional Encoding**: Since absolute positions don't work across segments (pos 0 happens every segment), it uses relative distances ($i - j$).\n",
                "\n",
                "This allows modeling **very long-term dependencies** beyond the training segment length."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install torch matplotlib"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import math\n",
                "\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f'Using device: {device}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Relational Attention (Relative Position)\n",
                "\n",
                "Standard attention: $A_{ij} = (W_q E_i + W_q U_i)^T (W_k E_j + W_k U_j)$\n",
                "Transformer-XL attention disentangles content and position:\n",
                "\n",
                "$$A_{rel} = \\underbrace{E_i^T W_q^T W_k E_j}_{\\text{content-content}} + \\underbrace{E_i^T W_q^T W_k R_{i-j}}_{\\text{content-position}} + \\underbrace{u^T W_k E_j}_{\\text{global-content}} + \\underbrace{v^T W_k R_{i-j}}_{\\text{global-position}}$$\n",
                "\n",
                "Where $R$ is relative pos embedding, $u, v$ are learnable global biases."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class RelationalAttention(nn.Module):\n",
                "    def __init__(self, d_model, n_heads):\n",
                "        super().__init__()\n",
                "        self.n_heads = n_heads\n",
                "        self.d_head = d_model // n_heads\n",
                "        \n",
                "        self.qkv_net = nn.Linear(d_model, 3 * d_model, bias=False)\n",
                "        self.o_net = nn.Linear(d_model, d_model, bias=False)\n",
                "        \n",
                "        # Global content bias (u) and global position bias (v)\n",
                "        self.u = nn.Parameter(torch.Tensor(self.n_heads, self.d_head))\n",
                "        self.v = nn.Parameter(torch.Tensor(self.n_heads, self.d_head))\n",
                "        \n",
                "        # Relative position embedding table (sinusoidal usually, learned here simpler)\n",
                "        self.r_emb = nn.Embedding(512, self.d_head)  # Max relative distance\n",
                "\n",
                "    def forward(self, x, mem=None):\n",
                "        # x: [len, batch, d_model]\n",
                "        # mem: [m_len, batch, d_model]\n",
                "        \n",
                "        qlen, bsz, _ = x.size()\n",
                "        mlen = mem.size(0) if mem is not None else 0\n",
                "        klen = qlen + mlen\n",
                "        \n",
                "        # Concatenate x with memory for Keys and Values\n",
                "        cat = torch.cat([mem, x], 0) if mem is not None else x\n",
                "        \n",
                "        # Compute Q, K, V\n",
                "        qkv = self.qkv_net(cat)\n",
                "        q, k, v = qkv.chunk(3, dim=-1)\n",
                "        \n",
                "        # Reshape for multi-head: [len, batch, n_heads, d_head]\n",
                "        # Only Query needs to cover 'x' (current segment)\n",
                "        q = q[-qlen:].view(qlen, bsz, self.n_heads, self.d_head)\n",
                "        k = k.view(klen, bsz, self.n_heads, self.d_head) # Keys cover history+current\n",
                "        v = v.view(klen, bsz, self.n_heads, self.d_head)\n",
                "\n",
                "        # Content-Content score: (Q + u) @ K^T\n",
                "        # We add bias u to Q for content matching\n",
                "        AC = torch.einsum('ibnd,jbnd->ijbn', q + self.u, k)\n",
                "        \n",
                "        # Content-Position score: (Q + v) @ R^T\n",
                "        # We use relative positions R\n",
                "        # Generate relative positions: 0, 1, ..., klen\n",
                "        pos_seq = torch.arange(klen - 1, -1, -1.0, device=x.device, dtype=torch.long)\n",
                "        pos_seq = pos_seq.clamp(max=511) # Clamp to embedding size\n",
                "        R = self.r_emb(pos_seq)\n",
                "        \n",
                "        # BD = R @ (Q + v)^T, simplified as Einstein sum\n",
                "        BD = torch.einsum('ibnd,jd->ijbn', q + self.v, R)\n",
                "        \n",
                "        scores = (AC + BD) / math.sqrt(self.d_head)\n",
                "        \n",
                "        # Causal Masking (standard lower triangular)\n",
                "        # [i, j] valid if i >= j - mlen (i.e., query i can attend to k_j if j is in past)\n",
                "        mask = torch.triu(\n",
                "            torch.ones((qlen, klen), device=x.device, dtype=torch.bool), \n",
                "            diagonal=1 + mlen\n",
                "        )\n",
                "        scores = scores.masked_fill(mask[:, :, None, None], float('-inf'))\n",
                "        \n",
                "        attn = torch.softmax(scores, dim=1) # Softmax over Key dimension\n",
                "        out = torch.einsum('ijbn,jbnd->ibnd', attn, v)\n",
                "        out = out.contiguous().view(qlen, bsz, -1)\n",
                "        \n",
                "        return self.o_net(out)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Transformer-XL Model with Recurrence\n",
                "\n",
                "Forward pass takes a `mems` list (hidden states from previous step)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class TransformerXLBlock(nn.Module):\n",
                "    def __init__(self, d_model, n_heads, d_ff):\n",
                "        super().__init__()\n",
                "        self.attn = RelationalAttention(d_model, n_heads)\n",
                "        self.ln1 = nn.LayerNorm(d_model)\n",
                "        self.ln2 = nn.LayerNorm(d_model)\n",
                "        self.ff = nn.Sequential(\n",
                "            nn.Linear(d_model, d_ff),\n",
                "            nn.GELU(),\n",
                "            nn.Linear(d_ff, d_model)\n",
                "        )\n",
                "    \n",
                "    def forward(self, x, mem=None):\n",
                "        # Pre-LN\n",
                "        attn_out = self.attn(self.ln1(x), mem)\n",
                "        x = x + attn_out\n",
                "        x = x + self.ff(self.ln2(x))\n",
                "        return x\n",
                "\n",
                "class TransformerXL(nn.Module):\n",
                "    def __init__(self, vocab_size, d_model, n_heads, n_layers):\n",
                "        super().__init__()\n",
                "        self.emb = nn.Embedding(vocab_size, d_model)\n",
                "        self.layers = nn.ModuleList([TransformerXLBlock(d_model, n_heads, 4*d_model) for _ in range(n_layers)])\n",
                "        self.head = nn.Linear(d_model, vocab_size)\n",
                "        \n",
                "    def forward(self, x, mems=None):\n",
                "        # x: [seq_len, batch]\n",
                "        # mems: list of [m_len, batch, d_model] for each layer\n",
                "        \n",
                "        if mems is None:\n",
                "            mems = [None] * len(self.layers)\n",
                "            \n",
                "        x = self.emb(x)\n",
                "        new_mems = []\n",
                "        \n",
                "        for i, layer in enumerate(self.layers):\n",
                "            # Store current input x to be next memory\n",
                "            # Important: Detach from graph to stop gradients flowing back endlessly!\n",
                "            new_mems.append(x.detach())\n",
                "            \n",
                "            x = layer(x, mem=mems[i])\n",
                "\n",
                "        logits = self.head(x)\n",
                "        return logits, new_mems\n",
                "\n",
                "# Init Model\n",
                "model = TransformerXL(vocab_size=10000, d_model=512, n_heads=8, n_layers=6).to(device)\n",
                "print(\"Transformer-XL Initialized\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Visualize Recurrence\n",
                "\n",
                "We process a long sequence in chunks, passing `mems` forward."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Simulate processing 2 segments\n",
                "seq_len = 20\n",
                "batch = 1\n",
                "\n",
                "# Segment 1\n",
                "input1 = torch.randint(0, 10000, (seq_len, batch), device=device)\n",
                "out1, mems1 = model(input1, mems=None)\n",
                "print(f\"Segment 1 output: {out1.shape}\")\n",
                "print(f\"Memory size: {mems1[0].shape}\")\n",
                "\n",
                "# Segment 2 (feeding mems1)\n",
                "input2 = torch.randint(0, 10000, (seq_len, batch), device=device)\n",
                "out2, mems2 = model(input2, mems=mems1)\n",
                "print(f\"Segment 2 output: {out2.shape} (Used memory from Seg 1)\")\n",
                "print(\"Success! This demonstrates state carry-over across segments.\")"
            ]
        }
    ]
}