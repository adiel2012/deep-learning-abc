{
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "colab": {
            "provenance": [],
            "gpuType": "T4"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        },
        "language_info": {
            "name": "python"
        },
        "accelerator": "GPU"
    },
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Vision Transformer (ViT) from Scratch\n",
                "\n",
                "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/adiel2012/deep-learning-abc/blob/main/vision_transformer.ipynb)\n",
                "\n",
                "ViT applies the pure Transformer architecture directly to sequences of image patches.\n",
                "\n",
                "Key Steps:\n",
                "1. **Patchify:** Split image into fixed-size patches.\n",
                "2. **Linear Projection:** Flatten patches and map to `d_model`.\n",
                "3. **Position Embeddings:** Add learnable position vectors.\n",
                "4. **Transformer Encoder:** Standard BERT-like encoder.\n",
                "5. **Classification Head:** MLP on `[CLS]` token."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install torch torchvision matplotlib"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import matplotlib.pyplot as plt\n",
                "import math\n",
                "\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f'Using device: {device}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Patch Embeddings\n",
                "\n",
                "Convert image (C, H, W) -> Sequence of (N, d_model).\n",
                "Can be implemented using a Conv2d layer with kernel_size = stride = patch_size."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class PatchEmbed(nn.Module):\n",
                "    def __init__(self, img_size=224, patch_size=16, in_chans=3, d_model=768):\n",
                "        super().__init__()\n",
                "        self.img_size = img_size\n",
                "        self.patch_size = patch_size\n",
                "        self.n_patches = (img_size // patch_size) ** 2\n",
                "        \n",
                "        # Use Conv2d to implement patch projection\n",
                "        self.proj = nn.Conv2d(in_chans, d_model, kernel_size=patch_size, stride=patch_size)\n",
                "\n",
                "    def forward(self, x):\n",
                "        # x: [B, C, H, W]\n",
                "        x = self.proj(x)  # [B, d_model, H/P, W/P]\n",
                "        x = x.flatten(2)  # [B, d_model, N_patches]\n",
                "        x = x.transpose(1, 2)  # [B, N_patches, d_model]\n",
                "        return x"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Transformer Encoder (Standard)\n",
                "\n",
                "Same as BERT encoder."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class Attention(nn.Module):\n",
                "    def __init__(self, dim, n_heads):\n",
                "        super().__init__()\n",
                "        self.n_heads = n_heads\n",
                "        self.scale = (dim // n_heads) ** -0.5\n",
                "        self.qkv = nn.Linear(dim, dim * 3, bias=False)\n",
                "        self.proj = nn.Linear(dim, dim)\n",
                "\n",
                "    def forward(self, x):\n",
                "        B, N, C = x.shape\n",
                "        qkv = self.qkv(x).reshape(B, N, 3, self.n_heads, C // self.n_heads).permute(2, 0, 3, 1, 4)\n",
                "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
                "\n",
                "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
                "        attn = attn.softmax(dim=-1)\n",
                "\n",
                "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
                "        x = self.proj(x)\n",
                "        return x\n",
                "\n",
                "class Block(nn.Module):\n",
                "    def __init__(self, dim, n_heads, mlp_ratio=4.):\n",
                "        super().__init__()\n",
                "        self.norm1 = nn.LayerNorm(dim)\n",
                "        self.attn = Attention(dim, n_heads)\n",
                "        self.norm2 = nn.LayerNorm(dim)\n",
                "        self.mlp = nn.Sequential(\n",
                "            nn.Linear(dim, int(dim * mlp_ratio)),\n",
                "            nn.GELU(),\n",
                "            nn.Linear(int(dim * mlp_ratio), dim)\n",
                "        )\n",
                "\n",
                "    def forward(self, x):\n",
                "        x = x + self.attn(self.norm1(x))\n",
                "        x = x + self.mlp(self.norm2(x))\n",
                "        return x"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Full ViT Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class VisionTransformer(nn.Module):\n",
                "    def __init__(self, img_size=224, patch_size=16, in_chans=3, num_classes=1000, \n",
                "                 d_model=768, depth=12, n_heads=12):\n",
                "        super().__init__()\n",
                "        self.patch_embed = PatchEmbed(img_size, patch_size, in_chans, d_model)\n",
                "        \n",
                "        # CLS token and Positional Embeddings\n",
                "        self.cls_token = nn.Parameter(torch.zeros(1, 1, d_model))\n",
                "        self.pos_embed = nn.Parameter(torch.zeros(1, 1 + self.patch_embed.n_patches, d_model))\n",
                "        \n",
                "        self.blocks = nn.ModuleList([\n",
                "            Block(d_model, n_heads) for _ in range(depth)\n",
                "        ])\n",
                "        \n",
                "        self.norm = nn.LayerNorm(d_model)\n",
                "        self.head = nn.Linear(d_model, num_classes)\n",
                "\n",
                "    def forward(self, x):\n",
                "        B = x.shape[0]\n",
                "        x = self.patch_embed(x)\n",
                "        \n",
                "        # Append CLS token\n",
                "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
                "        x = torch.cat((cls_tokens, x), dim=1)\n",
                "        \n",
                "        # Add pos embedding\n",
                "        x = x + self.pos_embed\n",
                "        \n",
                "        for blk in self.blocks:\n",
                "            x = blk(x)\n",
                "            \n",
                "        x = self.norm(x)\n",
                "        cls_out = x[:, 0]  # Take CLS token only\n",
                "        return self.head(cls_out)\n",
                "\n",
                "# Init ViT-Base\n",
                "model = VisionTransformer(img_size=224, patch_size=16, d_model=768, depth=12, n_heads=12).to(device)\n",
                "print(f\"ViT-Base Initialized: {sum(p.numel() for p in model.parameters())/1e6:.1f}M params\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Visualize Patch Embeddings\n",
                "\n",
                "Visualizing how an image is split."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Fake image batch\n",
                "img = torch.randn(1, 3, 224, 224, device=device)\n",
                "output = model(img)\n",
                "print(f\"Output shape: {output.shape} (Batch, Classes)\")\n",
                "\n",
                "# Visualize filters of first layer\n",
                "filters = model.patch_embed.proj.weight.data.cpu()\n",
                "print(f\"Patch Filters: {filters.shape}\")\n",
                "\n",
                "fig, axes = plt.subplots(4, 8, figsize=(10, 5))\n",
                "for i, ax in enumerate(axes.flatten()):\n",
                "    # Normalize filter for visualization\n",
                "    f = filters[i].permute(1, 2, 0)\n",
                "    f = (f - f.min()) / (f.max() - f.min())\n",
                "    ax.imshow(f)\n",
                "    ax.axis('off')\n",
                "plt.suptitle('First 32 Patch Projection Filters (Random Init)')\n",
                "plt.show()"
            ]
        }
    ]
}