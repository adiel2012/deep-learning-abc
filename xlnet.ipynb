{
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "colab": {
            "provenance": [],
            "gpuType": "T4"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        },
        "language_info": {
            "name": "python"
        },
        "accelerator": "GPU"
    },
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# XLNet: Generalized Autoregressive Pretraining\n",
                "\n",
                "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/adiel2012/deep-learning-abc/blob/main/xlnet.ipynb)\n",
                "\n",
                "XLNet combines the best of BERT (bidirectional context) and GPT (autoregressive generation) using **Permutation Language Modeling**.\n",
                "\n",
                "Key Concepts:\n",
                "1. **Permutation LM**: Predicts tokens in a random order (e.g., 3 → 1 → 2 → 4), allowing the model to see \"future\" context (bidirectional) while remaining autoregressive.\n",
                "2. **Two-Stream Attention**: To handle the permutation, it uses:\n",
                "   - **Content Stream**: Encodes the token itself (standard).\n",
                "   - **Query Stream**: Encodes position only (for prediction target).\n",
                "3. **Transformer-XL Backbone**: Inherits relative positional encoding and recurrence mechanism (recurrence omitted here for simplicity)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install torch matplotlib"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import math\n",
                "\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f'Using device: {device}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Permutation Masking\n",
                "\n",
                "Create a mask that enforces a specific permutation order. If order is [3, 1, 2, 4]:\n",
                "- 3 sees nothing (start)\n",
                "- 1 sees 3\n",
                "- 2 sees 3, 1\n",
                "- 4 sees 3, 1, 2"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def create_permutation_mask(seq_len, device):\n",
                "    \"\"\"Generate a random permutation mask.\"\"\"\n",
                "    perm = torch.randperm(seq_len, device=device)\n",
                "    inv_perm = torch.argsort(perm)\n",
                "    \n",
                "    # Standard causal mask for the permuted sequence\n",
                "    # mask[i, j] = 1 if i can attend to j\n",
                "    # In permuted order: i can attend to j if index(j) <= index(i) in permutation\n",
                "    \n",
                "    # More efficiently: Create causal mask, then shuffle rows/cols\n",
                "    causal_mask = torch.tril(torch.ones(seq_len, seq_len, device=device))\n",
                "    \n",
                "    # This is tricky to visualize: effectively we want the original positions\n",
                "    # i and j to have mask[i, j] = 1 if perm_rank[j] <= perm_rank[i]\n",
                "    \n",
                "    # Simple implementation: Shuffle mask rows/cols\n",
                "    # This corresponds to: permute inputs -> run causal attention -> unpermute outputs\n",
                "    # XLNet usually keeps inputs fixed and permutes the attention mask\n",
                "    \n",
                "    # Let's visualize the permutation\n",
                "    return perm, causal_mask\n",
                "\n",
                "perm, mask = create_permutation_mask(6, device)\n",
                "print(f\"Permutation order: {perm.tolist()}\")\n",
                "print(\"Conceptually: The model predicts tokens in this order.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Two-Stream Attention\n",
                "\n",
                "The core of XLNet. Standard query (Q), content key (K), content value (V).\n",
                "- **Content Stream ($h$)**: Self-attention on context + token itself.\n",
                "- **Query Stream ($g$)**: Self-attention on context + position embedding (but NOT token itself).\n",
                "\n",
                "This allows predicting $x_t$ using its position $p_t$ and all context $x_{<t}$, without seeing $x_t$."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class TwoStreamAttention(nn.Module):\n",
                "    def __init__(self, d_model, n_heads):\n",
                "        super().__init__()\n",
                "        self.n_heads = n_heads\n",
                "        self.d_k = d_model // n_heads\n",
                "        \n",
                "        # Parameters shared between streams usually, simplified here\n",
                "        self.q_proj = nn.Linear(d_model, d_model)\n",
                "        self.k_proj = nn.Linear(d_model, d_model)\n",
                "        self.v_proj = nn.Linear(d_model, d_model)\n",
                "        self.o_proj = nn.Linear(d_model, d_model)\n",
                "        \n",
                "    def forward(self, h, g, mask):\n",
                "        # h: Content stream (batch, len, d_model)\n",
                "        # g: Query stream (batch, len, d_model)\n",
                "        \n",
                "        # Content Stream Attention (standard self-attention with causal mask)\n",
                "        # h attends to h\n",
                "        Q_h = self.q_proj(h)\n",
                "        K_h = self.k_proj(h)\n",
                "        V_h = self.v_proj(h)\n",
                "        \n",
                "        # Query Stream Attention\n",
                "        # g attends to h (using position info from g, content from h)\n",
                "        Q_g = self.q_proj(g)\n",
                "        # K and V come from h (context)\n",
                "        \n",
                "        # Compute scores\n",
                "        def attention(Q, K, V, mask_type):\n",
                "            scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
                "            # Apply mask (simplified)\n",
                "            if mask is not None:\n",
                "                scores = scores + mask\n",
                "            probs = torch.softmax(scores, dim=-1)\n",
                "            return torch.matmul(probs, V)\n",
                "\n",
                "        # 1. Content update: h uses standard causal mask (can see itself)\n",
                "        # In permutation LM, mask ensures we see only 'previous' tokens in perm order\n",
                "        h_new = attention(Q_h, K_h, V_h, \"content\")\n",
                "        \n",
                "        # 2. Query update: g uses strict mask (cannot see itself)\n",
                "        g_new = attention(Q_g, K_h, V_h, \"query\")\n",
                "        \n",
                "        return self.o_proj(h_new), self.o_proj(g_new)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. XLNet Model Structure\n",
                "\n",
                "Stacking Two-Stream layers."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class XLNetLayer(nn.Module):\n",
                "    def __init__(self, d_model, n_heads):\n",
                "        super().__init__()\n",
                "        self.attn = TwoStreamAttention(d_model, n_heads)\n",
                "        self.ln1 = nn.LayerNorm(d_model)\n",
                "        self.ln2 = nn.LayerNorm(d_model)\n",
                "        self.ff = nn.Sequential(\n",
                "            nn.Linear(d_model, 4 * d_model),\n",
                "            nn.GELU(),\n",
                "            nn.Linear(4 * d_model, d_model)\n",
                "        )\n",
                "        \n",
                "    def forward(self, h, g, mask):\n",
                "        # Two-stream attention\n",
                "        h_attn, g_attn = self.attn(self.ln1(h), self.ln1(g), mask)\n",
                "        h = h + h_attn\n",
                "        g = g + g_attn\n",
                "        \n",
                "        # Feed-foward (applied to both streams independently)\n",
                "        h = h + self.ff(self.ln2(h))\n",
                "        g = g + self.ff(self.ln2(g))\n",
                "        return h, g\n",
                "\n",
                "class XLNet(nn.Module):\n",
                "    def __init__(self, vocab_size, d_model, n_heads, n_layers):\n",
                "        super().__init__()\n",
                "        self.token_emb = nn.Embedding(vocab_size, d_model)\n",
                "        self.layers = nn.ModuleList([XLNetLayer(d_model, n_heads) for _ in range(n_layers)])\n",
                "        self.lm_head = nn.Linear(d_model, vocab_size)\n",
                "        \n",
                "    def forward(self, input_ids):\n",
                "        # Initialize streams\n",
                "        # Content stream (h): Initialized with word embeddings\n",
                "        h = self.token_emb(input_ids)\n",
                "        \n",
                "        # Query stream (g): Initialized with a learnable vector 'w' (omitted, using zeros/random)\n",
                "        # In practice, g starts as positional embeddings only\n",
                "        g = torch.zeros_like(h)\n",
                "        \n",
                "        # Mask would be passed here\n",
                "        for layer in self.layers:\n",
                "            h, g = layer(h, g, mask=None)\n",
                "            \n",
                "        # Prediction is done using query stream 'g' (which didn't see target content)\n",
                "        logits = self.lm_head(g)\n",
                "        return logits\n",
                "\n",
                "# Init XLNet\n",
                "model = XLNet(vocab_size=32000, d_model=768, n_heads=12, n_layers=6).to(device)\n",
                "print(f\"XLNet Initialized: {sum(p.numel() for p in model.parameters())/1e6:.1f}M params\")"
            ]
        }
    ]
}